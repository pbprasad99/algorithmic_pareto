{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>Welcome to patibanb.me</p>"},{"location":"About/","title":"Index","text":""},{"location":"About/#_1","title":"Index","text":"<p>Hello! I'm P. Bhanuprasad , an Engineer with 10 years of experience building robust data pipelines, scalable analytics platforms, and cloud-based solutions. My expertise spans across data engineering, ETL development, big data technologies, and cloud infrastructure.</p> <p>Over the past decade, I have worked with diverse teams to design and implement end-to-end data solutions, optimize data workflows, and enable data-driven decision making. I am passionate about leveraging technology to solve complex business problems and deliver actionable insights.</p> <p>I enjoy collaborating with cross-functional teams, building useful things, solving pesky problems and staying up-to-date with the latest advancements in data engineering, analytics and AI.</p> <p>Feel free to connect with me on LinkedIn or explore this website!</p>"},{"location":"Algorithms/","title":"Algorithms - The Pareto Set","text":""},{"location":"Algorithms/#algorithms","title":"Algorithms","text":"<p>These are my notes on Algorithms. This is very much a work in progress. My plan is to put down everything in a rough way first, and keep refining it over time.</p>"},{"location":"Algorithms/#philosophy","title":"Philosophy","text":"<p>Broadly, I classify any algorithm or data structure into recursive or iterative. Of course, this is not a strict demarcation, but only in terms of the usual implementations or concpetualization.</p> <p>As for data structures, there are two fundamental types in terms of how they are stored : contiguous storage (arrays, dynamic arrays) and linked storage (linked lists, trees, graphs) </p>"},{"location":"Algorithms/#mental-models","title":"Mental models","text":"<p>To effectively use a concept, it helps greatly to have the right mental model which allows you to use it in a consistent manner.</p> <p>For example, understanding and thinking of binary search in terms of bisect left and bisect right makes it easy to actually use it in a consistent manner.</p> <p>Another example, for array partitioning, just thinking of and  defining the loop invariant correctly is key. </p> <p>My effort is always to develop the correct understanding and mental model for any concept.</p>"},{"location":"Algorithms/#call-a-thing-by-its-right-name","title":"Call a thing by its right name","text":"<p>Just knowing the name of a thing is very powerful.  Know the name of the problem and know the name of the algorithm.</p>"},{"location":"Algorithms/External_Sorting/","title":"External Sorting","text":""},{"location":"Algorithms/External_Sorting/#external-sorting","title":"External Sorting","text":""},{"location":"Algorithms/External_Sorting/#problem-definition","title":"Problem Definition","text":"<p>Given limited RAM, how do you sort a file that is too large to fit into memory?</p>"},{"location":"Algorithms/External_Sorting/#solution","title":"Solution","text":"<p>Suppose you have an X GB file and only 2 GB of RAM, where X &gt; 2.</p> <p>Steps:</p> <ol> <li>Divide the file into chunks of size equal to available RAM (e.g., X // 2 GB chunks if RAM is 2 GB).</li> <li>For each chunk:<ul> <li>Read the chunk into memory.</li> <li>Sort it.</li> <li>Write it back to disk as a \"part file\".</li> </ul> </li> <li>Merge the sorted part files:<ul> <li>This is now a k-way merge problem.</li> <li>You can either:<ul> <li>Run a single k-way merge, or</li> <li>Use multiple passes of two-way merges to ultimately merge all part files into a single sorted file.</li> </ul> </li> </ul> </li> </ol>"},{"location":"Algorithms/External_Sorting/#additional-references","title":"Additional References","text":"<ol> <li>Wikipedia: External Sorting</li> </ol>"},{"location":"Algorithms/properties_of_comparison_based_sorting_algorithms/","title":"Comparison Based Sorting","text":""},{"location":"Algorithms/properties_of_comparison_based_sorting_algorithms/#properties-of-comparison-based-sorting-algorithms","title":"Properties of Comparison-Based Sorting Algorithms","text":""},{"location":"Algorithms/properties_of_comparison_based_sorting_algorithms/#quicksort","title":"Quicksort","text":"<ul> <li>Stability: Not stable  </li> <li>Adaptivity: Not adaptive  </li> <li>In-Place: Yes  </li> <li>Time Complexity: O(N log N) average case  </li> <li>Worst Case: O(N\u00b2) (e.g., sorted input, duplicate keys, poor pivot selection)  </li> </ul> <p>Notes: - Pivot selection and partitioning schemes significantly affect complexity.</p> <p>Pivot Selection: </p> <ul> <li>Fixed at high or low index (worst case)</li> <li>Random (good enough in practice)</li> <li>Median-of-three, etc.</li> </ul> <p>Partitioning: </p> <ul> <li>Two-way (e.g., Hoare partitioning with weak condition)</li> <li>Three-way (e.g., Dijkstra)</li> <li>Dual-pivot partitioning</li> </ul> <p>Quicksort is a highly researched algorithm with many variations and derivatives\u2014it's more like a family of algorithms.</p> <p>Important: </p> <ul> <li>The most common partitioning scheme found online (Lomuto partition) performs more swaps than Hoare partition with a weak condition.</li> <li>Lomuto degrades to O(N\u00b2) when all elements are the same.</li> <li>Hoare partition with a strict condition also degrades to O(N\u00b2) for duplicate keys.</li> <li>Lomuto was popularized by Bentley in \"Programming Pearls\" because he found Hoare unintuitive.  </li> <li>Advantage: Lomuto uses two forward iterators, so it can be used on singly linked lists.</li> </ul>"},{"location":"Algorithms/properties_of_comparison_based_sorting_algorithms/#mergesort","title":"Mergesort","text":"<ul> <li>Stability: Stable  </li> <li>Adaptivity: Not adaptive  </li> <li>In-Place: No (needs extra space)  </li> <li>Time Complexity: O(N log N) worst case  </li> </ul> <p>Can be combined with insertion sort for small subproblem sizes.</p>"},{"location":"Algorithms/properties_of_comparison_based_sorting_algorithms/#bubble-sort","title":"Bubble Sort","text":"<ul> <li>Stability: Stable  </li> <li>Adaptivity: Can be made adaptive (add a flag to check if a swap occurred; if not, exit early)  </li> <li>In-Place: Yes  </li> <li>Time Complexity: O(N\u00b2)  </li> </ul> <p>Optimized Bubble Sort Example:</p> <pre><code>def optimized_bubble_sort(arr):\n    n = len(arr)\n    for i in range(n - 1):\n        swapped = False  # Flag for no swaps in a pass\n        for j in range(n - i - 1):\n            if arr[j] &gt; arr[j + 1]:\n                arr[j], arr[j + 1] = arr[j + 1], arr[j]\n                swapped = True  # Swap occurred\n        if not swapped:\n            return  # Already sorted, no more passes needed\n    return arr\n</code></pre>"},{"location":"Algorithms/properties_of_comparison_based_sorting_algorithms/#insertion-sort","title":"Insertion Sort","text":"<ul> <li>Stability: Stable  </li> <li>Adaptivity: Adaptive  </li> <li>In-Place: Yes  </li> <li>Time Complexity: O(N\u00b2)  </li> </ul> <p>Example:</p> <pre><code>def insertion_sort(arr):\n    for i in range(1, len(arr)):\n        key = arr[i]\n        j = i - 1\n        while j &gt;= 0 and key &lt; arr[j]:\n            arr[j + 1] = arr[j]\n            j -= 1\n        arr[j + 1] = key\n</code></pre> <ul> <li>Similar to optimized bubble sort, except:</li> <li>In bubble sort, the maximum element is bubbled into the unsorted section.</li> <li>In insertion sort, the minimum element is inserted into the sorted section.</li> <li>Online: Each new element can be put in its sorted place as it is received.</li> <li>Shell Sort: A faster variation of insertion sort (named after D.L. Shell), using insertion sort on periodic subarrays.</li> </ul> <p>See also: - Stack Overflow: Online vs Offline Sorting Algorithms - CS StackExchange: Fastest Online Sorting Algorithm</p>"},{"location":"Algorithms/properties_of_comparison_based_sorting_algorithms/#selection-sort","title":"Selection Sort","text":"<ul> <li>Stability: Not stable  </li> <li>Adaptivity: Not adaptive  </li> <li>In-Place: Yes  </li> <li>Time Complexity: O(N\u00b2)  </li> </ul> <p>Note: - Only advantage over insertion sort: performs fewer swaps.</p>"},{"location":"Algorithms/properties_of_comparison_based_sorting_algorithms/#additional-resources","title":"Additional Resources","text":"<ol> <li>Toptal: Sorting Algorithms</li> <li>YouTube: Sorting Algorithms Playlist</li> </ol>"},{"location":"Algorithms/Array_Linearization/Mapping_1d_to_2d_Arrays/","title":"Mapping 1D Array to 2D Array","text":""},{"location":"Algorithms/Array_Linearization/Mapping_1d_to_2d_Arrays/#mapping-1d-array-to-2d-array","title":"Mapping 1D Array to 2D Array","text":"<p>Consider a 1D array that represents a 2D array:</p> <pre><code>[ 00, 01, 02, 11, 12, 13 ]  # Row-major order\n  0   1   2   3   4   5\n</code></pre> <p>This could represent the following 2D array: </p><pre><code>[\n  [00, 01, 02],\n  [11, 12, 13]\n]\n</code></pre><p></p>"},{"location":"Algorithms/Array_Linearization/Mapping_1d_to_2d_Arrays/#row-major-order","title":"Row-Major Order","text":"<p>To convert a 1D array index back to 2D array coordinates in row-major order, use these formulas:</p> \\[ \\text{row_index} = \\left\\lfloor\\frac{\\text{index}}{\\text{number_of_cols}}\\right\\rfloor \\] \\[ \\text{col_index} = \\text{index} \\bmod \\text{number_of_cols} \\] <p>Explanation:</p> <ul> <li>Integer division (\u230aindex/cols\u230b) gives the row number because each row contains <code>number_of_cols</code> elements</li> <li>Remainder (modulo) gives the position within that row</li> </ul> <p>For example, given index <code>4</code> with <code>number_of_cols = 3</code>:</p> <ul> <li>row_index = \u230a4/3\u230b = 1</li> <li>col_index = 4 mod 3 = 1</li> <li>Therefore, index 4 maps to position (1,1)</li> </ul>"},{"location":"Algorithms/Array_Linearization/Mapping_1d_to_2d_Arrays/#column-major-order","title":"Column-Major Order","text":"<p>For a 1D array in column-major order: </p><pre><code>[ 00, 11, 01, 12, 02, 13 ]  # Column-major order\n  0   1   2   3   4   5\n</code></pre><p></p> <p>The formulas are:</p> \\[ \\text{row_index} = \\text{index} \\bmod \\text{number_of_rows} \\] \\[ \\text{col_index} = \\left\\lfloor\\frac{\\text{index}}{\\text{number_of_rows}}\\right\\rfloor \\] <p>Explanation:</p> <ul> <li>Remainder (modulo) gives position within the current column</li> <li>Integer division gives which column we're in</li> </ul> <p>For example, given index <code>3</code> with <code>number_of_rows = 2</code>:</p> <ul> <li>row_index = 3 mod 2 = 1</li> <li>col_index = \u230a3/2\u230b = 1</li> <li>Therefore, index 3 maps to position (1,1)</li> </ul>"},{"location":"Algorithms/Array_Linearization/Mapping_1d_to_2d_Arrays/#practice","title":"Practice","text":"Search in a 2D Matrix <pre><code>         \"\"\"\n         Solution for :  https://leetcode.com/problems/search-a-2d-matrix/description/\n\n\n         If you look at the examples, the data in the 2d matrix is sorted in row-major order.\n\n         You can use a 1D index to binary search (bisect left) over the range of the entire array.\n\n         The range of the 1D index will be :\n         lo =0 , hi = num_rows*num_cols\n\n         To get the value at any 1D index :\n         matrix[1d_index//num_cols][1d_index%num_cols]\n         \"\"\"\n         class Solution:\n             @staticmethod\n             def get_val_at_sn(sn: int, matrix : List[List[int]]) -&gt; int :\n                 \"\"\"Given a 1D index into 2D Matrix, Return the corresponding Value.\"\"\"\n                 num_rows,num_cols = len(matrix), len(matrix[0])\n\n                 return matrix[sn//num_cols][sn%num_cols ]\n\n             def searchMatrix(self, matrix: List[List[int]], target: int) -&gt; bool:\n                 \"\"\"\n                 00 - 0 01 -1 02 -2\n                 10 - 3 11 -4 12 -5\n                 21 - 6  22 -7 23 -8 \n\n                 3 - 3 \n                 r = 1 , c = 0\n\n                 r = sn // num_cols , c = sn % num_columns\n                 \"\"\"\n\n                 num_rows,num_cols = len(matrix), len(matrix[0])\n                 lo,hi = 0 , num_rows*num_cols\n\n                 while lo &lt; hi :\n                     mid = (lo + hi) // 2\n                     val = self.get_val_at_sn(mid,matrix)\n                     if val &lt; target :\n                         lo = mid + 1\n                     else :\n                         hi = mid\n\n                 if lo ==  num_rows*num_cols :\n                     return False\n\n                 if self.get_val_at_sn(lo,matrix) == target :\n                     return True\n\n                 return False\n</code></pre> Reshape Matrix <pre><code>         \"\"\"\n         Solution for: https://leetcode.com/problems/reshape-the-matrix/\n\n         Allocate the target array.\n         Use 1D indexing to read and write values from the source matrix to the target matrix.\n         \"\"\"\n         class Solution:\n             @staticmethod\n             def read_val(linear_index, matrix) : \n                 num_cols = len(matrix[0])\n                 return matrix[linear_index//num_cols][linear_index%num_cols]\n\n             @staticmethod\n             def write_val(linear_index, matrix,val) :\n                 \"\"\"\n                 write val at linear index into matrix\n                 \"\"\"\n                 num_cols = len(matrix[0])\n                 matrix[linear_index//num_cols][linear_index%num_cols] = val \n\n             def matrixReshape(self, mat: List[List[int]], r: int, c: int) -&gt; List[List[int]]:\n\n                 #Quote :\"If the reshape operation with given parameters is possible and legal, output the new reshaped matrix; Otherwise, output the original matrix.\"\n                 if not len(mat)*len(mat[0]) == r*c :\n                     return mat\n\n                 # Allocate target Matrix \n                 target = [ [0]*c for _ in range(r) ]\n\n                 #read from source matrix and write into target matrix\n                 for i in range(len(mat) * len(mat[0])) :\n                     val = self.read_val(i,mat)\n                     self.write_val(i,target,val)\n\n                 return target\n</code></pre>"},{"location":"Algorithms/Array_Linearization/Mapping_2d_to_1d_Arrays/","title":"Mapping 2D Array to 1D Array","text":""},{"location":"Algorithms/Array_Linearization/Mapping_2d_to_1d_Arrays/#mapping-2d-array-to-1d-array","title":"Mapping 2D Array to 1D Array","text":"<p>Consider a 2D array:</p> <pre><code>[\n  [00, 01, 02],\n  [11, 12, 13]\n]\n</code></pre> <p>To convert this 2D array to a 1D array using row-major order, you list the elements row by row:</p> <pre><code>[ 00  ,01 , 02,   11,   12 ,  13 ]\n  0     1   2      3     4    5 \n</code></pre>"},{"location":"Algorithms/Array_Linearization/Mapping_2d_to_1d_Arrays/#row-major-order","title":"Row Major Order","text":"<p>For example, the element <code>12</code> is at position <code>(1, 2)</code> (row index 1, column index 2). Its corresponding index in the 1D array is <code>4</code>.</p> <p>The general formula for mapping a 2D array element at position \\((\\text{row_index},\\ \\text{col_index})\\) to a 1D array index is:</p> \\[ \\text{index} = \\text{row_index} \\times \\text{number_of_cols} + \\text{col_index} \\] <p>Explanation:</p> <ul> <li>\\(\\text{row_index} \\times \\text{number_of_cols}\\): Counts all elements in the rows before the current row.</li> <li>\\(\\text{col_index}\\): Gives the position within the current row.</li> </ul> <p>So, to find the 1D index, count all elements in previous rows, then add the column index of the current element.</p>"},{"location":"Algorithms/Array_Linearization/Mapping_2d_to_1d_Arrays/#column-major-order","title":"Column Major Order","text":"<p>To convert a 2D array to a 1D array using column-major order, you list the elements column by column:</p> <pre><code>[ 00, 11,   01,  12,  02,  13]\n  0   1     2    3    4    5\n</code></pre> <p>For example, the element <code>12</code> is at position <code>(1, 2)</code> (row index 1, column index 2). Its corresponding index in the 1D array is <code>3</code>.</p> <p>The general formula for mapping a 2D array element at position \\((\\text{row_index},\\ \\text{col_index})\\) to a 1D array index in column-major order is:</p> \\[ \\text{index} = \\text{col_index} \\times \\text{number_of_rows} + \\text{row_index} \\] <p>Explanation:</p> <ul> <li>\\(\\text{col_index} \\times \\text{number_of_rows}\\): Counts all elements in the columns before the current column.</li> <li>\\(\\text{row_index}\\): Gives the position within the current column.</li> </ul> <p>So, to find the 1D index, count all elements in previous columns, then add the row index of the current element.</p>"},{"location":"Algorithms/Array_Linearization/Mapping_2d_to_1d_Arrays/#addition-resources","title":"Addition Resources","text":"<ol> <li>Wikipedia - Row Major vs Column Major</li> <li>Performance Analysis : Row Major vs Column Major</li> <li>Why Use Parquet</li> <li>Reddit Discussion on Columnar DBs</li> </ol>"},{"location":"Algorithms/Array_Partitioning/Djikstra/","title":"Djikstra's Three Way Partition","text":""},{"location":"Algorithms/Array_Partitioning/Djikstra/#djikstras-three-way-partition","title":"Djikstra's Three Way Partition","text":"<p>Coming Soon</p>"},{"location":"Algorithms/Array_Partitioning/Hoare/","title":"Hoare Partition","text":""},{"location":"Algorithms/Array_Partitioning/Hoare/#hoare-partition","title":"Hoare Partition","text":""},{"location":"Algorithms/Array_Partitioning/Hoare/#hoare-partition-with-strict-condition","title":"Hoare partition with strict condition","text":"<p>Let us consider the simplest version of the problem.</p> <p>Given an array of integers, rearrange the elements such that the left part contains elements less than or equal to a pivot value and the right part contains values greater than the pivot value.</p> <p>Consider this example array and a pivot value of 4   :</p> <p>Example</p> <p>[ 1 , 9, 3, 5, 7, 4, 8  ]</p> <p>If this array were already partitioned s.t. the left partition has only elements less than or equal to the pivot. These would hold true : </p><pre><code>Any element in left partition &lt;= pivot\nAny element in right partition &gt; pivot\n</code></pre> If we take left and right pointers  while verifying that arr[left] &lt;= pivot and arr[right] &gt; pivot, left and right pointers would move past each other before evaluating to False for the first time. Left pointer would end up at the BEGINNING of the right partition and the right pointer would end up at the END of the left partition.<p></p> <p>Pointers</p> <pre><code>[ 1,   3,     4,   5,   7,   8,   9 ]\n  L/T  L/T   L/T *L/F*                            # L = Left pointer, T = arr[left]&lt;= pivot is True , F = Its False\n            *R/F* R/T   R/T     R/T    R/T        # R = right pointer, T = arr[right] &gt; pivot is True, F = Its False   \n</code></pre> <p>If left evaluates to True while its less than right, it would mean that this element is in the wrong partition. This also implies that there must be an element in the right partition which belongs to the left partition. </p> <p>Pointers</p> <pre><code>[  1 ,   9,   3,   5,   7,   4,   8  ]         \n  L/T **L/F**                                    # L = Left pointer, T = arr[left]&lt;= pivot is True, F = Its False\n                          **R/F**   R/T          # R = right pointer, T = arr[right] &gt; pivot is True, F = Its False                \n</code></pre> <p>We can simply swap these and keep moving until left and right pass each other.</p> <p>So, the algorithm would be:</p> <p>Algorithm</p> <pre><code>#Initialize pivot = arr[0]\n\n#Invariant 1) [0,right] only contains elements less than or equal to pivot\n#Invariant 2) (right, len(arr) -1] only contains elements greater than pivot\n\n\nWhile left &lt;= right :\n      while left&lt;=right and (Invaraint 1) is True :\n           left +=1\n      while left&lt;=right and (Invariant 2) is True :\n           right -=1\n      if left &lt; right :\n          swap left and right\nreturn right\n</code></pre> <p>This implementation does not necessarily place the pivot in its sorted position.</p> <p>Or rather, the problem is not asking us to do this,</p> <p>This array is partitioned by 4 but 4 is not in its sorted position.</p> <p>Example</p> <pre><code>[ 1,   4,    3,  |5,   7,   8,   9 ]\n</code></pre> <p>We can put the pivot in its sorted position by swapping it with the right pointer after running the algorithm.  A good way to handle this is :</p> <ul> <li>Ensure that  the pivot at the lowest index.</li> <li>Run the partitioning algorithm on the rest of the array.</li> <li>Swap right with low.</li> </ul> <p>But again, this will not group multiple instances of pivot together if pivot is duplicated. That is another problem called three way partitioning.</p> <p>Here is the python implementation for both cases :</p> Code Without placing pivot in sorted postitionPlacing Pivot in its sorted position <pre><code>from typing import List\n\n#Given a list return partition index after partition on the pivot index value\ndef partition(nums: List[int],lo,hi) -&gt; int :\n    \"\"\"\n    Hoare partition with strong condition.\n    Picks the first element as the pivot value.\n    There is no guarantee that pivot will be in its right place in this implementation.\n    All we get is that the first half has elements less than or equal to pivot.\n    Pivot can be any value in the array. (Pivot Value can actually be any arbitrary value in this version).\n    \"\"\"\n    pivot = nums[lo]\n    left,right = lo+1 ,hi\n\n    while left &lt;= right :\n        while left&lt;= right and nums[left] &lt;=  pivot :\n            left+=1\n        while left &lt;=right and nums[right] &gt; pivot :\n            right-=1\n        if left &lt; right :\n            nums[left],nums[right] = nums[right],nums[left]\n\n    print(f\"Pivot Value: {pivot} , Partitioned list : {nums}, Partition Index = {right}\")\n    return right\n\nif __name__ == \"__main__\" :\n    nums = [2,4,5,1,4,8,9]\n    partition(nums, 0, len(nums)-1)\n    nums = [2,2,2,2,2,2]\n    partition(nums, 0, len(nums)-1)\n</code></pre> <pre><code>from typing import List\nimport random\n\n# Given a list return partition index after partition on the pivot index value\ndef partition(nums: List[int],lo,hi) -&gt; int :\n    \"\"\"\n    Hoare partition with strong condition.\n    Here we modififed the code slightly to:\n    Fix pivot value at index zero.\n    Partition by the pivot value.\n    Finally place the pivot  at the end of the left partition.\n    This implementation puts pivot in the sorted position as well after partitioning.\n    This version is  not entropy optimal . That is, it does not lead to balanced partitions when there are a lot of duplicates. \n\n    Why? Consider,  if this partition scheme is used in quickselect for finding the smallest element in an array of size n). \n    What happens when all elements are duplicates (disregarding random pivot selection):\n    [1,1,1,1,1,1]\n     l         r   Iter 1 : 6  comparisons\n     l       r     Iter 2 : 5  comparisons\n     l     r       Iter 3 : 4  comparisons\n     l   r         Iter 4 : 3  comparisons\n     l r           Iter 5 : 2  comparisons\n     lr            Iter 6 : 1  comparisons\n\n    Number of comparisons = SUM([1.....n]) = n(n+1)/2 = (n*2 + n ) /2\n    Therefore, Complexity is O(n^2) \n    \"\"\"\n\n    print(f\"Input List : {nums}\")\n    left, right = lo,hi\n\n    # RANDOM PIVOT SELECTION\n    # Always keep pivot at index lo\n    # pivot_index = random.randint(lo,hi)\n    # nums[lo],nums[pivot_index] =  nums[pivot_index],nums[lo]\n\n    #PARTITIONING\n    left+=1\n    pivot_val = nums[lo]\n\n    while left &lt;= right :\n\n        while left &lt;= right and nums[left] &lt;= pivot_val :\n            left+=1\n\n        while left &lt;= right and nums[right] &gt; pivot_val :\n            right-=1\n\n        if left &lt; right :\n            #swap\n            nums[left],nums[right] = nums[right],nums[left]\n\n    #Put pivot Value in sorted  position\n    nums[lo] , nums[right] =  nums[right] ,nums[lo]\n    print(f\"Pivot Value: {pivot_val}  ,Partitioned list : {nums}, Partition Index = {right}\")\n    return right\n\nif __name__ == \"__main__\" : \n    # nums= [1,2,3]\n    # partition(nums, 0, len(nums)-1)\n    # nums = [2,2,2,2]\n    # partition(nums, 0, len(nums)-1)\n    # nums = [2,4,5,1,4,8,9]\n    # partition(nums, 0, len(nums)-1)\n    # nums= [3,2]\n    # partition(nums, 0, len(nums)-1)\n    # nums= [3]\n    # partition(nums, 0, len(nums)-1)\n    nums = [5,2,1,1,1,1,1,1,1,1,1,5,5,-3,-2,-5]\n    partition(nums, 0, len(nums)-1)\n</code></pre>"},{"location":"Algorithms/Array_Partitioning/Hoare/#hoare-partition-with-weak-condition","title":"Hoare partition with weak condition","text":"<p>The problem with the partition scheme with a strict condition (where we always put pivot values  in one of the partitions) is that it produces unbalanced partitions when used in quickselect or quicksort.</p> <p>Consider if the above partition scheme were used in quickselect for finding the smallest element in an array with k=0 and fixed pivot selection at arr[0]. </p> <p>What happens when all elements are duplicates :</p> <p>Dry Run</p> <p>l and r indicate the search space for each iteration of quickselect loop/each recursive call to quickselect. Comparisons refer to those made by the partitioning subroutine.</p> <pre><code>    [1,1,1,1,1,1]\n     l         r   Iter 1 : 5  comparisons\n     l       r     Iter 2 : 4  comparisons\n     l     r       Iter 3 : 3  comparisons\n     l   r         Iter 4 : 2  comparisons\n     l r           Iter 5 : 1  comparisons\n     lr            Iter 6 : 0  comparisons\n</code></pre> <p>The Hoare partition, after performing r-l+1 comparisons, would return the rightmost position for each call recursive call to quickselect. r would reduce linearly until it meets l, thus giving a quadratic running time!</p> <p>Number of comparisons = SUM([1.....n]) = n(n+1)/2 = (n*2 + n ) /2 Therefore, Complexity is O(n^2)  in this case.</p> <p>To mitigate this, we could slightly modify the definition of our partitions :</p> <p></p><pre><code>Any element in left partition &lt;= pivot\nAny element in right partition &gt;= pivot\n</code></pre> In terms of Invariants :<p></p> <p>Invariants</p> <pre><code>Invariant 1) [low,right] only contains elements less than OR EQUAL to pivot\n\nInvariant 2) (right, high] only contains elements greater OR EQUAL to pivot\n</code></pre> <p>That is, pivot values are allowed to be in either partition.</p> <p>Our problem now becomes : </p> <p>Reframed Problem</p> <p>Given an array of integers, rearrange the elements such that the left part contains elements less than or equal to a pivot value and the right part contains values greater than or equal to the pivot value.</p> <p>The implementation is tricky when handling values equal to pivot. We stop scanning when left or right are equal to pivot in either partition. We swap and move the left and right pointers. After any swap, both partitions will increase by 1. Pivot values might end up in any partition. This will also be the case when, left and right are both pointing to pivot value i.e. both partitions increase by 1 and there is a redundant swap between two pivot values.</p> <p>Algorithm</p> <pre><code>While left &lt;= right :\n      while  left&lt;=right and left &lt; pivot :\n           left +=1\n      while left&lt;=right and right &gt; pivot:\n           right -=1\n      if left &lt;= right :\n          swap left and right\n          left+=1\n          right-=1\nreturn right\n</code></pre> <p>Let us see the behavior of quickselect with this scheme for the same case :</p> <p>Dry Run</p> <pre><code>    [1,1,1,1,1,1]\n     l         r   Iter 1 : 5  comparisons\n     l   r         Iter 2 : 3  comparisons; This iteration returns r=0 and ends the loop\n     rl            Iter 2 END \n</code></pre> <p>Now the hoare partitioning subroutine will reduce r by half in each iteration. And quickselect is called LOG(n) times in the worst case, when k = 0. The complexity is reduced to LOG(n)*LOG(n).</p> <p>Here is the implementation :</p> Code Hoare Partition with Weak Condition <pre><code>from typing import List\nimport random\n\n\ndef partition(arr, lo, hi):\n    \"\"\"\n    Entropy Optimal Hoare partition.\n    Produces balanced partitions when there are large number of duplicates.\n    [lo,right] contains elements less than or equal to pivot.\n    (right,hi]  contains elements greater than or equal to pivot.\n    Invariants for the while loop :\n    [lo] has pivot  \n    [lo, left)  &lt;= pivot    #Has values &lt;= pivot\n    (right, hi] &gt;= pivot\n    [pivot|--- &lt;=pivot-----|-----Undetermined-------|---&gt;=pivot----]\n                            left               right\n    After execution of while loop : \n    [pivot|----&lt;=pivot------|-----&gt;=pivot------]\n    lo                  right                  hi\n    [lo,right] &lt;= pivot\n    After Putting pivot in sorted position : \n    [----&lt;=pivot----|pivot|-----&gt;=pivot------]\n    lo               right                  hi\n    Finally, return right.\n    \"\"\"\n    #PIVOT SELECTION\n    #Pick a random pivot index and always keep pivot at index lo\n    #NB:  random.randint(0,0) is 0.\n    pivot_index = random.randint(lo,hi)\n    arr[lo],arr[pivot_index] =  arr[pivot_index],arr[lo]\n    #read pivot value\n    pivot = arr[lo]\n\n    #PARTITIONING\n    #partition [lo+1,hi] ; \n    #NB : when lo == hi , while loop will not be executed\n    left,right = lo+1, hi\n    while left&lt;=right:\n        #Move left ahead if arr[left] is strictly less than pivot value\n        while left &lt;= right and arr[left] &lt; pivot :\n            left+=1\n        #Move right to the left if it is strictly higher than pivot\n        while left &lt;= right  and arr[right] &gt; pivot :\n            right-=1\n        #Swap left and right and move pointers\n        #If both values are equal to pivot this will do a swap,move pointers and effectively leave pivot values where they are. \n        if left &lt;=  right :\n            arr[left], arr[right] = arr[right], arr[left]\n            right-=1\n            left+=1\n    #Put pivot in sorted position\n    arr[lo], arr[right] = arr[right], arr[lo]\n    print(f\"Pivot Value: {pivot}  ,Partitioned list : {arr}, Partition Index = {right}\")\n    return right\n\nif __name__ == \"__main__\" : \n    # nums= [1,2,3]\n    # partition(nums, 0, len(nums)-1)\n    # nums = [2,2,2,2]\n    # partition(nums, 0, len(nums)-1)\n    # nums = [2,4,5,1,4,8,9]\n    # partition(nums, 0, len(nums)-1)\n    # nums= [3,2]\n    # partition(nums, 0, len(nums)-1)\n    # nums= [3]\n    # partition(nums, 0, len(nums)-1)\n    nums = [5,2,1,1,1,1,1,1,1,1,1,5,5,-3,-2,-5]\n    partition(nums, 0, len(nums)-1)\n</code></pre>"},{"location":"Algorithms/Array_Partitioning/Hoare/#important-things-to-note","title":"Important things to Note:","text":"<ul> <li> <p>This algorithm is not stable. The relative order of elements will not be preserved. Naive partitioning using extra space is the only algorithm which preserves relative order of elements.</p> </li> <li> <p>When used in quicksort/quickselect, instead of fixed pivot selection either random pivot selection or selection algorithms like Median of Medians are used to pick the pivot.  </p> </li> </ul>"},{"location":"Algorithms/Array_Partitioning/Hoare/#additional-resources","title":"Additional Resources","text":"<ol> <li>ALGS4 - Sedgewick </li> </ol>"},{"location":"Algorithms/Array_Partitioning/In-Place_Deduplication/","title":"In-Place Deduplication","text":""},{"location":"Algorithms/Array_Partitioning/In-Place_Deduplication/#in-place-deduplication","title":"In-Place Deduplication","text":""},{"location":"Algorithms/Array_Partitioning/In-Place_Deduplication/#in-place-deduplication-of-sorted-array","title":"In-Place deduplication of Sorted Array","text":""},{"location":"Algorithms/Array_Partitioning/In-Place_Deduplication/#defining-the-problem","title":"Defining the Problem","text":"<p>The task is to partition a sorted array s.t the left partition contains only unique elements. The relative order of elements must be maintained. The function should return the length of the left partition. (Leetcode Link)</p> Example <pre><code>Example 1:\n\nInput: nums = [1,1,2]\nOutput: 2, nums = [1,2,_]\nExplanation: Your function should return k = 2, with the first two elements of nums being 1 and 2 respectively.\nIt does not matter what you leave beyond the returned k (hence they are underscores).\nExample 2:\n\nInput: nums = [0,0,1,1,1,2,2,3,3,4]\nOutput: 5, nums = [0,1,2,3,4,_,_,_,_,_]\nExplanation: Your function should return k = 5, with the first five elements of nums being 0, 1, 2, 3, and 4 respectively.\nIt does not matter what you leave beyond the returned k (hence they are underscores).\n</code></pre>"},{"location":"Algorithms/Array_Partitioning/In-Place_Deduplication/#algorithm","title":"Algorithm","text":"<p>Key Ideas</p> <ul> <li>[0,s] - Is the inclusive interval containing only non-duplicate elements</li> <li>(s,f) - Is the exclusive interval containing duplicates from [0,s]</li> </ul> <p>Initialize s=0 and f=0.</p> <p>s+1 is the write position for the next unique element. f scouts ahead for the next unique element.</p> <p>The key insight is to compare f with s. Since the array is sorted, all elements between s and f that equal arr[s] are duplicates. </p> <ul> <li> <p>s stays at the inclusive right boundary of the interval with no duplicates.</p> </li> <li> <p>To maintain this invariant, f moves forward to find the next unique element which should be placed at s+1. </p> </li> <li> <p>This means if s and f are equal, we move only f ahead to find the next unique element.</p> </li> <li> <p>Once a swap is made the interval with duplicates increases by 1 so s is incremented and Invariant 1 is maintained.</p> </li> </ul> <p>Notice that f is incremented in both cases. But s is only incremented if a swap is made.</p> <p>Algorithm Steps:</p> <ol> <li>Initialize s=0, f=0 (both pointing to first element)</li> <li> <p>For each position f:</p> <ul> <li> <p>If nums[f] \u2260 nums[s]:</p> <ul> <li>Write nums[f] to position s+1</li> <li>Increment s</li> </ul> </li> </ul> </li> <li> <p>Return s+1 as the length of valid partition</p> </li> </ol> <p>Initially,</p> <ul> <li>s,f = 0,0</li> <li>[0,0] - Has only 1 element. First Invariant is trivially True.</li> <li>(0,0) - Empty set. Second Invariant is trivially True.</li> </ul> <p>Maintenance</p> <p>In the loop we will maintain the invariants. Here is the dry run:</p> <p>DRY RUN</p> <pre><code>    [ 1  1  1  2  2  3  4  5]\n      sf                      # s and f are equal, only f moves forward  \n      s  f                    # ----\"----\n      s     f                 # ----\"----\n      s        f              # ----\"----\n    [ 1  2  1  1  2  3  4  5] # s and f are not equal ; f is at next unique element; Swap s+1 with f and move both pointers forward\n         s        f           # s and f are equal, only f moves forward\n         s           f        # s and f are not equal ;f is at next unique element; Swap s+1 with f and move both pointers forward\n    [ 1  3  1  1  2  2  4  5]\n            s           f     # s and f are not equal ;f is at next unique element; Swap s+1 with f and move both pointers forward\n    [ 1  3  4  1  2  2  1  5]\n               s           f  # s and f are not equal ;f is at next unique element; Swap s+1 with f and move both pointers forward \n    [ 1  3  4  5  2  2  1  1]\n                  s           f  # Final state after loop ends     \n\n    Notice that (s,f) always contains duplicates of elements from [0,s]\n</code></pre> <p>Code</p> <pre><code>class Solution:\n    def removeDuplicates(self, nums: List[int]) -&gt; int:\n        s =0\n        for f in range(len(nums)) :\n            if nums[s] != nums[f] :\n                nums[s+1] , nums[f] = nums[f],nums[s+1]\n                s+=1\n        #[0,s] is the interval with no duplicates\n        k = s+1 # k is the number of elements in the first interval\n        return k \n</code></pre>"},{"location":"Algorithms/Array_Partitioning/In-Place_Deduplication/#complexity","title":"Complexity","text":"<ul> <li><code>O(n) time complexity</code></li> <li><code>O(1) Space Complexity</code></li> </ul>"},{"location":"Algorithms/Array_Partitioning/In-Place_Deduplication/#in-place-deduplication-with-unique-elements-appearing-at-most-twice","title":"In-Place deduplication with Unique Elements appearing at most Twice","text":""},{"location":"Algorithms/Array_Partitioning/In-Place_Deduplication/#defining-the-problem_1","title":"Defining the Problem","text":"<p>Partition a sorted array s.t. the left partition contains at most two duplicates for each value. The relative order should be maintained. </p> <p>More formally, if there are k elements after removing the duplicates, then the first k elements of nums should hold the final result. It does not matter what you leave beyond the first k elements.</p> <p>This is similar to the first problem with a relaxed constraint that allows one duplicate in the left partition.(Leetcode Link)</p> Example <pre><code>Example 1:\n\nInput: nums = [1,1,1,2,2,3]\nOutput: 5, nums = [1,1,2,2,3,_]\nExplanation: Your function should return k = 5, with the first five elements of nums being 1, 1, 2, 2 and 3 respectively.\nIt does not matter what you leave beyond the returned k (hence they are underscores).\nExample 2:\n\nInput: nums = [0,0,1,1,1,1,2,3,3]\nOutput: 7, nums = [0,0,1,1,2,3,3,_,_]\nExplanation: Your function should return k = 7, with the first seven elements of nums being 0, 0, 1, 1, 2, 3 and 3 respectively.\nIt does not matter what you leave beyond the returned k (hence they are underscores).\n</code></pre>"},{"location":"Algorithms/Array_Partitioning/In-Place_Deduplication/#algorithm_1","title":"Algorithm","text":"<p>This problem is all about defining the invariant correctly. Let's dig into the solution.</p> <p>Key Ideas</p> <ul> <li>[0,s] - Is the inclusive interval containing no more than two duplicate</li> <li>(s,f) - Is the exclusive interval containing duplicates from [0,s]</li> </ul> <p>Initialize s=1 and f=2.</p> <p>s+1 is the write position for the next valid element. f scouts for the next valid element which can be placed at s+1.</p> <p>The key insight is to look two positions back to identify when we've seen more than two of the same element. Since the array is sorted, we can identify duplicates by comparing with the element at position s-1. We know that if arr[f] == arr[s-1] -&gt; arr[f] == arr[s] as well.</p> <p>If it's same as s-1, we've seen this element more than twice, therefore only f moves forward and s stays where it is.</p> <p>If it is different, then it is the next element which belongs at position s+1. Swap s+1 with f and move both s and f forward.</p> <p>Algorithm Steps:</p> <ol> <li>Handle edge case of array with length \u2264 1</li> <li>Initialize s=1, accepting first two elements by default</li> <li>For each position f starting from 2:</li> <li>If nums[f] \u2260 nums[s-1]:<ul> <li>Write nums[f] to position s+1</li> <li>Increment s</li> </ul> </li> <li>Return s+1 as the length of valid partition</li> </ol> <p>Warning</p> <p>#s,f= 1,1 # Cannot Initialize like this. f has to be s+1 initially for swapping to work correctly. If f=s, you will end up putting s ahead of f initially if f !=s-1, since we are swapping s+1 with f. Besides if (s,f) is an exclusive interval, f should be initialized to s+1.</p> <p>Initially,</p> <ul> <li>s,f = 1,2</li> <li>[0,1] - Has only two elements. First Invariant is Trivially True.</li> <li>(1,2) - Empty set. Second Invariant is Trivially True.</li> </ul> <p>Maintenance</p> <p>In the loop we will maintain the invariants. Here is the dry run: </p> <p>DRY RUN</p> <pre><code>[1,1,1,2,2,3] # everything before s including s is good\n   s f        # (s,f) empty,  f = s-1 . Only f moves ahead, After f moves ahead : (s,f) will contain the excess 1. 1 is a duplicate wrt to elements in [0,1]. This works because array is sorted. We know that if arr[f] == arr[s-1] -&gt; arr[f] == arr[s] as well..\n   s   f      # If we move f ahead now, it will contain 2 which is not a duplicate,swap s+1 with f and move both ahead to maintain both invariants \n[1,1,2,1,2,3] # swapped s+1 with f and s+=1, f+=1\n     s   f    # f!=s-1 : swap s+1 with f\n[1,1,2,2,1,3] #swapped s+1 with f and s+=1,f+=1\n       s   f  # f!=s-1  : swap f with s+1 \n[1,1,2,2,3,1] #swapped s+1 with f and s+=1,f+=1\n         s   f\n</code></pre> <p>Code</p> <pre><code>class Solution:\n    def removeDuplicates(self, nums: List[int]) -&gt; int:\n        #Edge case - nums has only one element\n        if len(nums) == 1 :\n            return 1\n\n        #Initialization: s=1 and f=2\n        s=1\n        for f  in range(2,len(nums)) :\n            #Maintenance\n            if nums[f] != nums[s-1] :\n                nums[s+1],nums[f] = nums[f],nums[s+1]\n                s+=1\n        return s+1\n</code></pre>"},{"location":"Algorithms/Array_Partitioning/In-Place_Deduplication/#complexity_1","title":"Complexity","text":"<ul> <li><code>O(n) time complexity</code></li> <li><code>O(1) Space Complexity</code></li> </ul>"},{"location":"Algorithms/Array_Partitioning/In-Place_Deduplication/#comparison-with-lomutos-partition","title":"Comparison with Lomuto's Partition","text":"<p>The deduplication algorithm looks awfully similar to Lomuto, but there are subtle and crucial differences.While both algorithms use two-pointer techniques, their invariants reveal fascinating differences in how they handle elements:</p>"},{"location":"Algorithms/Array_Partitioning/In-Place_Deduplication/#interval-types-and-their-significance","title":"Interval Types and Their Significance","text":"<p>A key distinction lies in how these algorithms define their intervals:</p> <p>Lomuto's Partition: Uses half-open intervals</p> <ul> <li><code>[lo,s)</code> \u2264 pivot: Excludes s</li> <li><code>[s,f)</code> &gt; pivot: Excludes f</li> <li>This makes sense because s acts as a boundary between two partitions</li> <li>Next valid element (\u2264 pivot) is written at position s</li> </ul> <p>Deduplication: Uses closed interval for primary invariant</p> <ul> <li><code>[0,s]</code>: Includes s</li> <li>This is necessary because s marks the last valid unique element</li> <li>The next unique element will be placed at s+1</li> </ul> <p>Deduplication needs s to be part of the valid region since it represents the last confirmed unique element</p>"},{"location":"Algorithms/Array_Partitioning/In-Place_Deduplication/#deduplication-as-a-lomuto-variant","title":"Deduplication as a Lomuto Variant","text":"<p>The deduplication algorithm can be viewed as a specialized variant of Lomuto's partition where:</p> <ol> <li> <p>Pivot Selection: Instead of a fixed pivot, we use a \"moving pivot\" (nums[s]) that changes as we progress</p> </li> <li> <p>Partition Criteria: Rather than comparing with a fixed value:</p> <ul> <li>Lomuto: Elements \u2264 pivot go left</li> <li>Deduplication: Elements \u2260 current value go to next position</li> </ul> </li> <li> <p>Pointer Movement: </p> <ul> <li>Both use s as a boundary of the \"accepted\" elements</li> <li>Both increment f to scan through remaining elements</li> <li>Both swap when their condition is met</li> </ul> </li> </ol> <p>The key insight is that deduplication adapts Lomuto's two-pointer partitioning strategy but:</p> <ul> <li>Uses equality comparison instead of less-than</li> <li>Updates its \"pivot\" value after each partition</li> <li>Requires input to be sorted </li> </ul> <p>This connection explains why both algorithms share similar pointer movement patterns despite serving different purposes.</p>"},{"location":"Algorithms/Array_Partitioning/Introduction/","title":"Introduction","text":""},{"location":"Algorithms/Array_Partitioning/Introduction/#introduction","title":"Introduction","text":"<p>Array partitioning is the problem of rearranging the elements of an array according to some predicate. </p> <p>We are interested here, firstly, in array partitioning schemes usually applied in quicksort, particularly Hoare's and Lomuto's partitioning schemes. These are two way partitioning schemes, which is to say that they have a binary predicate which could place elements in either left or right partitions based on comparison with a pivot value. There are different ways of implementing two way paritioning based on how you define the predicate. We will see this in some depth while understanding Hoare Partitioning.</p> <p>If we instead define a ternary predicate which puts an element in one of three partitions : left , middle or right based on comparison with a pivot value, it is called three way partitioning. All the duplicates are grouped into a middle partition.  Three way partitioning is also called the Dutch National Flag Problem, because the flag has three color bands. There are again different algorithms for doing this, one of which is Djikstra's, who was the one to name this as the Dutch National Flag problem. On the same pattern, the two way partitioning problem is sometimes called the Polish National Flag problem, because that flag has two color bands.</p> <p>Tip</p> <p>Once you understand Partitioning you get two other Algorithms for free : Quicksort and Quickselect.</p> <p>Paritioning Schemes are the actual meat of the Quicksort (and Quickselect)  algorithms. What makes Quicksort so interesting is that it can be implemented in slightly different ways with very different performance characteritics using any of these partitioning schemes. This is why Quicksort is not really a single algorithm but a family of algorithms and is a research area by itself in Computer Science.</p>"},{"location":"Algorithms/Array_Partitioning/Lomuto/","title":"Lomuto Partition","text":""},{"location":"Algorithms/Array_Partitioning/Lomuto/#lomuto-partition","title":"Lomuto Partition","text":""},{"location":"Algorithms/Array_Partitioning/Lomuto/#defining-the-problem","title":"Defining the problem","text":"<p>Let us consider the simplest case, that of an array of integers.</p> <p>Given an array of integers, rearrange the elements such that the left part contains elements less than or equal to a pivot value and the right part contains values greater than the pivot value.</p>"},{"location":"Algorithms/Array_Partitioning/Lomuto/#the-algorithm","title":"The Algorithm","text":"<p>The Lomuto partition uses two forward iterators to maintains two invariants :</p> <p>[lo,s) contains only elements less than or equal to the pivot.  # Left closed Right open interval</p> <p>[s,f) contains elements larger than the pivot.      # Left closed right open interval</p> <p>Usually, pivot is fixed at hi. </p> <p>Initial conditions are :</p> <p>s, f = 0,0</p> <p>Both invariants are trivially true initially.</p> <p>During execution :</p> <p>Pointers</p> <pre><code>[---&lt;=pivot----|---&gt;pivot------|---Unexamined---|pivot] \nlo              s               f                    hi \n</code></pre> <p>s and f can be thought of as slow and fast pointers with s at the write position and f at the read position.</p> <p>The algorithm is </p> Code <pre><code>def partition(arr,lo,hi) :\n    s = lo\n    pivot = arr[hi]\n    for f in range(lo,hi+1) : \n         if arr[f] &lt;= pivot : \n              arr[s],arr[f] = arr[f],arr[s] #swap s and f\n              s+=1\n    return s-1      \n</code></pre> <p>Since [s,f] is a half closed interval, it is empty if  s and f are equal. If there is a gap, this interval is gauranteed to only contain elements greater than pivot, since a gap INCREASES as a result of f pointing to a value less than or equal to the pivot and s pointing to a value greater than pivot,  in the first place.</p> Dry Run <pre><code>General case :\n     [ 1,  4,  7,  3,  1,  4 ]\n0      sf                      # Initial condition, pivot = 4, arr[f] &lt;= 4 is True,  s will be swapped with f and both will move ahead\n1          sf                  # arr[f] &lt;= 4 , swap s with f and move both ahead\n2              sf              # arr[f] &lt;=4 is False, s stays where it is and only f moves ahead\n3              s  f            # arr[f] &lt;=4 is True, swap s with f and move both ahead\n4    [ 1,  4,  3,  7,  1,  4 ]\n                   s   f       # arr[f] &lt;=4 is True, swap s with f and move both ahead\n5    [ 1,  4,  3,  1,  7,  4 ] # arr[f] &lt;=4 is True, swap s with f and move both ahead\n                       s   f\n6    [ 1,  4,  3,  1,  4,  7 ] # arr[f] &lt;=4 is True, swap s with f and move both ahead\n                           s   # For loop ends\nReturn s -1\n\nOnly one element :\n  [4]\n0  sf      #arr[f] &lt;= f is True, swap s with f and move both ahead\n1    sf\nReturn s-1\n\nSorted array \n  [1,2,3]\n0  sf           \n1    sf\n2      sf\n3        s\n\nreturn s-1\n\nReverse sorted array :\n  [ 3, 2, 1 ]\n0   sf               #Initial conditions, arr[s] &lt;= 1 is False, only f will move ahead\n1   s  f             #arr[s] &lt;= 1 is False, only f will move ahead          \n2   s     f          #arr[s] &lt;= 1 is True, s and f will be swapped and both will move ahead \n3 [ 1, 2, 3 ]\n       s     f       #for loop ends\nreturn s -1 \n\nReverse sorted array with duplicate keys:\n[3, 2 ,1, 1]\n s     f\n[1, 2 ,3, 1]\n    s     f\n[1, 1 ,3, 2]\n       s     f   # for loop ends, \nreturn s-1 \n\nDuplicate keys :\n[1 , 1, 1, 1]\n             sf\nreturn s-1\n</code></pre>"},{"location":"Algorithms/Array_Partitioning/Lomuto/#variant-implementation","title":"Variant Implementation","text":"<p>We could redefine our problem statement slightly : Given an array of integers, rearrange the elements such that the left part contains elements less  a pivot value and the right part contains values greater than or equal to the pivot value.</p> <p>[lo,s) only contains elements strictly less than pivot [s,f)  contains elements equal to or greater than pivot.</p> <p>Initial conditions are :</p> <p>s, f = 0,0</p> <p>Both invariants are trivially true initially.</p> <p>During execution :</p> <p>Pointers</p> <pre><code>[---&lt;pivot----|---&gt;=pivot------|---Unexamined---|pivot] \nlo             s                f                    hi \n</code></pre> <p>The algorithm is </p> Code <pre><code>def partition(arr,lo,hi) :\n    s = lo\n    pivot = arr[hi]\n    # The range might as well be range(lo,hi+1). It would make no difference except one extra redundant comparsion.\n    for f in range(lo,hi) : \n         if arr[f] &lt; pivot : \n              arr[s],arr[f] = arr[f],arr[s] #swap s and f\n              s+=1\n    arr[s],arr[hi] = arr[hi],arr[s]\n    return s      \n</code></pre> <p>When for loop exits, s is positioned at the sorted position of pivot. We swap pivot with s and return s.</p> Dry Run <pre><code>[ 1,  4,  7,  3,  1,  4 ]\n      s       f             # arr[f] &lt; pivot is True, s and f will be swapped and both will move ahead.\n[ 1,  3,  7,  4,  1,  4 ]    \n          s       f\n[ 1,  3,  1,  4,  7,  4 ]\n              s       f\n[ 1,  3,  1,  4,  7,  4 ]\n                  s       f  # for loop ends\n[ 1,  3,  1,  4,  4,  7 ]\n                  s       f  # FInally Swap s with f\nReturn s \n\n\nReverse sorted array :\n  [ 3, 2, 1 ]\n    s        f \n  [ 1, 2, 3 ]\n    s        f\nreturn s\n\nReverse sorted array with duplicate keys:\n[3, 2 ,1, 1]\n s          f\nreturn s \n\nDuplicate keys :\n1  1  1  1 \ns          f #swap s[0] with s[-1]\nreturn s \n</code></pre> <p>Remember</p> <p>The two implementations behave differently when it comes to duplicate keys :</p> <p>The first implementation puts one instance of the pivot value in the LAST sorted position. The second implementation puts one instance of the pivot value in the FIRST sorted position.</p>"},{"location":"Algorithms/Array_Partitioning/Lomuto/#important-things-to-note","title":"Important things to note.","text":"<p>Tip</p> <p>With quickselect or quicksort, using Hoare partition with a weak condition is preferable to Lomuto.</p> <p>Lomuto partition has the same problem as Hoare partition with a strict condition when it somes to duplicate keys. It degenrates to O(n^2). Also, performs more swaps.</p> <p>Tip</p> <p>Lomuto is semi-stable. Some 'easy' interview style problems can be solved only if you know Lomuto and this property of the algorithm.</p> <p>Also, since it uses two forward iterators, it can be applied to singly linked lists.</p>"},{"location":"Algorithms/Array_Partitioning/Lomuto/#additional-resources","title":"Additional Resources","text":"<ol> <li>Sorting Lecture Notes</li> <li>Lomuto Partition Scheme</li> <li>Partition Notes by Stepanov</li> <li>Lomuto's Comeback</li> <li>Array Partition Blog</li> <li>Quick Sort Notes</li> </ol>"},{"location":"Algorithms/Backtracking/","title":"Backtracking","text":""},{"location":"Algorithms/Backtracking/#backtracking","title":"Backtracking","text":"<p>Backtracking seems confusing and difficult to learn, because there are many different ways of doing the same thing.</p> <p>Here, we solve the same problem in many different ways only to see the possibilities.</p> <p>There is no staisfying and clear definition of backtracking I have found yet. I like to define it as an exhaustive search technique where you make a choice, explore that choice and then undo the choice (backtrack) to make another choice. </p> <p>This act of making a choice and undoing it is backtracking.</p> <p>It looks very much like a depth first tree traversal (or graph) traversal with state management.</p>"},{"location":"Algorithms/Backtracking/#problem-generate-n-bit-binary-numbers","title":"Problem: Generate N-bit Binary Numbers","text":"<p>Given an integer n, generate all possible binary numbers of length n and return them as a list of strings.</p>"},{"location":"Algorithms/Backtracking/#examples","title":"Examples:","text":"<pre><code>Input: n = 2\nOutput: [\"00\", \"01\", \"10\", \"11\"]\n\nInput: n = 3\nOutput: [\"000\", \"001\", \"010\", \"011\", \"100\", \"101\", \"110\", \"111\"]\n</code></pre>"},{"location":"Algorithms/Backtracking/#constraints","title":"Constraints:","text":"<ul> <li>1 \u2264 n \u2264 16</li> <li>The result list should be in lexicographically sorted order</li> <li>Each binary number in the output should be exactly n digits long (pad with leading zeros if necessary)</li> </ul>"},{"location":"Algorithms/Backtracking/#implementation-approaches","title":"Implementation Approaches","text":"<p>Approach 0: Pure Recursion</p> <p>Uses recursive decomposition without backtracking. Each recursive call returns its own result list. </p><pre><code>\"\"\"\nRecursive Decomposition- There is no need to maintain a path in this implementation. Therefore, no backtracking is required.\n\"\"\"\ndef generate(n) : \n    \"\"\"\n    Eg : For n = 3 , We have 3 spots each of which can be filled with 0 or 1.\n\n    -    -   -\n    1/0\n         1/0  \n             1/0\n\n    1 1 1\n    1 1 0\n    1 0 1\n    1 0 0\n    0 1 1\n    0 1 0\n    0 0 1\n    0 0 0 \n    \"\"\"\n    if n == 0 : \n        return [\"\"]\n    # Recursively generate the binary numbers for n-1 bits\n    # generate(n-1) returns a list of binary numbers of length n-1\n    # We will take each of these binary numbers and prepend '1' and '0' to each of them\n    l = [ \"1\" + _ for _ in  generate(n-1) ] \n    r = [ \"0\" + _ for _ in  generate(n-1) ]\n\n    print(f\"n = {n} , l = {l} , r = {r}\")\n    #Concatenate the two lists\n    return l + r  \n\nprint(generate(3))\n</code></pre><p></p> <p>This approach: - Uses pure recursive decomposition - Each call builds its own result independently - No need for global state or backtracking - More intuitive but less memory efficient - Time: O(2^n), Space: O(2^n) due to string copies</p> <p>Approach 1: Pre-allocated Array</p> <p>Uses a fixed-size array with direct index assignments. Most memory efficient for known size problems. </p><pre><code>\"\"\"\nBacktracking Implementation Using Pre-allocated Array Strategy\nKey Features:\n- Uses fixed-size array instead of growing/shrinking structure\n- No push/pop operations needed\n- Memory efficient as array size is known upfront\n- Implicit undoing of choices through overwriting\n\"\"\"\ndef generate(n) : \n    \"\"\"\n    Implementation Notes:\n    1. Pre-allocated array of size 'n' with None values\n    2. Each position directly overwritten with 0 or 1\n    3. No need for explicit undo operations\n    4. More efficient than stack-based approaches for known size problems\n\n    Memory Layout (n=3):\n    Initial: [None, None, None]\n    During:  ['0'/'1', '0'/'1', '0'/'1']\n\n    Time Complexity: O(2^n) - must generate all binary numbers\n    Space Complexity: O(n) - fixed size array, no additional growth\n    \"\"\"\n\n    # Pre-allocate array of size n - more efficient than growing/shrinking a list\n    path = [None] * n\n    res = []\n\n    #choose  0 or 1 at idx\n    def backtrack(idx) : \n        if idx == len(path): \n            res.append(\"\".join(path[:]))\n            return \n        # Direct assignment to index - no push/pop needed\n        path[idx] = '0'\n        backtrack(idx+1)\n        # No explicit undo needed - next assignment overwrites\n\n        path[idx] = '1'\n        backtrack(idx+1)\n        # No explicit undo needed at end - array will be reused\n\n    backtrack(0)\n    return res\n\nprint(generate(4))\n</code></pre><p></p> <p>Approach 2: Stack-based with Dual Recursion</p> <p>Uses explicit push/pop operations with two separate recursive calls for '0' and '1'. </p><pre><code>\"\"\"\nGenerate n-bit binary numbers using backtracking.\nImplementation Strategy: Stack-based Backtracking \n- Uses list as stack for building binary numbers\n- For each position, we make TWO recursive calls because:\n  1. First recursive call explores path after choosing '0'\n  2. Second recursive call explores path after choosing '1'\n- This creates a binary tree where each node represents a digit position\n  and has exactly two children (one for '0' and one for '1')\n\"\"\"\ndef generate(n) : \n    \"\"\"\n    Decision Tree Shows Two Branches at Each Level:\n    Root\n    \u251c\u2500\u2500 0 (First recursive call)\n    \u2502   \u251c\u2500\u2500 00\n    \u2502   \u2514\u2500\u2500 01\n    \u2514\u2500\u2500 1 (Second recursive call)\n        \u251c\u2500\u2500 10\n        \u2514\u2500\u2500 11\n\n    Two Recursive Calls Pattern:\n    1. First Call (with '0'): Explores all possibilities starting with '0'\n    2. Second Call (with '1'): Explores all possibilities starting with '1'\n\n    This ensures we generate ALL possible combinations systematically\n    \"\"\"\n\n    path = [] #We will treat this as a stack in this implementation\n    res = []\n\n    #choose  0 or 1 at idx\n    def backtrack(idx) : \n        if idx == n: \n            res.append(\"\".join(path[:]))\n            return \n\n        # First Recursive Branch: Try '0' at current position\n        path.append('0')\n        backtrack(idx+1)  # Explore all possibilities after choosing '0'\n        path.pop()        # Backtrack by removing '0'\n\n        # Second Recursive Branch: Try '1' at current position\n        path.append('1')\n        backtrack(idx+1)  # Explore all possibilities after choosing '1'\n        path.pop()        # Backtrack by removing '1'\n\n    backtrack(0)\n    return res\n\nprint(generate(3))\n</code></pre><p></p> <p>Approach 3: For-loop Choices</p> <p>Uses iteration over choices, making it more extensible for problems with multiple choices. </p><pre><code>\"\"\"\nGenerate n-bit binary numbers using backtracking with for-loop iteration.\nImplementation Strategy: For-loop Based Choice Selection\n- Uses pre-allocated array like version 2\n- Iterates over choices using for-loop instead of explicit recursive calls\n- More generalizable pattern that can handle multiple choices beyond just binary\n- Common pattern in backtracking problems where we iterate over available choices\n\"\"\"\ndef generate(n) : \n    \"\"\"\n    Key Differences from Other Versions:\n    1. Uses for-loop to iterate over choices ['0','1']\n    2. Single recursive call instead of multiple explicit ones\n    3. More extensible - easy to add more choices if needed\n    4. Common pattern seen in other backtracking problems like:\n       - Generating permutations (choices are remaining numbers)\n       - N-Queens (choices are valid positions)\n       - Sudoku (choices are valid digits)\n\n    Memory Layout:\n    - Pre-allocated array: [None] * n\n    - Choices handled by for-loop: for choice in ['0','1']\n    - Implicit undo through overwriting\n    \"\"\"\n\n    path = [None] * n\n    res = []\n\n    #choose  0 or 1 at idx\n    def backtrack(idx) : \n        if idx == len(path): \n            res.append(\"\".join(path[:]))\n            return \n        for choice in ['0','1'] : \n            path[idx] = choice\n            backtrack(idx+1)\n            #Here we dont care if path[idx] overwritten,\n            #Undo of choice is implicit\n\n    backtrack(0)\n    return res\n\nprint(generate(3))\n</code></pre><p></p> <p>Approach 4: Dynamic Stack with For-loop</p> <p>Combines dynamic stack operations with for-loop choice iteration for clearer state management. </p><pre><code>\"\"\"\nBacktracking with Dynamic Stack &amp; For-loop Choices\nKey Features:\n- Combines dynamic stack operations (like version 3) with for-loop choice iteration (like version 4)\n- Explicit undo operations through pop() (unlike version 2 &amp; 4's implicit overwrites)\n- More verbose but clearer state management\n- Shows complete choice-explore-undo cycle explicitly\n\"\"\"\ndef generate(n) : \n    \"\"\"\n    Implementation Characteristics:\n    1. Dynamic Growth: path grows/shrinks as we make/undo choices\n    2. Explicit State Management:\n       - Choice: path.append(choice)\n       - Explore: backtrack(idx+1)\n       - Undo: path.pop()\n    3. Uses for-loop for choices (more flexible than dual recursion)\n    4. Memory Usage: O(n) but with dynamic resizing overhead\n\n    Choice-Explore-Undo Pattern:\n    For each position:\n        For each choice (0,1):\n            1. Make choice (append)\n            2. Explore further (recurse)\n            3. Undo choice (pop)\n    \"\"\"\n\n    path = []  # We will treat this like a stack in this implementation\n    res = []\n\n    #choose  0 or 1 at idx\n    def backtrack(idx) : \n        if idx == n: \n            res.append(\"\".join(path[:]))\n            return \n        for choice in ['0','1'] : \n            # State Change: Add choice to current path\n            path.append(choice)\n            # Explore: Recurse with this choice in place\n            backtrack(idx+1)\n            # State Restoration: Remove choice before next iteration\n            path.pop()  # Explicit undo - different from array overwrite versions\n\n    backtrack(0)\n    return res\n\nprint(generate(3))\n</code></pre><p></p>"},{"location":"Algorithms/Backtracking/#comparison-of-approaches","title":"Comparison of Approaches","text":"Approach Memory Usage State Management Extensibility Code Clarity Pure Recursion O(2^n) None (functional) Limited Excellent Pre-allocated Array Most efficient (O(n) fixed) Implicit (overwrites) Limited Good Stack with Dual Recursion O(n) with resizing Explicit (push/pop) Limited Very Good For-loop Choices O(n) fixed Implicit (overwrites) Excellent Good Dynamic Stack with For-loop O(n) with resizing Most explicit Excellent Excellent <p>Key Takeaways</p> <ol> <li>Pure Recursion: Intuitive but least memory efficient. No bactracking involved.</li> <li>Pre-allocated Array: Best for memory efficiency when size is known</li> <li>Stack with Dual Recursion: Most intuitive for binary choices</li> <li>For-loop Choices: Most extensible for varying number of choices</li> <li>Dynamic Stack with For-loop: Best balance of clarity and flexibility</li> </ol>"},{"location":"Algorithms/Basic_Recursion/","title":"Basic Recursion","text":""},{"location":"Algorithms/Basic_Recursion/#basic-recursion","title":"Basic Recursion","text":"<p>Recursion can be tricky to understand, especially when it comes to the order of operations. Here, we'll try to understand what happens when you \"do\" something before the recursive call (preorder position) or after the recursive call (postorder position). </p> <p>Debug this in your IDE and see what is happening on the stack and how function calls are stacked and unwound.</p>"},{"location":"Algorithms/Basic_Recursion/#understanding-recursive-positions","title":"Understanding Recursive Positions","text":"<ol> <li>Preorder Position: Operations performed before the recursive call</li> <li>Postorder Position: Operations performed after the recursive call</li> <li>Base Case: Condition to stop recursion</li> </ol>"},{"location":"Algorithms/Basic_Recursion/#implementation-approaches","title":"Implementation Approaches","text":"<p>Print i to 5 forwards and backwards</p> <p>Input is the starting point .Use preorder position to print in ascending order and postorder position to print in reverse order. Base Case is when i exceeds 5. </p><pre><code>\"\"\"\nPrint Numbers from i to 5.\n\"\"\"\n\ndef print_nums(i) :\n    if i &gt; 5 :\n        return\n\n    #preorder position\n    print(i)\n    #recursive call\n    print_nums(i+1)\n\n\"\"\"\nPrint Numbers from i to 5 in reverse order\n\"\"\"\n\ndef print_nums_backwards(i) :\n    if i &gt; 5 :\n        return\n\n    #recursive call\n    print_nums_backwards(i+1)\n    #postorder position\n    print(i)\n\n\nprint_nums(1)\nprint_nums_backwards(1)\n</code></pre> In preorder position, numbers are printed along with stack build up. In postorder, stack builds up before any print statement is executed and numbers are printed as the stack unwinds.<p></p> <p>Print numbers forward and backward in the range 1 to n.</p> <p>n is the input parameter to the function. Base case is when n becomes less than 1 i.e. equal to zero. Here, we have to print in postorder position to get the output in ascending order. We use preorder position to print in descending order. This is becuase we are taking the upper bound as input. Whereas, in the previous example, we took the lower bound as input.  </p><pre><code>\"\"\"\nPrint Numbers from 1 to n .\n\"\"\"\n\ndef print_nums_1_to_n(n) :\n    if n == 0 :\n        return\n    print_nums_1_to_n(n-1)\n    print(n)\n\n#print  1 to n\nprint_nums_1_to_n(5)\n\n\n\"\"\"\nPrint Numbers from n to 1 .\n\"\"\"\n\ndef print_nums_n_to_1(n) :\n    #base cases\n    if n == 0 :\n        return\n\n    print(n)\n    print_nums_n_to_1(n-1)\n\n\nprint_nums_n_to_1(5)\n</code></pre> The takeaway is that preorder position matches the input order and postorder position matches the reverse of the input order.<p></p> <p>Approach 3: Print Both Ways</p> <p>Uses both preorder and postorder positions to print numbers in both orders. </p><pre><code>\"\"\"\nPrint Numbers n to 1 and then 1 to n\n\"\"\" \ndef print_nums(n) :\n    if n == 0 :\n        print(\"Base Case Hit.\")\n        return\n    #preorder position\n    print(f\"Preorder : {n}\")\n    #recursive call\n    print_nums(n-1)\n    #postorder position\n    print(f\"Postorder : {n}\")\n\n\nprint_nums(5)\n</code></pre><p></p>"},{"location":"Algorithms/Basic_Recursion/#comparison-of-approaches","title":"Comparison of Approaches","text":"Position When Operation Happens Stack State Output Order Preorder Before recursive call During stack building Same as Inout Order Postorder After recursive call During stack unwinding Reverse of Input Order Both Before and after Both phases Both orders <p>Key Takeaways</p> <ol> <li>Operation Placement: The position of operations relative to the recursive call determines the order of execution</li> <li>Stack Behavior:<ul> <li>Stack builds up: Follows input order (n \u2192 n-1 \u2192 n-2 \u2192 ... \u2192 1)</li> <li>Stack unwinds: Reverse of input order (1 \u2192 2 \u2192 ... \u2192 n-1 \u2192 n)</li> </ul> </li> <li>Memory Usage: All approaches use O(n) stack space</li> <li>Choose Based On:<ul> <li>Same as input order \u2192 Use preorder position</li> <li>Reverse of input order \u2192 Use postorder position</li> <li>Both orders \u2192 Use both positions</li> </ul> </li> </ol>"},{"location":"Algorithms/Binary_search/Introduction/","title":"Introduction","text":""},{"location":"Algorithms/Binary_search/Introduction/#introduction","title":"Introduction","text":"<p>Note</p> <p>This article is heavily based on this awesome video by mcoding. This is perhaps the best resource on the internet to understand binary search.</p>"},{"location":"Algorithms/Binary_search/Introduction/#bisect-left","title":"Bisect Left","text":"<p>Binary search (also called bisection search) is an algorithm to find an element quickly in a sorted array.</p> <p>Example</p> <p>Suppose we are trying to find 7 in this array : <code>[2,3,3,4,6,7,8,9]</code></p> <ol> <li> <p>Is the element you want to find in the left half or the right half? \u2192 Right half</p> <p>Our search space is now only the right half.  </p><pre><code>[        6,7,8,9 ] # remaining 4\n</code></pre> 2. Repeat the process <pre><code>[        6,7     ] # remaining 2\n[          7     ] # remaining 1\n</code></pre> 3. Finally there is just one element left. That's our element.<p></p> </li> </ol> <p>At each step you cut out half of the search space. How can the idea be so simple, but the implementation be so difficult?. It's not - you just have to think about it in the right way.</p> <p>The First Decision: What Do You Want to Return? The object itself, or the index where it is at?</p> <ul> <li>The index itself is a \"more useful piece of information\". You can always just grab the object at that index later.</li> </ul> <p>What do we do when the object we are looking for is not there?</p> <ul> <li>Return -1? Raise exception? </li> <li>What if there are multiple valid indices to choose from? </li> </ul> <p>With every arbitrary choice we make, we would have to remember the implementation details for each choice.</p> <p>Implementation Complexity</p> <p>A better way to do it, is to rephrase the question, so that there is always exactly one answer.If I were going to add another element to the array, say 7, where should I put it so that it is the first 7?</p> <ul> <li>If there are no 7s in the array, this answer would be the index where you would put 7 to maintain the sorted property of the array. </li> <li>If there is at least one 7, the answer would be the index of the first 7.</li> <li>The index would be zero if the array is empty, which is not actually a valid index, but is the right answer to our Question!</li> <li>It could also be <code>(n-1) + 1</code> if the element is absent from the array and greater than all elements in a non-empty array. This is also not a valid index. But, again,it is the right answer to our Question.</li> </ul> <p>Correct Implementation Strategy</p> <p>The array being sorted is not actually the property being used in binary search. The property is that everything greater than 7 is to the right of 7 and everything less than 7 is to the left of 7.</p> <p>Here, I disturbed the order of some elements in a sorted array: <code>[2, 3, 5, 4, 6, 7, 9, 8]</code></p> <p>The array is no longer in sorted order, but notice the steps are exactly the same. What we need is that everything less than 7 is to the left of 7 and everything greater than 7 should be to the right. </p> <p>Important Insight</p> <p>In fact, the numbers can be replaced with the important information: Is this number less than 7?</p> <p>Pointers</p> <pre><code>[2, 3, 5, 4, 6, 7, 9, 8]\n T  T  T  T  T  F  F  F  #&lt;7?\n</code></pre> <p>Notice how any Trues are on the left and any Falses are on the right.</p> <p>Now that we are looking at the relevant data, where do we insert 7 so that it is the first 7?</p> <p>Notice that the answer we are looking for is the index of the first False value or the end of the array if there are no false values. This is the key insight, we need to keep in mind that makes the implementation extremely easy to remember :</p> <p>Tip</p> <p>Squint Your Eyes and Find the First False Value</p> <p>Ok, let's write the implementation:</p> <p>Bisect Left</p> <pre><code>def bisect(arr, x):\n    lo = 0\n    hi = len(arr)     \n    while lo &lt; hi:\n        mid = (lo + hi) // 2\n        if arr[mid] &lt; x:  # mid is a True value\n            lo = mid + 1\n        else:            # mid is a False Value\n            hi = mid\n    return lo\n</code></pre> <p>The first two lines inside the function, initialize lo and hi.</p> <p>Tip</p> <p>lo and hi are lower and upper bounds on where this First FALSE index is. </p> <p>If our answer is zero, it would mean that first value is False or array is empty. If it is len(arr), it would mean that all the values are True and we would have to put the new element at the end of the array.</p> <p>The idea of the while loop (line 4), is that we continuously decrease the upper bound and increase the upper bound until they are equal. lo and hi will meet at the unique index we are looking for. So, we could return lo or high, once the while loop runs, it would make no difference.</p> <p>Next, we calculate the midpoint in each iteration of the while loop. Python can represent arbitrarily large integers, so you do not have to worry about overflow. But, if you were using another language, you might have to worry about overflow. In that case, you would represent mid as <code>lo+(hi-lo)//2</code> : lo plus half the distance between hi and lo. If you do it this way, it will never overflow.</p> <p>Comparing mid with x</p> <p>Next comes the part which is often written incorrectly. We must compare whatever is at mid with our search term <code>x</code>. Remember, we are looking for the first False value where True of False is determined by the question : Is this element strictly Less than <code>x</code>:  </p><pre><code>if arr[mid] &lt; x :\n    lo = mid+1   # mid is a True Value\nelse :\n    hi =mid      # mid is a False Value\n</code></pre> If <code>mid</code> is strictly less than <code>x</code>, it is a <code>True</code> Value. Since the Value at <code>mid</code> is <code>True</code>, the earliest a <code>False</code> Value could occur is the next index which is <code>mid+1</code>. Therefore, our lower bound becomes <code>mid+1</code><p></p> <p>What if <code>mid</code> is a <code>False</code> Value? In this case the FIRST <code>False</code> Value could not be any later than mid. But it could be mid or earlier than mid. There could be other <code>False</code> values before midm. So our upper bound becomes mid.</p> <p>When the while loop is done, lo and hi meet at the index we are looking for.</p> <p>How do we know that this implementation always gives the right answer?  - We know that lo is always &lt;=  answer  - We also know that answer is always &lt;= hi  - In each iteration the difference between hi and lo always decreases by at least one.</p> <p>Thereifre, lo and hi always meet at the answer.</p> <p>But how do we know that in each iteration lo and hi come closer to each other by at least one? If we enter the True clause on line no. 6 <code>lo = mid+1</code> ensures that lo always increases by at least one.</p> <p>If we enter the False clause on lines 8,9 we are setting hi to mid. So, hopefully we are lowering hi. But how do we know mid is actually strictly less than hi?</p> <p>Here we are actually using the property that integer division rounds down. Inside the while loop: while lo &lt; hi, lo is strictly less than hi. So lo + hi is strictly less than hi + hi.</p> <p>That means for calculating mid: mid = (lo + hi) // 2</p> <p>When we divide (lo + hi) by 2 and round down we will get something strictly less than (hi + hi) // 2 i.e. hi.</p>"},{"location":"Algorithms/Binary_search/Introduction/#checking-the-return-value-of-bisect-left","title":"Checking the return value of Bisect Left","text":"<p>Since bisect left can return an out of bounds index and not necessarily the index of a matching value, you have to handle this when using it, depending on what you are trying to do.  </p> <p>Checking if x exists using bisect left</p> <pre><code>def search(arr:list, x) -&gt;int :\n    idx = bisect_left(arr,x)\n    if i != len(arr) and arr[i] == x:\n        return i\n    raise ValueError(f\"{x} not found!\")\n</code></pre>"},{"location":"Algorithms/Binary_search/Introduction/#bisect-right","title":"Bisect Right","text":"<p>But wait. Your ask: instead of returning a 7 so that it is the first seven, could I not return the index, so that it would be the last 7?</p> <p>You could. You could follow the same analysis and get a very similar algorithm.</p> <p>To find where it should go to be the first 7, the relevant piece of information is: \"Is this (current element) strictly less than 7\"</p> <p>To find where it should go to be the last element, the relevant piece of information i: \"Is this (current element) less than or equal to  7\"</p> <p>Bisect Left vs Bisect Right</p> <pre><code>[2, 3, 5, 4, 6, 7, 7, 7, 8, 9]\n[T, T, T, T, T, F, F, F, F, F] #&lt;7  ; Bisect Left\n[T, T, T, T, T, T, T, T, F, F] #&lt;=7 ; Bisect Right\n</code></pre> <p>In either case, you are looking for the first False value. You just need to decide what you mean by True or False.</p> <p>Bisect Right</p> <pre><code>def bisect_right(arr, x):\n    lo = 0\n    hi = len(arr)     \n    while lo &lt; hi:\n        mid = (lo + hi) // 2\n        if arr[mid] &lt;= x:  # mid is a True value\n            lo = mid + 1\n        else:            # mid is a False Value\n            hi = mid\n    return lo\n</code></pre>"},{"location":"Algorithms/Binary_search/Introduction/#checking-the-return-value-of-bisect-right","title":"Checking the return value of Bisect Right","text":"<p>Bisect right can return values from 0 to len(arr). If it returns zero it means that either arr is empty or all elements in the array are greater than x. Any other value could mean x exists at i-1. A return value of len(arr) means x is greater than or equal to all elements in the array.</p> <p>Checking if x exists using bisect right</p> <pre><code>def search(arr:list, x) -&gt;int :\n    idx = bisect_right(arr,x)\n    if i &gt; 0 and arr[i-1] == x:\n        return i\n    raise ValueError(f\"{x} not found!\")\n</code></pre>"},{"location":"Algorithms/Binary_search/Introduction/#summary","title":"Summary","text":"<p>Key Takeaways</p> <ul> <li>Binary search is fundamentally about finding boundaries in bisected data</li> <li>Reframe the problem as \"find the first False value\" </li> <li>The difference between <code>bisect_left</code> and <code>bisect_right</code> is just changing <code>&lt;</code> to <code>\u2264</code></li> <li>Always think about what \"True\" and \"False\" mean in your specific context</li> </ul>"},{"location":"Algorithms/Binary_search/Median_of_two_sorted_arrays/","title":"Median of two sorted arrays","text":""},{"location":"Algorithms/Binary_search/Median_of_two_sorted_arrays/#median-of-two-sorted-arrays","title":"Median of two sorted arrays","text":"<p>This is one of those problems which I like to call an \"Algorithm in itself\". Although categorized under Binary Search, its not just a straightforward application of it.</p> <p>Description</p> <p>Leetcode Link </p> <pre><code>Given two sorted arrays nums1 and nums2 of size m and n respectively, return the median of the two sorted arrays.\n\nThe overall run time complexity should be O(log (m+n)).\n\n\n\nExample 1:\n\nInput: nums1 = [1,3], nums2 = [2]\nOutput: 2.00000\nExplanation: merged array = [1,2,3] and median is 2.\nExample 2:\n\nInput: nums1 = [1,2], nums2 = [3,4]\nOutput: 2.50000\nExplanation: merged array = [1,2,3,4] and median is (2 + 3) / 2 = 2.5.\n\n\nConstraints:\n\nnums1.length == m\nnums2.length == n\n0 &lt;= m &lt;= 1000\n0 &lt;= n &lt;= 1000\n1 &lt;= m + n &lt;= 2000\n-106 &lt;= nums1[i], nums2[i] &lt;= 106\n</code></pre> <p>Let us figure out how binary search can be applied to solve this problem..</p>"},{"location":"Algorithms/Binary_search/Median_of_two_sorted_arrays/#what-is-the-data","title":"What is the data?","text":"<p>nums1 = [1,2], nums2 = [3,4]</p> <p>Two sorted arrays.</p>"},{"location":"Algorithms/Binary_search/Median_of_two_sorted_arrays/#what-is-being-asked","title":"What is being asked?","text":"<p>To find the median in logarithmic time.</p>"},{"location":"Algorithms/Binary_search/Median_of_two_sorted_arrays/#thoughts","title":"Thoughts","text":"<p>What is a median</p> <p>Consider : </p><pre><code>[1 2 |3 4]  #right partition is inclusive \n</code></pre> If we partition an even lengthed sorted array such that the left and right partitions are of equal size.<p></p> <p>The median is ( max(left partition) + min(right partition)  ) / 2</p> <p>If the array is odd lengthed : </p><pre><code>[1 2 3 4 5]\n     *\n</code></pre> We can consider the middle element to belong to both partitions. In which case <pre><code>[1 2 3 4 5]\n     *\n</code></pre><p></p> <p>The middle element IS the median i.e. index len(arr)//2</p> <p>Provided A and B are both sorted, We can always find partitions on A and B such that elements to the left of both partitions are less than elements to their right and both partitions combined divide the combined arrays in half. It will be clear what is meant from the examples below (Consider left out of bounds position to be -inf and right out of bounds position to be +inf). </p> <p>Example 1 : Valid Partitions</p> <pre><code>Example 1: \nB [ 1, 2, |6, 7]  # right pertition is inclusive\nA  [  5, |10  ]\n\n1,2,5  &lt; 6,7,10\n</code></pre> <p>Example 2 : Valid Partitions</p> <pre><code>Example 2 :\nB [ |5, 6, 7, 8, 9 ] # consider left out of bounds position to be -inf and right out of bounds position to be +inf\n\nA [ 1, 2, 3, 4| ]\n\n1,2,3,4 &lt; 5,6,7,8,9\n</code></pre> <p>Note that if you pick a partition on array A, there is only one corresponding partition on array B because we are looking for the set of partitions which divide the combined arrays in two halves.</p> <p>We can tell a set of partitions COULD BE a valid by verifying that the last element of the left partition of one array is less than or equal to the first element of the other arrays right partition. We can check the inverse to confirm that the partitions are in fact not valid. </p> <p>We can in fact use binary search to find the correct partitions. All we need is to know which way should we move one of the partitions. We will go into this in more detail. </p> <p>The idea is if we find the correct partitions, we can calculate the median by looking at the elements clustered around both partitions. Let's see how..</p> <p>The first question is how should we define the partitions?</p> <p>partition_a + partition_b = (m+n+1) //2 OR partition_a + partition_b = (m+n) // 2 </p> <p>let len(A) =m and len(B) = n </p> <p>FOR ODD LENGTHED ARRAY :</p> <pre><code>B  [1]   ; #IFF partition_b = (m+n +1 )//2 - partition_a  = 2//2 -0 = 1 But *IFF partition_b =  (m+n)//2 - partition_a =  1//2 -0 = 0-0 = 0 \n   *0 #1  \nA  [ ]   ; partition_a = (0+ 0)//2 =0 \n    0\n</code></pre> <p>If we say that partition_x + partition_y = (m+n)//2 then median is in the right partition in case of odd lengthed array.</p> <p>If we say that partition_x + partition_y = (m+n+1)//2 then median is in the left partition in case of odd lenghted array.</p> <p>It makes no difference for arrays where m+n is even. </p><pre><code>B [1 2]   ; #IFF partition_b = (m+n+1)//2 - partition_a  = 5//2 - 1 = 2- 1 = 1 and *IFF partition_b = (m+n)//2 - partition_a = 4//2 -1 = 1\n    #*1\nA [1 2]   ; partition_a = (0 + 2) // 2 = 1\n</code></pre><p></p> <p>Lets pick one convention : </p> <p>partition_a + partition_b = (m+n) //2</p> <p>Now for the binary search:</p> <p>It makes sense to binary search on the smaller array. Lets call this array A and the partition on it,  partition_a.</p> <p>Binary Search Logic</p> <p>In each iteration of the binary search, we consider mid as partition_a. For each partition_a we determine partition_b (as per our definition) and ask is left_b &gt; partition_a where left_b is the last element in the left partition of array B. </p> <ul> <li>If this is TRUE, then partition_a cannot be the pivot. Now the question is where do we move partition_a: to the left or to the right? We know that left_b is too big right now, so it needs to be made SMALLER i.e. Moved left. For doing this, by definition of our partitions, we NEED TO MOVE partition_a to the right i.e. partition_a need to be made larger. This is like increasing our lower bound.</li> <li>If this is FALSE, it means that the current set of partitions COULD BE VALID. Therefore, we set hi = mid. That is, decrease our upper bound.</li> </ul> <p>Dry Run</p> <pre><code>INIT:         \nB [ 1  3  4  5   6  ]\nA  [ 2  6  7   8   ] \n     lo          hi\n\nITER 1:\nA [1  3  4  5  6  ]\n         *            #left_b is 3.\nB  [ 2 6  7  8 ]  \n     lo   *            #3 &gt; 7 is F; this could be the right partition_a ; hi = mid\n         F/hi\n\nITER 2 :\nA [1  3  4  5  6  ]\n            *\nB  [ 2  6 7  8 ]\n   lo   *           #4&gt; 6 is F  ; this could be partition_a hi = mid            \n       F/hi\n\nITER 3 :\nA [1  3  4  5  6  ]\n               *\nB  [ 2    6  7  8 ]\n     *              #5&gt;2 T; This cannot be the right partition_a ;\n     T/lo hi        #lo = mid+1  ; Now lo == hi; lo and hi have converged and  loop exits in next iteration\n\nITER 5 :\nB [2   6   7   8]  # lo = hi = 1 and while loop exits\n       *\n       F/lo,hi\n</code></pre> <p>tl;dr</p> <p>Binary search will converge on the first false value. </p><pre><code>   [1    3    4     |5    6  ]\n     [2     |6    7    8]\n      T     F    F    F\n</code></pre><p></p> <p>In our binary search we need a function move_right which simply returns left_b &gt; right_a</p> <p>Thats it. Once , we have the right partition, calculate median:</p> <ul> <li> <p>If (m+n) is odd : return min(right_a, right_b)</p> </li> <li> <p>If (m+n) is even : return (max(left_a, left_b) + min(right_a, right_b)) /2 </p> </li> </ul>"},{"location":"Algorithms/Binary_search/Median_of_two_sorted_arrays/#code","title":"Code","text":"Code <pre><code>class Solution:\n    def findMedianSortedArrays(self, nums1: List[int], nums2: List[int]) -&gt; float:\n        # A is always the smaller array\n        A, B = nums1, nums2 \n        if len(A) &gt; len(B) :\n            A , B = B, A\n\n        #measure their lengths    \n        m,n = len(A), len(B)\n\n        def find_partition_b(partition_a) : \n            return (m+n)//2 - partition_a\n\n        def get_left(partition,array) :\n            return float('-inf') if (partition-1 )&lt;0 else array[partition-1]\n\n        def get_right(partition,array) :\n            return float('inf') if (partition)&gt;= len(array) else array[partition]\n\n        def move_right(partition_a) : \n            partition_b = find_partition_b(partition_a)\n            left_b = get_left(partition_b,B)\n            right_a = get_right(partition_a,A)\n            return left_b &gt; right_a\n\n        #binary search to find the correct partition_a\n        lo, hi  = 0 , len(A)\n        while lo &lt; hi :\n            partition_a = (lo +hi) // 2\n            if move_right(partition_a) :\n                lo = partition_a+1\n            else :\n                hi = partition_a\n\n        #lo and hi have converged on the correct partition_a \n        partition_a,partition_b   = lo, find_partition_b(lo)\n        right_a = get_right(partition_a,A)\n        right_b = get_right(partition_b,B)\n\n        #combined array length is odd\n        if (m+n) % 2 == 1 :\n            return min(right_a,right_b)\n\n        #combined array length is even    \n        left_a = get_left(partition_a,A)\n        left_b = get_left(partition_b,B)\n        return ( max(left_a,left_b) + min(right_a,right_b) ) / 2        \n</code></pre>"},{"location":"Algorithms/K_Way_Merge/","title":"The k-way Merge problem","text":""},{"location":"Algorithms/K_Way_Merge/#the-k-way-merge-problem","title":"The k-way Merge problem","text":""},{"location":"Algorithms/K_Way_Merge/#merging-k-sorted-arrays","title":"Merging k sorted arrays","text":"<p>We can think of the k-way merge problem as a generalization of the 2-way merge problem. That is given, k sorted containers, return a merged sorted container.</p> <p>In the 2-way merge problem , we had a binary choice when picking the next value to write to the merged container.  Now we have a k-way choice among k or less elements at each step. We basically need a good way to pick the smallest value from among all possible choices. This is what a heap does. Since we are considering containers sorted in a non decreasing order, a Min Heap is what we need to dynamically pick the smallest value at each step.</p> <p>The crux of the algorithm is :</p> <p>Algorithm</p> <pre><code>initialize a Min heap of all possible choices. \nWhile there is something in the heap : \n   Pop the heap top and write it to the merged container.\n   Get the element next to popped element in the container to which it belongs (If it exists). Put it in the heap.\n</code></pre> <p>Thats it.</p> <p>The code should actually simpler than handling the 2-way merge because there are no if else conditions to handle unequal lengthed arrays.</p> Code K Way Merge on Arrays <pre><code>\"\"\"\nThe same problem but with a different signature :\nhttps://www.geeksforgeeks.org/problems/merge-k-sorted-arrays/1\n\"\"\"\n#User function Template for python3\nfrom typing import List\nfrom heapq import heapify, heappop, heappush\nclass Solution:\n    #Function to merge k sorted arrays.\n    def mergeKArrays(self, arr : List[List[int]]) -&gt; List[int]:\n        \"\"\"\n        Given k sorted arrays, return a single merged array.\n        Input :\n         arr : a list of integer arrays\n        \"\"\"\n        # return merged list\n        len_res = 0\n\n        #Priority Queue- min heap\n        pq = []\n        #Assume that arrays can be of unequal length\n        #Initialize min heap\n        for array in arr :\n            len_res += len(array)\n            #Store the current index and a reference to the array for getting the next value in the array\n            pq.append((array[0] , 0 , array))\n\n        #heapify pq\n        heapify(pq)\n\n        #allocate result array\n        res = [None] * len_res\n\n        #write pointer\n        write_ptr = 0 \n        #Pick the lowest value from the current choices and write it to res\n        while pq :\n            curr_val, curr_ptr, curr_arr = heappop(pq)\n            res[write_ptr] = curr_val\n            if curr_ptr &lt; len(curr_arr) -1  : \n                heappush(pq, (curr_arr[curr_ptr+1] , curr_ptr+1,curr_arr ))\n            write_ptr+=1\n        return res\n\nif __name__ == \"__main__\" :\n    print( Solution().mergeKArrays([[1,2,3] , [5,6] , [9,9,10]]) )\n</code></pre>"},{"location":"Algorithms/K_Way_Merge/#merging-k-sorted-linked-lists","title":"Merging k sorted linked lists","text":"<p>The same thing for linked lists. </p> Code K Way Merge on Arrays <pre><code>from heapq import heapify,heappush,heappop\n#Definition for singly-linked list.\nclass ListNode:\n    def __init__(self, val=0, next=None):\n        self.val = val\n        self.next = next\nclass Solution:\n    def mergeKLists(self, lists: List[Optional[ListNode]]) -&gt; Optional[ListNode]:\n        pq=[]\n        for idx, head in enumerate(lists) : \n            if not head : continue\n            #Put idx between the value and the node object  to avoid comparing objects in case of a tie \n            #Which will lead to this error :'&lt;' not supported between instances of 'ListNode' and 'ListNode'\n            heappush(pq,(head.val,idx, head)) \n\n        #Allocate Write container and write pointer\n        dummy = ListNode(0)\n        write_ptr = dummy\n\n        while pq :\n            #Pick the lowest valued node from available choices.\n            _,idx,curr_node = heappop(pq)\n\n            #write to write container and move write pointer ahead\n            write_ptr.next = curr_node\n            write_ptr = write_ptr.next\n\n            #For curr_node, put its next node in the heap  \n            nxt_node = curr_node.next \n            if nxt_node : heappush(pq,(nxt_node.val, idx, nxt_node ))\n        return dummy.next\n</code></pre>"},{"location":"Algorithms/K_Way_Merge/#practice","title":"Practice","text":"Kth Smallest Element in a Sorted Matrix <pre><code>\"\"\"\nSolution for https://leetcode.com/problems/kth-smallest-element-in-a-sorted-matrix/\n\nThis can be solved using k-way merge. Instead of putting the next smaller value in a resultant merged array, \nsimply put it in a max heap of size k.\n\nExample :\n[\n[1,5,9],\n       *\n[10,11,13],\n    *\n[12,13,15]\n *\n]\n\nsay , k = 4\nCan we use A Max heap to maintain the k largest numbers?\n[10,9,5,1]\n\nWhen size becomes greater than k pop from the max heap:\n\n&gt;[11,10,9,5,1]\n&gt;size is greater than 5 \n&gt;pop from max heap\n[10,9,5,1]\n\nMemory required is : max heap of size k to  keep track of k smallest numbers\nA min heap of size len(matrix) (equal to the number of rows in matrix) to pick the next smallest number\n\nAlgorithm :\n\n1) Initialize a min heap to pick next smallest number from all rows of matrix\n2) While min heap is not empty:\n    -  Pop from min heap and put in a max heap of size k\n    -  Put element next to popped element into min heap  \n3) Return the top of max heap.\n\n\"\"\"\nfrom heapq import heapify, heappush , heappop\nclass Solution:\n    def kthSmallest(self, matrix: List[List[int]], k: int) -&gt; int:\n        pq = []\n        for row in matrix : \n            e = (row[0], 0, row)  # tuple of Curr_Value, curr_index, row reference\n            pq.append(e)\n        heapify(pq)\n\n        max_hp = []\n        while pq :\n            #pick next smallest number\n            val, idx, row = pq.pop()\n            #Simulate max heap by negating values\n            heappush(max_hp,-val)\n            if len(max_hp) &gt; k :\n                heappop(max_hp)\n            if idx &lt; len(row) -1 :\n                heappush(pq,(row[idx+1],idx+1,row))\n        # Retun top of max heap\n        return -heappop(max_hp) \n</code></pre> Find K Pairs with Smallest Sums <pre><code>\"\"\"\nSolution for : https://leetcode.com/problems/find-k-pairs-with-smallest-sums/\n\nThe problem statement is confusing. Key informations is that all possible pairs are formed by \ntaking one number from first array and combining it with a second number from the second array.\n        [1,7,11]\n        [2,4,6]\n [ (1,2) , (1,4) (1,6) ,(7,2) , (7,4) , (7,6) , (11,1), (11,2) , (11,7) ]\n\nThe problem is asking us , if we sort the above list of tuples by their sum, what would be the first k tuples.\n\nThoughts :\n\nDo we have to enumerate all pairs? Maybe. If we do that, we can maintain a max heap fo size k. \nIn the max heap, keep tuples ( (sum of two elements), (el1, el2))\n\nWe can do this :\n\nfrom heapq import heappop, heappush, heapify \nclass Solution:\n    def kSmallestPairs(self, nums1: List[int], nums2: List[int], k: int) -&gt; List[List[int]]:\n        res = []\n        ## Quadratic Time Complexity.\n        for n1 in nums1 :\n            for n2 in nums2 :\n                heappush(res,(-1*(n1+n2),(n1,n2)))\n                if len(res) &gt; k :\n                    heappop(res)\n\n        ans = []\n        while res : \n            ans.append(heappop(res)[1])\n        return ans\n\nBut this has a  quadratic run time and will give TLE.\n\nCan we do better ? \nWhy yes we can!\n\n\nConsider :\n[1,1,2], nums2 = [1,2,3], k = 3\n\nIsn't this how we are enumerating pairs?\n1,1 -&gt; 1,2 -&gt; 1,3\n*\n1,1 -&gt; 1,2 -&gt; 1,3\n*\n2,1 -&gt; 2,2 -&gt; 2,3\n* \n\nThese look an awful lot like three linked lists. \nWe know how to get the next element for each linked list. \nTo get the n smallest pairs we just need to run a k-way merge n times. \n(Lets call the number of pairs required n to avoid confusion with the concept of k-way merge.\nIn k-way merge, k refers to the number of linked lists we are merging )\n\nLets implement the solution. \n\"\"\"\n\nfrom heapq import heappop,heappush\n\nclass Solution:\n    def kSmallestPairs(self, nums1: List[int], nums2: List[int], k: int) -&gt; List[List[int]]:\n        n = k # rename k to n to avoid confusion with out mental model\n        pq = []\n        #Init pq \n        #Each linked list Node is a tuple of two elements.\n        #In each tuple the first element is fixed and from the first array. We take second element from the second array.\n        #We will use the index of the first element as tie breaker\n        #Use index of second element to get the next element in the same linked list\n\n        idx_2 = 0 \n        for idx_1, val in enumerate(nums1) :\n            heappush(pq,( ( nums1[idx_1] + nums2[0] ,idx_1, idx_2) \n            ))#  (sum of two elements, index of first element, index of second element )   \n\n        res = []\n\n        # Pick the next smallest tuple k times\n        while pq and n :\n            n-=1\n            _,idx_1,idx_2 = heappop(pq) # idx_2 is the index of element2 in the tuple\n            res.append((nums1[idx_1],nums2[idx_2])\n            )\n            #Put the element next to popped element in the min heap\n            if idx_2 &lt; len(nums2)-1 :\n                heappush(pq,( ( nums1[idx_1] + nums2[idx_2+1] ,idx_1, idx_2+1) )\n                 ) #  (sum of two elements, index of first element, index of second element) \n        return res\n</code></pre>"},{"location":"Algorithms/Math/Division_Algorithm/","title":"The Division Algorithm","text":""},{"location":"Algorithms/Math/Division_Algorithm/#the-division-algorithm","title":"The Division Algorithm","text":"<p>LibreTexts- Mathematics</p> <p>Given any integers \\(a\\) and \\(b\\), where \\(a &gt; 0\\), there exist integers \\(q\\) and \\(r\\) such that</p> <p>\\(b = aq + r\\)</p> <p>where \\(0 \\leq r &lt; a\\). Furthermore, \\(q\\) and \\(r\\) are uniquely determined by \\(a\\) and \\(b\\).</p> <p>The integers \\(b\\), \\(a\\), \\(q\\), and \\(r\\) are called the dividend, divisor, quotient, and remainder, respectively. Notice that \\(b\\) is a multiple of \\(a\\) if and only if \\(r = 0\\).</p> <ul> <li>b - Dividend</li> <li>a - Divisor</li> <li>q - Quotient</li> <li>r - Remainnder </li> </ul>"},{"location":"Algorithms/Math/Division_Algorithm/#in-python","title":"In Python","text":"<p>divmod(dividend,divisor) -&gt; (quotient, remainder)</p> <p>divmod(b,a) -&gt; (b//a,b%a)</p> <p>Example</p> <pre><code>&gt;&gt;&gt;divmod(1,10)\n&gt;&gt;&gt;(0, 1)\n</code></pre> <p>Example</p> <pre><code>&gt;&gt;&gt; divmod(4,2)\n(2, 0)\n&gt;&gt;&gt; 4 // 2\n2\n&gt;&gt;&gt; 4 % 2\n0\n</code></pre>"},{"location":"Algorithms/Math/Division_Algorithm/#python-rounds-negative-numbers-away-from-zero","title":"Python Rounds Negative Numbers away from zero","text":"<p>In python :</p> <p>15 / -4  = - 3.75</p> <p>But,</p> <p>14// - 4 =  - 4</p> <p>To round towards zero, do floating point division and convert to int.</p> <pre><code>Do floating point division then convert to an int. No extra modules needed.\n\nPython 3:\n\n&gt;&gt;&gt; int(-1 / 2)\n0\n&gt;&gt;&gt; int(-3 / 2)\n-1\n&gt;&gt;&gt; int(1 / 2)\n0\n&gt;&gt;&gt; int(3 / 2)\n1\n\nPython 2:\n\n&gt;&gt;&gt; int(float(-1) / 2)\n0\n&gt;&gt;&gt; int(float(-3) / 2)\n-1\n&gt;&gt;&gt; int(float(1) / 2)\n0\n&gt;&gt;&gt; int(float(3) / 2)\n1\n</code></pre> Example - Evaluate RPN <pre><code>\"\"\"\nWhat is the data :\nRPN expression as a list of tokens\ntokens = [\"2\",\"1\",\"+\",\"3\",\"*\"]\n\n\nFor each opearator +,-,*,/ there will be two pops.\nfirst pop is the rhs and second pop is the lhs \n\nPitfall :\nIn python integer division truncates away from zero\n&gt;&gt;&gt; -4//3\n-2             ## Truncates away from zero for negative numbers\n&gt;&gt;&gt; 4//3    \n1              ## Truncates towards zero for positive numbers\n\nTo handle this :\n&gt;&gt;&gt; -4/3               ## Do floating point division first\n-1.3333333333333333\n&gt;&gt;&gt; int(-4/3)          ## Then cast to int and result will always truncate towards zero.\n-1\n\"\"\"\nclass Solution:\n    def evalRPN(self, tokens: List[str]) -&gt; int:\n        evaluate  = {\n            '+' : lambda x,y : x+y ,\n            '-' : lambda x,y : x-y,\n            '/' : lambda x,y : int(x/y),\n            '*' : lambda x,y : x*y \n        }\n        stack = []\n        for token in tokens : \n            if token in evaluate :\n                rhs = stack.pop()\n                lhs = stack.pop()\n                stack.append(evaluate[token](lhs,rhs))\n            else :\n                stack.append(int(token))\n        return stack[-1]\n</code></pre>"},{"location":"Algorithms/Math/Division_Algorithm/#additional-resources","title":"Additional Resources :","text":"<p>LibreTexts- Mathematics</p>"},{"location":"Algorithms/Math/Appendix/Number_Theoretic_Notation/","title":"Number Theoretic Notation","text":""},{"location":"Algorithms/Math/Appendix/Number_Theoretic_Notation/#number-theoretic-notation","title":"Number Theoretic Notation","text":"<p>Refresh Page if math notation is not rendered</p> <p>From here</p> <pre><code>Lemma. If m | pq and (m,p) = 1, then m | q.\n</code></pre> <p>Vertical Bar is the divides symbol.</p> <p>m | pq means that m divides the product pq, meaning there exists some integer k such that pq = km.</p> <p>This is called the gcd notation :</p> <p><code>(m,p) = 1</code></p> <p>Here it is used to express that m and p are co-prime or relatively prime. Meaning that their GCD is 1 and they share no common factors other than 1.</p> <p>Translation</p> <pre><code>If m divides pq and m and p are co-prime, then m must divide q.\n</code></pre> <p>Isn't it simple or at least approachable, once you understand what the notation means? </p> <p>So, I worked with Claude 4.0 to generate the following guide. I think it did a pretty good job. </p> <p>Following Content is AI Generated</p>"},{"location":"Algorithms/Math/Appendix/Number_Theoretic_Notation/#basic-number-systems-and-sets","title":"Basic Number Systems and Sets","text":"<p>Number theory begins with understanding different types of numbers and their standard mathematical notation.</p>"},{"location":"Algorithms/Math/Appendix/Number_Theoretic_Notation/#standard-number-sets","title":"Standard Number Sets","text":"<p>The foundation of number theory rests on several key number systems, each with specific notation:</p> <p>Natural Numbers: \\(\\mathbb{N}\\) represents the set of natural numbers. The exact definition varies by context:</p> <ul> <li>\\(\\mathbb{N} = {1, 2, 3, 4, \\ldots}\\) (positive integers only)</li> <li>\\(\\mathbb{N}_0 = {0, 1, 2, 3, \\ldots}\\) (including zero)</li> <li>\\(\\mathbb{N}^* = {1, 2, 3, 4, \\ldots}\\) (explicitly positive)</li> </ul> <p>Integers: \\(\\mathbb{Z} = {\\ldots, -2, -1, 0, 1, 2, \\ldots}\\) includes all positive and negative whole numbers.</p> <p>Rational Numbers: \\(\\mathbb{Q} = {\\frac{a}{b} : a, b \\in \\mathbb{Z}, b \\neq 0}\\) represents all fractions.</p> <p>Real Numbers: \\(\\mathbb{R}\\) encompasses all rational and irrational numbers.</p> <p>Complex Numbers: \\(\\mathbb{C} = {a + bi : a, b \\in \\mathbb{R}, i^2 = -1}\\) extends the reals.</p>"},{"location":"Algorithms/Math/Appendix/Number_Theoretic_Notation/#modular-arithmetic-sets","title":"Modular Arithmetic Sets","text":"<p>Integers modulo n: The set \\(\\mathbb{Z}/n\\mathbb{Z} = {0, 1, 2, \\ldots, n-1}\\) represents equivalence classes under modular arithmetic. Alternative notation \\(\\mathbb{Z}_n\\) exists but can create confusion with p-adic integers.</p> <p>Units modulo n: \\((\\mathbb{Z}/n\\mathbb{Z})^*\\) denotes integers relatively prime to \\(n\\) in the range \\({1, 2, \\ldots, n-1}\\).</p>"},{"location":"Algorithms/Math/Appendix/Number_Theoretic_Notation/#divisibility-and-basic-relations","title":"Divisibility and Basic Relations","text":"<p>Understanding when one integer divides another forms a cornerstone of number theory.</p>"},{"location":"Algorithms/Math/Appendix/Number_Theoretic_Notation/#divisibility-notation","title":"Divisibility Notation","text":"<p>The expression \\(a \\mid b\\) means \"a divides b\" or \"a is a divisor of b.\" This indicates that \\(b = ka\\) for some integer \\(k\\).</p> <p>Examples:</p> <ul> <li>\\(3 \\mid 12\\) since \\(12 = 4 \\times 3\\)</li> <li>\\(5 \\mid 35\\) since \\(35 = 7 \\times 5\\)</li> </ul> <p>The negation \\(a \\nmid b\\) means \"a does not divide b.\"</p>"},{"location":"Algorithms/Math/Appendix/Number_Theoretic_Notation/#greatest-common-divisor-gcd","title":"Greatest Common Divisor (GCD)","text":"<p>The greatest common divisor has several notational conventions. The most unambiguous is \\(\\gcd(a,b)\\), though traditional mathematics sometimes uses \\((a,b)\\).</p> <p>Key Properties:</p> <ul> <li>\\(\\gcd(a,b) = \\gcd(b, a \\bmod b)\\) (Euclidean algorithm)</li> <li>\\(\\gcd(a,b) \\cdot \\text{lcm}(a,b) = ab\\)</li> </ul> <p>Examples:</p> <ul> <li>\\(\\gcd(12, 18) = 6\\)</li> <li>\\(\\gcd(17, 19) = 1\\) (relatively prime)</li> </ul>"},{"location":"Algorithms/Math/Appendix/Number_Theoretic_Notation/#least-common-multiple-lcm","title":"Least Common Multiple (LCM)","text":"<p>The least common multiple uses \\(\\text{lcm}(a,b)\\) or traditional bracket notation \\([a,b]\\).</p> <p>Examples:</p> <ul> <li>\\(\\text{lcm}(12, 18) = 36\\)</li> <li>\\(\\text{lcm}(4, 6) = 12\\)</li> </ul>"},{"location":"Algorithms/Math/Appendix/Number_Theoretic_Notation/#prime-numbers-and-factorization","title":"Prime Numbers and Factorization","text":"<p>Prime numbers serve as the building blocks of all integers through unique factorization.</p>"},{"location":"Algorithms/Math/Appendix/Number_Theoretic_Notation/#prime-notation-and-factorization","title":"Prime Notation and Factorization","text":"<p>Individual primes are typically denoted by lowercase \\(p\\), with subscripts for distinct primes: \\(p_1, p_2, \\ldots, p_k\\).</p> <p>Fundamental Theorem of Arithmetic: Every integer \\(n &gt; 1\\) has a unique prime factorization:</p> <p>\\[n = p_1^{e_1} p_2^{e_2} \\cdots p_k^{e_k}\\]</p> <p>where \\(p_1 &lt; p_2 &lt; \\cdots &lt; p_k\\) are distinct primes and \\(e_i \\geq 1\\).</p> <p>Examples:</p> <ul> <li>\\(12 = 2^2 \\cdot 3^1\\)</li> <li>\\(100 = 2^2 \\cdot 5^2\\)</li> </ul>"},{"location":"Algorithms/Math/Appendix/Number_Theoretic_Notation/#prime-related-functions","title":"Prime-Related Functions","text":"<p>Prime counting function: \\(\\pi(x)\\) counts primes up to \\(x\\)</p> <ul> <li>\\(\\pi(10) = 4\\) (primes: 2, 3, 5, 7)</li> </ul> <p>Number of prime factors:</p> <ul> <li>\\(\\Omega(n)\\) counts prime factors with multiplicity</li> <li>\\(\\omega(n)\\) counts distinct prime factors</li> </ul> <p>Examples:</p> <ul> <li>\\(\\Omega(12) = 3\\) since \\(12 = 2^2 \\cdot 3\\)</li> <li>\\(\\omega(12) = 2\\) since 12 has two distinct prime factors</li> </ul>"},{"location":"Algorithms/Math/Appendix/Number_Theoretic_Notation/#congruences-and-modular-arithmetic","title":"Congruences and Modular Arithmetic","text":"<p>Congruences provide a powerful framework for studying integer relationships.</p>"},{"location":"Algorithms/Math/Appendix/Number_Theoretic_Notation/#congruence-notation","title":"Congruence Notation","text":"<p>The fundamental congruence relation \\(a \\equiv b \\pmod{n}\\) means \\(n \\mid (a-b)\\).</p> <p>Standard form: \\(a \\equiv b \\pmod{n}\\) with parentheses around the modulus.</p> <p>Examples:</p> <ul> <li>\\(13 \\equiv 1 \\pmod{12}\\) since \\(12 \\mid (13-1)\\)</li> <li>\\(25 \\equiv 4 \\pmod{7}\\) since \\(7 \\mid (25-4)\\)</li> </ul> <p>Key properties:</p> <ul> <li>Reflexive: \\(a \\equiv a \\pmod{n}\\)</li> <li>Symmetric: If \\(a \\equiv b \\pmod{n}\\), then \\(b \\equiv a \\pmod{n}\\)</li> <li>Transitive: If \\(a \\equiv b \\pmod{n}\\) and \\(b \\equiv c \\pmod{n}\\), then \\(a \\equiv c \\pmod{n}\\)</li> </ul>"},{"location":"Algorithms/Math/Appendix/Number_Theoretic_Notation/#modular-arithmetic-operations","title":"Modular Arithmetic Operations","text":"<p>Operations preserve congruences:</p> <ul> <li>\\((a + b) \\bmod n\\)</li> <li>\\((a \\cdot b) \\bmod n\\)</li> <li>\\(a^k \\bmod n\\)</li> </ul> <p>Multiplicative inverse: \\(a^{-1} \\pmod{n}\\) exists when \\(\\gcd(a,n) = 1\\).</p> <p>Example: \\(3^{-1} \\equiv 4 \\pmod{11}\\) since \\(3 \\cdot 4 \\equiv 1 \\pmod{11}\\).</p>"},{"location":"Algorithms/Math/Appendix/Number_Theoretic_Notation/#eulers-totient-function","title":"Euler's Totient Function","text":"<p>Euler's totient function counts integers relatively prime to a given number.</p>"},{"location":"Algorithms/Math/Appendix/Number_Theoretic_Notation/#definition-and-notation","title":"Definition and Notation","text":"<p>\\(\\phi(n)\\) (or \\(\\varphi(n)\\)) equals the number of positive integers less than or equal to \\(n\\) that are relatively prime to \\(n\\).</p> <p>Formula for prime powers: If \\(p\\) is prime, then:</p> <ul> <li>\\(\\phi(p) = p - 1\\)</li> <li>\\(\\phi(p^k) = p^k - p^{k-1} = p^{k-1}(p-1)\\)</li> </ul> <p>General formula: For \\(n\\) with prime factorization \\(n = p_1^{e_1} p_2^{e_2} \\cdots p_k^{e_k}\\):</p> <p>\\[\\phi(n) = n \\prod_{i=1}^{k} \\left(1 - \\frac{1}{p_i}\\right)\\]</p> <p>Examples:</p> <ul> <li>\\(\\phi(9) = 6\\) (numbers 1, 2, 4, 5, 7, 8 are relatively prime to 9)</li> <li>\\(\\phi(12) = 4\\) (numbers 1, 5, 7, 11 are relatively prime to 12)</li> </ul>"},{"location":"Algorithms/Math/Appendix/Number_Theoretic_Notation/#eulers-theorem","title":"Euler's Theorem","text":"<p>For \\(\\gcd(a,n) = 1\\): \\(a^{\\phi(n)} \\equiv 1 \\pmod{n}\\)</p> <p>Fermat's Little Theorem (special case): </p> <p>For prime \\(p\\) and \\(\\gcd(a,p) = 1\\): \\(a^{p-1} \\equiv 1 \\pmod{p}\\)</p>"},{"location":"Algorithms/Math/Appendix/Number_Theoretic_Notation/#floor-and-ceiling-functions","title":"Floor and Ceiling Functions","text":"<p>These functions convert real numbers to integers with specific rounding behavior.</p>"},{"location":"Algorithms/Math/Appendix/Number_Theoretic_Notation/#floor-function","title":"Floor Function","text":"<p>\\(\\lfloor x \\rfloor\\) gives the greatest integer less than or equal to \\(x\\).</p> <p>Examples:</p> <ul> <li>\\(\\lfloor 3.7 \\rfloor = 3\\)</li> <li>\\(\\lfloor -2.3 \\rfloor = -3\\)</li> <li>\\(\\lfloor 5 \\rfloor = 5\\)</li> </ul>"},{"location":"Algorithms/Math/Appendix/Number_Theoretic_Notation/#ceiling-function","title":"Ceiling Function","text":"<p>\\(\\lceil x \\rceil\\) gives the smallest integer greater than or equal to \\(x\\).</p> <p>Examples:</p> <ul> <li>\\(\\lceil 3.2 \\rceil = 4\\)</li> <li>\\(\\lceil -2.3 \\rceil = -2\\)</li> <li>\\(\\lceil 5 \\rceil = 5\\)</li> </ul>"},{"location":"Algorithms/Math/Appendix/Number_Theoretic_Notation/#applications-in-number-theory","title":"Applications in Number Theory","text":"<p>Counting divisors: The number of multiples of \\(d\\) up to \\(n\\) is \\(\\lfloor n/d \\rfloor\\).</p> <p>Division algorithm: For integers \\(a\\) and \\(b &gt; 0\\): \\(a = bq + r\\) where \\(q = \\lfloor a/b \\rfloor\\) and \\(0 \\leq r &lt; b\\).</p>"},{"location":"Algorithms/Math/Appendix/Number_Theoretic_Notation/#summation-and-product-notation","title":"Summation and Product Notation","text":"<p>These operators handle collections of terms systematically.</p>"},{"location":"Algorithms/Math/Appendix/Number_Theoretic_Notation/#summation-over-divisors","title":"Summation Over Divisors","text":"<p>A fundamental pattern in number theory involves summing over all divisors of an integer.</p> <p>Notation: \\(\\sum_{d \\mid n} f(d)\\) means sum \\(f(d)\\) over all positive divisors \\(d\\) of \\(n\\).</p> <p>Examples:</p> <ul> <li>\\(\\sum_{d \\mid n} 1 = d(n)\\) (number of divisors)</li> <li>\\(\\sum_{d \\mid n} d = \\sigma(n)\\) (sum of divisors)</li> <li>\\(\\sum_{d \\mid n} \\phi(d) = n\\) (fundamental identity)</li> </ul>"},{"location":"Algorithms/Math/Appendix/Number_Theoretic_Notation/#summation-over-primes","title":"Summation Over Primes","text":"<p>Examples:</p> <ul> <li>\\(\\sum_{p \\leq x} \\frac{1}{p}\\) (sum of reciprocals of primes up to \\(x\\))</li> <li>\\(\\sum_{\\substack{p \\text{ prime} \\ p \\equiv 1 \\pmod{4}}} \\frac{1}{p^s}\\) (conditional summation)</li> </ul>"},{"location":"Algorithms/Math/Appendix/Number_Theoretic_Notation/#product-notation","title":"Product Notation","text":"<p>Euler product formula: </p> <p>\\[\\zeta(s) = \\sum_{n=1}^{\\infty} \\frac{1}{n^s} = \\prod_{p \\text{ prime}} \\frac{1}{1-p^{-s}}\\]</p> <p>Totient function product:</p> <p>\\[\\phi(n) = n \\prod_{\\substack{p \\text{ prime} \\ p \\mid n}} \\left(1 - \\frac{1}{p}\\right)\\]</p>"},{"location":"Algorithms/Math/Appendix/Number_Theoretic_Notation/#asymptotic-notation","title":"Asymptotic Notation","text":"<p>Asymptotic analysis describes the growth behavior of functions as their arguments approach infinity.</p>"},{"location":"Algorithms/Math/Appendix/Number_Theoretic_Notation/#big-o-family-bachmann-landau-notation","title":"Big O Family (Bachmann-Landau Notation)","text":"<p>Big O: \\(f(n) = O(g(n))\\) means \\(f\\) grows at most as fast as \\(g\\).</p> <ul> <li>Formally: \\(\\exists\\ c &gt; 0, n_0\\) such that \\(|f(n)| \\leq c|g(n)|\\) for all \\(n \\geq n_0\\)</li> </ul> <p>Big Omega: \\(f(n) = \\Omega(g(n))\\) means \\(f\\) grows at least as fast as \\(g\\).</p> <p>Big Theta: \\(f(n) = \\Theta(g(n))\\) means \\(f\\) and \\(g\\) have the same growth rate.</p> <p>Little o: \\(f(n) = o(g(n))\\) means \\(f\\) grows strictly slower than \\(g\\).</p> <p>Asymptotic equivalence: \\(f(n) \\sim g(n)\\) means \\(\\lim_{n \\to \\infty} \\frac{f(n)}{g(n)} = 1\\).</p>"},{"location":"Algorithms/Math/Appendix/Number_Theoretic_Notation/#applications-in-number-theory_1","title":"Applications in Number Theory","text":"<p>Prime Number Theorem: \\(\\pi(x) \\sim \\frac{x}{\\ln x}\\)</p> <p>Divisor function: \\(d(n) = O(n^{\\epsilon})\\) for any \\(\\epsilon &gt; 0\\)</p> <p>Average order of Euler's totient: \\(\\sum_{n \\leq x} \\phi(n) \\sim \\frac{3x^2}{\\pi^2}\\)</p>"},{"location":"Algorithms/Math/Appendix/Number_Theoretic_Notation/#advanced-number-theory-symbols","title":"Advanced Number Theory Symbols","text":"<p>Specialized symbols appear in deeper number theory topics.</p>"},{"location":"Algorithms/Math/Appendix/Number_Theoretic_Notation/#legendre-symbol","title":"Legendre Symbol","text":"<p>For odd prime \\(p\\) and integer \\(a\\), the Legendre symbol \\(\\left(\\frac{a}{p}\\right)\\) indicates quadratic residue status:</p> <ul> <li>\\(\\left(\\frac{a}{p}\\right) = 1\\) if \\(a\\) is a quadratic residue modulo \\(p\\)</li> <li>\\(\\left(\\frac{a}{p}\\right) = -1\\) if \\(a\\) is a quadratic non-residue modulo \\(p\\)  </li> <li>\\(\\left(\\frac{a}{p}\\right) = 0\\) if \\(p \\mid a\\)</li> </ul> <p>Example: \\(\\left(\\frac{2}{7}\\right) = 1\\) since \\(3^2 \\equiv 2 \\pmod{7}\\).</p>"},{"location":"Algorithms/Math/Appendix/Number_Theoretic_Notation/#jacobi-symbol","title":"Jacobi Symbol","text":"<p>The Jacobi symbol \\(\\left(\\frac{a}{n}\\right)\\) extends the Legendre symbol to odd composite integers \\(n\\).</p> <p>For \\(n = p_1^{e_1} p_2^{e_2} \\cdots p_k^{e_k}\\): \\[\\left(\\frac{a}{n}\\right) = \\prod_{i=1}^{k} \\left(\\frac{a}{p_i}\\right)^{e_i}\\]</p>"},{"location":"Algorithms/Math/Appendix/Number_Theoretic_Notation/#quadratic-reciprocity","title":"Quadratic Reciprocity","text":"<p>For distinct odd primes \\(p\\) and \\(q\\):</p> <p>\\[\\left(\\frac{p}{q}\\right)\\left(\\frac{q}{p}\\right) = (-1)^{\\frac{p-1}{2} \\cdot \\frac{q-1}{2}}\\]</p>"},{"location":"Algorithms/Math/Appendix/Number_Theoretic_Notation/#arithmetic-functions","title":"Arithmetic Functions","text":"<p>M\u00f6bius function: \\(\\mu(n)\\) equals:</p> <ul> <li>\\(1\\) if \\(n\\) is a square-free positive integer with even number of prime factors</li> <li>\\(-1\\) if \\(n\\) is a square-free positive integer with odd number of prime factors  </li> <li>\\(0\\) if \\(n\\) has a squared prime factor</li> </ul> <p>Von Mangoldt function: \\(\\Lambda(n)\\) equals \\(\\ln p\\) if \\(n = p^k\\) for prime \\(p\\), and \\(0\\) otherwise.</p> <p>Dirichlet characters: \\(\\chi(n)\\) are completely multiplicative functions modulo some integer.</p>"},{"location":"Algorithms/Math/Appendix/Number_Theoretic_Notation/#understanding-mathematical-expressions","title":"Understanding Mathematical Expressions","text":"<p>When reading number theory texts, you'll encounter mathematical expressions that combine multiple notations. Here are some common patterns:</p>"},{"location":"Algorithms/Math/Appendix/Number_Theoretic_Notation/#complex-expressions","title":"Complex Expressions","text":"<p>Multiplicative functions: An expression like \\(\\sum_{d \\mid n} \\mu(d) f(n/d)\\) means \"sum over all divisors \\(d\\) of \\(n\\), the product of the M\u00f6bius function \\(\\mu(d)\\) and \\(f(n/d)\\).\"</p> <p>Conditional sums: \\(\\sum_{\\substack{p \\leq x \\ p \\equiv 1 \\pmod{4}}} \\frac{1}{p}\\) means \"sum \\(\\frac{1}{p}\\) over all primes \\(p\\) up to \\(x\\) that are congruent to 1 modulo 4.\"</p> <p>Product formulas: \\(\\prod_{p \\mid n} \\left(1 + \\frac{1}{p}\\right)\\) means \"take the product over all primes \\(p\\) that divide \\(n\\) of the expression \\(\\left(1 + \\frac{1}{p}\\right)\\).\"</p>"},{"location":"Algorithms/Math/Appendix/Number_Theoretic_Notation/#reading-order-and-precedence","title":"Reading Order and Precedence","text":"<p>Mathematical expressions follow standard precedence rules:</p> <ol> <li>Exponents and function applications (like \\(\\phi(n)\\))</li> <li>Multiplication and division</li> <li>Addition and subtraction</li> <li>Relations (like \\(\\equiv\\), \\(\\mid\\), \\(&lt;\\))</li> </ol> <p>Example: \\(a^{\\phi(n)} \\equiv 1 \\pmod{n}\\) reads as \"(\\(a\\) raised to the power \\(\\phi(n)\\)) is congruent to 1 modulo \\(n\\).\"</p> <p>This comprehensive guide provides the foundational notation needed for understanding number theory concepts in academic literature and mathematical texts.</p>"},{"location":"Algorithms/Math/Appendix/Set_Theoretic_Notation/","title":"Set Theoretic Notation","text":""},{"location":"Algorithms/Math/Appendix/Set_Theoretic_Notation/#set-theoretic-notation","title":"Set Theoretic Notation","text":"<p>AI Generated Content</p> <p>Refresh Page if math notation is not rendered</p> <p>A comprehensive reference for mathematical notation used in set theory, from basic concepts to advanced topics.</p>"},{"location":"Algorithms/Math/Appendix/Set_Theoretic_Notation/#basic-set-notation","title":"Basic Set Notation","text":"<p>Set theory provides the foundation for modern mathematics through its system of notation for collections and relationships.</p>"},{"location":"Algorithms/Math/Appendix/Set_Theoretic_Notation/#set-definition-and-membership","title":"Set Definition and Membership","text":"<p>Sets are collections of objects called elements or members. The fundamental relationship is membership.</p> <p>Membership: \\(x \\in A\\) means \"\\(x\\) is an element of set \\(A\\)\" or \"\\(x\\) belongs to \\(A\\).\"</p> <p>Non-membership: \\(x \\notin A\\) means \"\\(x\\) is not an element of set \\(A\\).\"</p> <p>Examples: - \\(3 \\in {1, 2, 3, 4}\\) (3 is in the set) - \\(5 \\notin {1, 2, 3, 4}\\) (5 is not in the set) - \\(\\pi \\in \\mathbb{R}\\) (\u03c0 is a real number)</p>"},{"location":"Algorithms/Math/Appendix/Set_Theoretic_Notation/#set-builder-notation","title":"Set Builder Notation","text":"<p>Sets can be defined by listing elements or by describing properties.</p> <p>Roster notation: \\(A = {1, 2, 3, 4}\\) lists all elements explicitly.</p> <p>Set-builder notation: \\(A = {x : P(x)}\\) or \\(A = {x \\mid P(x)}\\) means \"the set of all \\(x\\) such that property \\(P(x)\\) holds.\"</p> <p>Examples: - \\({x \\in \\mathbb{Z} : x^2 &lt; 10}\\) (integers whose square is less than 10) - \\({x \\in \\mathbb{R} \\mid x &gt; 0}\\) (positive real numbers) - \\({2n : n \\in \\mathbb{N}}\\) (even natural numbers)</p>"},{"location":"Algorithms/Math/Appendix/Set_Theoretic_Notation/#special-sets","title":"Special Sets","text":"<p>Empty set: \\(\\emptyset\\) or \\({}\\) contains no elements.</p> <p>Universal set: \\(U\\) or \\(\\Omega\\) represents the collection of all objects under consideration in a particular context.</p> <p>Singleton set: \\({a}\\) contains exactly one element \\(a\\).</p>"},{"location":"Algorithms/Math/Appendix/Set_Theoretic_Notation/#set-relationships","title":"Set Relationships","text":"<p>Understanding how sets relate to each other is fundamental to set theory.</p>"},{"location":"Algorithms/Math/Appendix/Set_Theoretic_Notation/#subset-relations","title":"Subset Relations","text":"<p>Subset: \\(A \\subseteq B\\) means \"\\(A\\) is a subset of \\(B\\)\" or \"every element of \\(A\\) is also in \\(B\\).\"</p> <ul> <li>Formally: \\(\\forall x (x \\in A \\rightarrow x \\in B)\\)</li> </ul> <p>Proper subset: \\(A \\subsetneq B\\) or \\(A \\subset B\\) means \"\\(A\\) is a proper subset of \\(B\\)\" (\\(A \\subseteq B\\) and \\(A \\neq B\\)).</p> <p>Superset: \\(B \\supseteq A\\) means \"\\(B\\) is a superset of \\(A\\)\" (equivalent to \\(A \\subseteq B\\)).</p> <p>Proper superset: \\(B \\supsetneq A\\) or \\(B \\supset A\\) means \"\\(B\\) is a proper superset of \\(A\\).\"</p> <p>Examples: - \\({1, 2} \\subseteq {1, 2, 3}\\) - \\(\\mathbb{N} \\subset \\mathbb{Z} \\subset \\mathbb{Q} \\subset \\mathbb{R}\\) - \\(\\emptyset \\subseteq A\\) for any set \\(A\\)</p>"},{"location":"Algorithms/Math/Appendix/Set_Theoretic_Notation/#set-equality","title":"Set Equality","text":"<p>Set equality: \\(A = B\\) means sets \\(A\\) and \\(B\\) have exactly the same elements.</p> <ul> <li>Equivalent to: \\(A \\subseteq B\\) and \\(B \\subseteq A\\)</li> </ul> <p>Set inequality: \\(A \\neq B\\) means sets \\(A\\) and \\(B\\) do not have the same elements.</p>"},{"location":"Algorithms/Math/Appendix/Set_Theoretic_Notation/#set-operations","title":"Set Operations","text":"<p>Set operations combine or modify sets to create new sets.</p>"},{"location":"Algorithms/Math/Appendix/Set_Theoretic_Notation/#basic-operations","title":"Basic Operations","text":"<p>Union: \\(A \\cup B\\) is the set of elements in \\(A\\) or \\(B\\) (or both).</p> <ul> <li>\\(A \\cup B = {x : x \\in A \\text{ or } x \\in B}\\)</li> </ul> <p>Intersection: \\(A \\cap B\\) is the set of elements in both \\(A\\) and \\(B\\).</p> <ul> <li>\\(A \\cap B = {x : x \\in A \\text{ and } x \\in B}\\)</li> </ul> <p>Difference: \\(A \\setminus B\\) or \\(A - B\\) is the set of elements in \\(A\\) but not in \\(B\\).</p> <ul> <li>\\(A \\setminus B = {x : x \\in A \\text{ and } x \\notin B}\\)</li> </ul> <p>Complement: \\(A^c\\) or \\(\\overline{A}\\) or \\(A'\\) is the set of elements not in \\(A\\) (relative to some universal set).</p> <ul> <li>\\(A^c = {x \\in U : x \\notin A}\\)</li> </ul> <p>Examples: - \\({1, 2} \\cup {2, 3} = {1, 2, 3}\\) - \\({1, 2} \\cap {2, 3} = {2}\\) - \\({1, 2, 3} \\setminus {2} = {1, 3}\\)</p>"},{"location":"Algorithms/Math/Appendix/Set_Theoretic_Notation/#disjoint-sets","title":"Disjoint Sets","text":"<p>Disjoint sets: \\(A\\) and \\(B\\) are disjoint if \\(A \\cap B = \\emptyset\\).</p> <p>Pairwise disjoint: A collection of sets is pairwise disjoint if every pair of distinct sets in the collection is disjoint.</p>"},{"location":"Algorithms/Math/Appendix/Set_Theoretic_Notation/#extended-operations","title":"Extended Operations","text":"<p>Symmetric difference: \\(A \\triangle B\\) or \\(A \\oplus B\\) contains elements in exactly one of \\(A\\) or \\(B\\).</p> <ul> <li>\\(A \\triangle B = (A \\setminus B) \\cup (B \\setminus A) = (A \\cup B) \\setminus (A \\cap B)\\)</li> </ul>"},{"location":"Algorithms/Math/Appendix/Set_Theoretic_Notation/#generalized-operations","title":"Generalized Operations","text":"<p>Set operations can be extended to collections of multiple sets.</p>"},{"location":"Algorithms/Math/Appendix/Set_Theoretic_Notation/#indexed-families","title":"Indexed Families","text":"<p>Indexed union: \\(\\bigcup_{i \\in I} A_i\\) is the union of all sets \\(A_i\\) where \\(i\\) ranges over index set \\(I\\).</p> <ul> <li>\\(\\bigcup_{i \\in I} A_i = {x : \\exists i \\in I, x \\in A_i}\\)</li> </ul> <p>Indexed intersection: \\(\\bigcap_{i \\in I} A_i\\) is the intersection of all sets \\(A_i\\) where \\(i\\) ranges over index set \\(I\\).</p> <ul> <li>\\(\\bigcap_{i \\in I} A_i = {x : \\forall i \\in I, x \\in A_i}\\)</li> </ul> <p>Common notations:</p> <ul> <li>\\(\\bigcup_{i=1}^{n} A_i = A_1 \\cup A_2 \\cup \\cdots \\cup A_n\\)</li> <li>\\(\\bigcap_{i=1}^{\\infty} A_i\\) (intersection of countably many sets)</li> <li>\\(\\bigcup_{x \\in X} A_x\\) (union indexed by elements of set \\(X\\))</li> </ul>"},{"location":"Algorithms/Math/Appendix/Set_Theoretic_Notation/#partition","title":"Partition","text":"<p>Partition: A partition of set \\(A\\) is a collection of non-empty, pairwise disjoint subsets whose union is \\(A\\).</p> <p>If \\(\\{A_i\\}_{i \\in I}\\) is a partition of \\(A\\), then:</p> <ul> <li>\\(A_i \\neq \\emptyset\\) for all \\(i \\in I\\)</li> <li>\\(A_i \\cap A_j = \\emptyset\\) for \\(i \\neq j\\)</li> <li>\\(\\bigcup_{i \\in I} A_i = A\\)</li> </ul>"},{"location":"Algorithms/Math/Appendix/Set_Theoretic_Notation/#power-sets-and-cardinality","title":"Power Sets and Cardinality","text":"<p>These concepts deal with the \"size\" and structure of sets.</p>"},{"location":"Algorithms/Math/Appendix/Set_Theoretic_Notation/#power-set","title":"Power Set","text":"<p>Power set: \\(\\mathcal{P}(A)\\) or \\(2^A\\) is the set of all subsets of \\(A\\).</p> <ul> <li>\\(\\mathcal{P}(A) = {X : X \\subseteq A}\\)</li> </ul> <p>Examples: - \\(\\mathcal{P}({1, 2}) = {\\emptyset, {1}, {2}, {1, 2}}\\) - \\(\\mathcal{P}(\\emptyset) = {\\emptyset}\\)</p>"},{"location":"Algorithms/Math/Appendix/Set_Theoretic_Notation/#cardinality","title":"Cardinality","text":"<p>Cardinality: \\(|A|\\) or \\(\\#A\\) or \\(\\text{card}(A)\\) represents the number of elements in set \\(A\\).</p> <p>Finite sets: \\(|A| = n\\) for some natural number \\(n\\).</p> <p>Infinite cardinalities:</p> <ul> <li>\\(|\\mathbb{N}| = \\aleph_0\\) (aleph-null, countable infinity)</li> <li>\\(|\\mathbb{R}| = 2^{\\aleph_0} = \\mathfrak{c}\\) (continuum, uncountable)</li> </ul> <p>Cardinality relationships:</p> <ul> <li>\\(|A| = |B|\\) means \\(A\\) and \\(B\\) have the same cardinality</li> <li>\\(|A| \\leq |B|\\) means there exists an injection from \\(A\\) to \\(B\\)</li> <li>\\(|A| &lt; |B|\\) means \\(|A| \\leq |B|\\) and \\(|A| \\neq |B|\\)</li> </ul>"},{"location":"Algorithms/Math/Appendix/Set_Theoretic_Notation/#relations-and-functions","title":"Relations and Functions","text":"<p>Relations and functions are special types of sets that describe connections between elements.</p>"},{"location":"Algorithms/Math/Appendix/Set_Theoretic_Notation/#relations","title":"Relations","text":"<p>Relation: A relation \\(R\\) from set \\(A\\) to set \\(B\\) is a subset of the Cartesian product \\(A \\times B\\).</p> <p>Notation: \\(aRb\\) or \\((a,b) \\in R\\) means \"\\(a\\) is related to \\(b\\) by relation \\(R\\).\"</p> <p>Types of relations on a set \\(A\\):</p> <ul> <li>Reflexive: \\(\\forall a \\in A, aRa\\)</li> <li>Symmetric: \\(\\forall a,b \\in A, aRb \\rightarrow bRa\\)</li> <li>Transitive: \\(\\forall a,b,c \\in A, (aRb \\land bRc) \\rightarrow aRc\\)</li> <li>Antisymmetric: \\(\\forall a,b \\in A, (aRb \\land bRa) \\rightarrow a = b\\)</li> </ul> <p>Equivalence relation: A relation that is reflexive, symmetric, and transitive.</p> <p>Partial order: A relation that is reflexive, antisymmetric, and transitive.</p>"},{"location":"Algorithms/Math/Appendix/Set_Theoretic_Notation/#functions","title":"Functions","text":"<p>Function: \\(f: A \\rightarrow B\\) is a relation where each element in \\(A\\) is related to exactly one element in \\(B\\).</p> <p>Function notation: \\(f(a) = b\\) means \"function \\(f\\) maps element \\(a\\) to element \\(b\\).\"</p> <p>Domain: \\(\\text{dom}(f)\\) is the set of all inputs to function \\(f\\).</p> <p>Codomain: The set \\(B\\) in \\(f: A \\rightarrow B\\).</p> <p>Range or Image: \\(\\text{range}(f) = {f(a) : a \\in A}\\) is the set of all actual outputs.</p> <p>Image of a set: \\(f[X] = {f(x) : x \\in X}\\) for \\(X \\subseteq A\\).</p> <p>Preimage: \\(f^{-1}[Y] = {x \\in A : f(x) \\in Y}\\) for \\(Y \\subseteq B\\).</p>"},{"location":"Algorithms/Math/Appendix/Set_Theoretic_Notation/#function-properties","title":"Function Properties","text":"<p>Injective (one-to-one): \\(\\forall a_1, a_2 \\in A, f(a_1) = f(a_2) \\rightarrow a_1 = a_2\\)</p> <p>Surjective (onto): \\(\\forall b \\in B, \\exists a \\in A, f(a) = b\\)</p> <p>Bijective: Both injective and surjective.</p> <p>Inverse function: If \\(f: A \\rightarrow B\\) is bijective, then \\(f^{-1}: B \\rightarrow A\\) exists.</p>"},{"location":"Algorithms/Math/Appendix/Set_Theoretic_Notation/#cartesian-products-and-tuples","title":"Cartesian Products and Tuples","text":"<p>These constructions create new sets from existing ones.</p>"},{"location":"Algorithms/Math/Appendix/Set_Theoretic_Notation/#cartesian-product","title":"Cartesian Product","text":"<p>Cartesian product: \\(A \\times B = {(a,b) : a \\in A \\text{ and } b \\in B}\\) is the set of all ordered pairs.</p> <p>Higher-order products:</p> <ul> <li>\\(A \\times B \\times C = {(a,b,c) : a \\in A, b \\in B, c \\in C}\\)</li> <li>\\(A^n = A \\times A \\times \\cdots \\times A\\) (\\(n\\) times)</li> </ul> <p>Examples: - \\({1,2} \\times {a,b} = {(1,a), (1,b), (2,a), (2,b)}\\) - \\(\\mathbb{R}^2 = \\mathbb{R} \\times \\mathbb{R}\\) (the plane)</p>"},{"location":"Algorithms/Math/Appendix/Set_Theoretic_Notation/#tuples-and-sequences","title":"Tuples and Sequences","text":"<p>Ordered pair: \\((a,b)\\) where order matters (\\((a,b) \\neq (b,a)\\) unless \\(a = b\\)).</p> <p>n-tuple: \\((a_1, a_2, \\ldots, a_n)\\) is an ordered collection of \\(n\\) elements.</p> <p>Sequence: An ordered list, often infinite:</p> <ul> <li>Finite sequence: \\((a_1, a_2, \\ldots, a_n)\\)</li> <li>Infinite sequence: \\((a_1, a_2, a_3, \\ldots)\\) or \\((a_n)_{n=1}^{\\infty}\\)</li> </ul>"},{"location":"Algorithms/Math/Appendix/Set_Theoretic_Notation/#advanced-set-theory-concepts","title":"Advanced Set Theory Concepts","text":"<p>These concepts appear in more sophisticated mathematical contexts.</p>"},{"location":"Algorithms/Math/Appendix/Set_Theoretic_Notation/#topology-and-closure","title":"Topology and Closure","text":"<p>Closure: \\(\\overline{A}\\) or \\(\\text{cl}(A)\\) represents the closure of set \\(A\\) in a topological space.</p> <p>Interior: \\(\\text{int}(A)\\) or \\(A^\\circ\\) represents the interior of set \\(A\\).</p> <p>Boundary: \\(\\partial A\\) represents the boundary of set \\(A\\).</p>"},{"location":"Algorithms/Math/Appendix/Set_Theoretic_Notation/#ordinal-and-cardinal-numbers","title":"Ordinal and Cardinal Numbers","text":"<p>Ordinal numbers: \\(\\omega, \\omega + 1, \\omega \\cdot 2, \\ldots\\) represent positions in well-ordered sets.</p> <p>Cardinal numbers: \\(\\aleph_0, \\aleph_1, \\aleph_2, \\ldots\\) represent sizes of infinite sets.</p> <p>Continuum hypothesis: \\(2^{\\aleph_0} = \\aleph_1\\) (there is no cardinality strictly between \\(\\aleph_0\\) and \\(2^{\\aleph_0}\\)).</p>"},{"location":"Algorithms/Math/Appendix/Set_Theoretic_Notation/#choice-and-well-ordering","title":"Choice and Well-Ordering","text":"<p>Axiom of Choice: For any collection of non-empty sets, there exists a choice function that selects one element from each set.</p> <p>Well-ordering: Every non-empty subset has a least element.</p> <p>Zorn's Lemma: If every chain in a partially ordered set has an upper bound, then the set has a maximal element.</p>"},{"location":"Algorithms/Math/Appendix/Set_Theoretic_Notation/#logical-quantifiers-in-set-context","title":"Logical Quantifiers in Set Context","text":"<p>Set theory frequently uses logical notation to express properties precisely.</p>"},{"location":"Algorithms/Math/Appendix/Set_Theoretic_Notation/#universal-and-existential-quantifiers","title":"Universal and Existential Quantifiers","text":"<p>Universal quantifier: \\(\\forall x \\in A, P(x)\\) means \"for all \\(x\\) in \\(A\\), property \\(P(x)\\) holds.\"</p> <p>Existential quantifier: \\(\\exists x \\in A, P(x)\\) means \"there exists an \\(x\\) in \\(A\\) such that property \\(P(x)\\) holds.\"</p> <p>Unique existence: \\(\\exists! x \\in A, P(x)\\) means \"there exists a unique \\(x\\) in \\(A\\) such that \\(P(x)\\) holds.\"</p> <p>Examples: - \\(\\forall x \\in \\mathbb{R}, x^2 \\geq 0\\) (all real numbers have non-negative squares) - \\(\\exists x \\in \\mathbb{Q}, x^2 = 2\\) is false (no rational number squares to 2) - \\(\\exists! x \\in \\mathbb{R}, x^3 = 8\\) (exactly one real cube root of 8)</p>"},{"location":"Algorithms/Math/Appendix/Set_Theoretic_Notation/#logical-connectives","title":"Logical Connectives","text":"<p>Conjunction: \\(P \\land Q\\) means \"\\(P\\) and \\(Q\\)\"</p> <p>Disjunction: \\(P \\lor Q\\) means \"\\(P\\) or \\(Q\\)\"</p> <p>Negation: \\(\\neg P\\) means \"not \\(P\\)\"</p> <p>Implication: \\(P \\rightarrow Q\\) means \"if \\(P\\) then \\(Q\\)\"</p> <p>Biconditional: \\(P \\leftrightarrow Q\\) means \"\\(P\\) if and only if \\(Q\\)\"</p>"},{"location":"Algorithms/Math/Appendix/Set_Theoretic_Notation/#understanding-complex-set-expressions","title":"Understanding Complex Set Expressions","text":"<p>Reading complex set-theoretic statements requires understanding how notation combines.</p>"},{"location":"Algorithms/Math/Appendix/Set_Theoretic_Notation/#parsing-complex-statements","title":"Parsing Complex Statements","text":"<p>Multiple operations: \\(A \\cup (B \\cap C)\\) means \"\\(A\\) union with the intersection of \\(B\\) and \\(C\\).\"</p> <p>Quantified statements: \\({x \\in \\mathbb{R} : \\forall \\epsilon &gt; 0, \\exists \\delta &gt; 0, |x| &lt; \\delta \\rightarrow |f(x)| &lt; \\epsilon}\\) describes points where function \\(f\\) approaches 0.</p> <p>Nested operations: \\(\\bigcup_{n=1}^{\\infty} \\bigcap_{k=n}^{\\infty} A_k\\) represents the limit superior of a sequence of sets.</p>"},{"location":"Algorithms/Math/Appendix/Set_Theoretic_Notation/#common-patterns","title":"Common Patterns","text":"<p>Characterizing elements: Statements like \\(x \\in A \\leftrightarrow P(x)\\) define set membership by properties.</p> <p>Set equality proofs: To show \\(A = B\\), prove both \\(A \\subseteq B\\) and \\(B \\subseteq A\\).</p> <p>Distributive laws: \\(A \\cap (B \\cup C) = (A \\cap B) \\cup (A \\cap C)\\)</p> <p>This comprehensive guide provides the essential notation for understanding set theory concepts in mathematical literature, from basic membership to advanced topics in modern set theory.</p>"},{"location":"Algorithms/Math/Arithmetic_Series_and_Gauss_Summation/","title":"Arithmetic Series","text":""},{"location":"Algorithms/Math/Arithmetic_Series_and_Gauss_Summation/#arithmetic-series","title":"Arithmetic Series","text":"<p>An arithmetic series is one in which the difference between any two consecutive terms is constant.</p>"},{"location":"Algorithms/Math/Arithmetic_Series_and_Gauss_Summation/#gauss-summation","title":"Gauss' Summation","text":"<p>As the story goes, when Gauss was a schoolboy his teacher gave the class a task of adding the numbers 1..100 and that is when Gauss came up with this insight :</p> <pre><code>#terms for 3 and 4 are ommitted, but you get the picture..\n\n1 2 3 4 5 6 7 8 9 10\n|_|_____|_|_____|__| \n  |   1+10=11   |\n  |_____|_|_____|\n      2+9=11\n        |_|\n      5+6=11\n</code></pre> <p>There are 10//2 11s in this sum. So the sum for first 10 numbers is <code>10//2 * (10+1)</code>.</p> <p>For odd numbers we can compe with the same formula through a different route.Reverse the series and add each individual term of reverserse serioes to the original series :</p> <p></p><pre><code>1   2  3  4  5  6  7  8  9  10  11\n11 10  9  8  7  6  5  4  3  2   1\n----------------------------------\n12 12  12 12 12 12 12 12 12 12 12\n</code></pre> We are multiplying n+1,  n times.  And since we multiplied the entire series sum by two by adding it to itsself, we finally divide by two. <p></p> <p>Therefore for 11 the series sum is <code>( (11+1)*11 ) // 2</code>.</p> <p>So, utlimately the formula is the same for both even and odd number of terms.</p> <p>More formally, this is the formula for the sum of the first \\( n \\) natural numbers:</p> \\[ S = 1 + 2 + 3 + \\cdots + n = \\frac{n(n+1)}{2} \\]"},{"location":"Algorithms/Math/Arithmetic_Series_and_Gauss_Summation/#generalizing-to-any-arithmetic-series","title":"Generalizing to Any Arithmetic Series","text":"<p>The formula for the sum of an arithmetic series can be applied to any sequence where the difference between consecutive terms is constant.</p> <p>Given an arithmetic series:</p> \\[ a_1,\\, a_2,\\, a_3,\\, \\ldots,\\, a_n \\] <p>where:</p> <ul> <li>\\( a_1 \\) is the first term,</li> <li>\\( d \\) is the common difference,</li> <li>\\( n \\) is the number of terms,</li> </ul> <p>the sum \\( S \\) of the series is:</p> \\[ S = \\frac{n}{2} \\times (a_1 + a_n) \\] <p>where \\( a_n \\) is the last term.</p> <p>Alternatively, since \\( a_n = a_1 + (n-1)d \\), the formula can also be written as:</p> \\[ S = \\frac{n}{2} \\times [2a_1 + (n-1)d] \\] <p>Example:</p> <p>For the series \\( 2,\\, 4,\\, 6,\\, 8 \\) :</p> <ul> <li>\\( a_1 = 2 \\)</li> <li>\\( d = 2 \\)</li> <li>\\( n = 4 \\)</li> <li>\\( a_n = 8 \\)</li> </ul> <p>Applying the formula:</p> \\[ S = \\frac{4}{2} \\times (2 + 8) = 2 \\times 10 = 20 \\] <p>This formula works for any arithmetic series, not just the sum of the first \\( n \\) natural numbers.</p>"},{"location":"Algorithms/Math/Arithmetic_Series_and_Gauss_Summation/#additional-resources","title":"Additional Resources","text":"<ol> <li>https://mathbitsnotebook.com/Algebra2/Sequences/SSGauss.html </li> </ol>"},{"location":"Algorithms/Math/Arithmetic_Series_and_Gauss_Summation/#practice","title":"Practice","text":"Arranging Coins <pre><code>\"\"\"\nSolution for : https://leetcode.com/problems/arranging-coins/description/\n\nRef : https://mathbitsnotebook.com/Algebra2/Sequences/SSGauss.html\n\n#Gauss summation\n1 + 2 + 3 + 4\n5 + 5  = (x+1)*x//2 = 4(5) //2 # x = 3\n1+ 2 + 3 + 4 + 5\n6 + 6 +3   = (6*5)//2 = 15 # x = 5        \n\nFind x which is closest to n using bisect left\n   Question : for this x is gauss summation less than n. \n\nlo could be equal to n -&gt; return lo\nelse return lo -1\n\"\"\"\nclass Solution:\n    @staticmethod\n    def is_less(x,n) :\n        \"\"\"\n        Is gauss sum less than n \n        \"\"\"\n        return ( (x*(x+1))//2 ) &lt; n\n\n    def arrangeCoins(self, n: int) -&gt; int:\n        lo,hi = 1, n+1\n        while lo &lt; hi :\n            mid = (lo+hi)//2\n            #print(lo,hi,mid)\n            if self.is_less(mid,n) :\n                lo = mid+1\n            else :  \n                hi = mid \n        if lo*(lo+1)//2 == n :\n            return lo\n        else :\n            return lo-1 \n</code></pre>"},{"location":"Algorithms/Math/Fundamental_Theorem_Of_Arithmetic/Introduction/","title":"Introduction","text":""},{"location":"Algorithms/Math/Fundamental_Theorem_Of_Arithmetic/Introduction/#fundamental-theorem-of-arithmetic","title":"Fundamental Theorem of Arithmetic","text":"<p>Also called the unique Factorization Theorem.</p> <p>Unique Factoprization Theorem</p> <p>The Fundamental Theorem of Arithmetic states that every integer greater than 1 has a unique prime factorization.</p> <p>Additional Resources :</p> <ol> <li>Proof explained by Bruce Ikenaga</li> <li>Flexbooks - Prime Factorization</li> <li>Wikipedia Article</li> </ol>"},{"location":"Algorithms/Math/Fundamental_Theorem_Of_Arithmetic/claude_ugly_number_analysis/","title":"The Ugly Number Problem","text":""},{"location":"Algorithms/Math/Fundamental_Theorem_Of_Arithmetic/claude_ugly_number_analysis/#the-ugly-number-problem","title":"The Ugly Number Problem","text":"<p>This is a Conversation with Claude 4.0 about the solution to the Ugly Number Problem.</p>"},{"location":"Algorithms/Math/Fundamental_Theorem_Of_Arithmetic/claude_ugly_number_analysis/#prompt","title":"PROMPT","text":"<p>Can you add mathematical expressions for this explanation : </p> <p>Solution for : https://leetcode.com/problems/ugly-number/</p> <p>Problem Statement: An ugly number is a positive integer which does not have a prime factor other than 2, 3, and 5.Given an integer n, return true if n is an ugly number.</p> <p>This is a great number theory problem, The fundamental theorem of arithmetic, also callled the prime factorization theorem states that any number greater than 1 is either a prime number or can be expressed as a product of prime numbers (ignoring the order).</p> <p>12 = 2 * 2 * 3 = 2**2 * 3</p> <p>210 = 2 * 3 * 5 * 7</p> <p>By definition of ugnliness as stated in the problem, 12 is  ugly but 210 is not because it has 7 as a factor. To test ugliness, all you have to do is remove all the factors of 2,3 and 5  and assert that only 1 remains.</p> <p>But, what does it mean to remove, say ONE factor of 2 from some number x. It means, that the number x has is made up by adding some other number twice x = x/2 + x/2. We can test this by checking if 2 divides x evenly i.e. x%2 = 0 Removing one factor of two means removing one copy of x/2 from x. We keep doing this until x%2 != 0, to remove all factors of 2. Of the remaining number y, it can be said that there is no other number which if added to itself will yield y.</p> <p>Similarly, for 3, after removing all factors of 3 from x,  of the remaining number y, it can be said that there is no other number which if added to itself thrice will yield y.</p>"},{"location":"Algorithms/Math/Fundamental_Theorem_Of_Arithmetic/claude_ugly_number_analysis/#response","title":"RESPONSE","text":"<p>Here's the solution with mathematical expressions added to make the concepts clearer:</p>"},{"location":"Algorithms/Math/Fundamental_Theorem_Of_Arithmetic/claude_ugly_number_analysis/#solution-for-ugly-number-problem","title":"Solution for Ugly Number Problem","text":"<p>Problem Statement: An ugly number is a positive integer which does not have a prime factor other than 2, 3, and 5.</p>"},{"location":"Algorithms/Math/Fundamental_Theorem_Of_Arithmetic/claude_ugly_number_analysis/#mathematical-foundation","title":"Mathematical Foundation","text":"<p>The Fundamental Theorem of Arithmetic states that every integer <code>n &gt; 1</code> can be uniquely expressed as:</p> <pre><code>n = p\u2081^a\u2081 \u00d7 p\u2082^a\u2082 \u00d7 p\u2083^a\u2083 \u00d7 ... \u00d7 p\u2096^a\u2096\n</code></pre> <p>where <code>p\u2081, p\u2082, ..., p\u2096</code> are distinct prime numbers and <code>a\u2081, a\u2082, ..., a\u2096</code> are positive integers.</p> <p>Examples: - <code>12 = 2\u00b2 \u00d7 3\u00b9</code> (ugly - only primes 2 and 3) - <code>210 = 2\u00b9 \u00d7 3\u00b9 \u00d7 5\u00b9 \u00d7 7\u00b9</code> (not ugly - contains prime 7)</p>"},{"location":"Algorithms/Math/Fundamental_Theorem_Of_Arithmetic/claude_ugly_number_analysis/#definition-of-ugliness","title":"Definition of Ugliness","text":"<p>A number <code>n</code> is ugly if and only if: </p><pre><code>n = 2^a \u00d7 3^b \u00d7 5^c\n</code></pre> where <code>a, b, c \u2265 0</code> are non-negative integers.<p></p>"},{"location":"Algorithms/Math/Fundamental_Theorem_Of_Arithmetic/claude_ugly_number_analysis/#algorithm-factor-removal","title":"Algorithm: Factor Removal","text":"<p>To test ugliness, we systematically remove all factors of 2, 3, and 5:</p> <p>Step 1: Remove all factors of 2 </p><pre><code>While n \u2261 0 (mod 2):\n    n \u2190 n/2\n</code></pre> This continues until <code>n \u2262 0 (mod 2)</code>, meaning no more factors of 2 remain.<p></p> <p>Step 2: Remove all factors of 3 </p><pre><code>While n \u2261 0 (mod 3):\n    n \u2190 n/3\n</code></pre><p></p> <p>Step 3: Remove all factors of 5 </p><pre><code>While n \u2261 0 (mod 5):\n    n \u2190 n/5\n</code></pre><p></p>"},{"location":"Algorithms/Math/Fundamental_Theorem_Of_Arithmetic/claude_ugly_number_analysis/#mathematical-proof-of-correctness","title":"Mathematical Proof of Correctness","text":"<p>After removing all factors of 2, 3, and 5, we have: </p><pre><code>n_final = n_original / (2^a \u00d7 3^b \u00d7 5^c)\n</code></pre><p></p> <p>Case 1: If <code>n_original</code> was ugly, then <code>n_original = 2^a \u00d7 3^b \u00d7 5^c</code>, so: </p><pre><code>n_final = (2^a \u00d7 3^b \u00d7 5^c) / (2^a \u00d7 3^b \u00d7 5^c) = 1\n</code></pre><p></p> <p>Case 2: If <code>n_original</code> was not ugly, then <code>n_original = 2^a \u00d7 3^b \u00d7 5^c \u00d7 p\u2081^d\u2081 \u00d7 ... \u00d7 p\u2096^d\u2096</code> where <code>p\u1d62 \u2209 {2,3,5}</code>, so: </p><pre><code>n_final = (2^a \u00d7 3^b \u00d7 5^c \u00d7 p\u2081^d\u2081 \u00d7 ... \u00d7 p\u2096^d\u2096) / (2^a \u00d7 3^b \u00d7 5^c) = p\u2081^d\u2081 \u00d7 ... \u00d7 p\u2096^d\u2096 &gt; 1\n</code></pre><p></p>"},{"location":"Algorithms/Math/Fundamental_Theorem_Of_Arithmetic/claude_ugly_number_analysis/#modular-arithmetic-interpretation","title":"Modular Arithmetic Interpretation","text":"<p>When we check <code>n % 2 == 0</code>, we're testing the congruence: </p><pre><code>n \u2261 0 (mod 2)\n</code></pre><p></p> <p>This is equivalent to asking: \"Does there exist an integer <code>k</code> such that <code>n = 2k</code>?\"</p> <p>If yes, then 2 is a factor, and we can \"remove\" it by setting <code>n \u2190 k = n/2</code>.</p>"},{"location":"Algorithms/Math/Fundamental_Theorem_Of_Arithmetic/claude_ugly_number_analysis/#implementation","title":"Implementation","text":"<pre><code>def isUgly(n):\n    \"\"\"\n    Determines if n is an ugly number.\n\n    Mathematical approach: n is ugly iff n = 2^a \u00d7 3^b \u00d7 5^c for some a,b,c \u2265 0\n\n    Args:\n        n: integer to test\n\n    Returns:\n        bool: True if n is ugly, False otherwise\n    \"\"\"\n    # Base case: ugly numbers must be positive\n    if n &lt;= 0:\n        return False\n\n    # Remove all factors of 2: n \u2190 n/2 while n \u2261 0 (mod 2)\n    while n % 2 == 0:\n        n //= 2\n\n    # Remove all factors of 3: n \u2190 n/3 while n \u2261 0 (mod 3)\n    while n % 3 == 0:\n        n //= 3\n\n    # Remove all factors of 5: n \u2190 n/5 while n \u2261 0 (mod 5)\n    while n % 5 == 0:\n        n //= 5\n\n    # If n = 2^a \u00d7 3^b \u00d7 5^c originally, then n = 1 now\n    # If n had other prime factors, then n &gt; 1 now\n    return n == 1\n</code></pre>"},{"location":"Algorithms/Math/Fundamental_Theorem_Of_Arithmetic/claude_ugly_number_analysis/#implementation-time-and-space-complexity","title":"Implementation### Time and Space Complexity","text":"<p>Time Complexity: <code>O(log n)</code></p> <ul> <li>In the worst case, we divide by 2 approximately <code>log\u2082(n)</code> times</li> <li>Similarly for factors of 3 and 5</li> <li>Total: <code>O(log\u2082(n) + log\u2083(n) + log\u2085(n)) = O(log n)</code></li> </ul> <p>Space Complexity: <code>O(1)</code></p> <ul> <li>Only using a constant amount of extra space</li> </ul> <p>The mathematical foundation shows that this algorithm correctly identifies ugly numbers by leveraging the unique prime factorization and systematically removing the allowed prime factors through modular arithmetic operations.</p>"},{"location":"Algorithms/Math/Fundamental_Theorem_Of_Arithmetic/claude_ugly_number_analysis/#prompt_1","title":"PROMPT","text":"<p>Explain this notation :n \u2190 n/5 while n \u2261 0 (mod 5)</p>"},{"location":"Algorithms/Math/Fundamental_Theorem_Of_Arithmetic/claude_ugly_number_analysis/#response_1","title":"RESPONSE","text":"<p>Excellent! Let me break down this chain of mathematical equivalences step by step:</p>"},{"location":"Algorithms/Math/Fundamental_Theorem_Of_Arithmetic/claude_ugly_number_analysis/#the-triple-equivalence","title":"The Triple Equivalence","text":"<pre><code>n \u2261 0 (mod 5) \u27fa n % 5 == 0 \u27fa \u2203k \u2208 \u2124 such that n = 5k\n</code></pre> <p>The symbol <code>\u27fa</code> means \"if and only if\" - these three statements are completely equivalent and mean exactly the same thing.</p>"},{"location":"Algorithms/Math/Fundamental_Theorem_Of_Arithmetic/claude_ugly_number_analysis/#statement-1-n-0-mod-5","title":"Statement 1: <code>n \u2261 0 (mod 5)</code>","text":"<p>Reading: \"n is congruent to 0 modulo 5\"</p> <p>Mathematical Definition: Two integers are congruent modulo m if they have the same remainder when divided by m.</p> <p>Since we're comparing n to 0: - <code>n \u2261 0 (mod 5)</code> means n and 0 have the same remainder when divided by 5 - The remainder when 0 is divided by 5 is 0 - Therefore: n has remainder 0 when divided by 5</p>"},{"location":"Algorithms/Math/Fundamental_Theorem_Of_Arithmetic/claude_ugly_number_analysis/#statement-2-n-5-0","title":"Statement 2: <code>n % 5 == 0</code>","text":"<p>Reading: \"n modulo 5 equals 0\" or \"the remainder when n is divided by 5 is 0\"</p> <p>Programming Context: This is how we write modular arithmetic in most programming languages.</p> <p>Examples: - <code>10 % 5 = 0</code> \u2713 - <code>15 % 5 = 0</code> \u2713 - <code>17 % 5 = 2</code> \u2717 - <code>23 % 5 = 3</code> \u2717</p>"},{"location":"Algorithms/Math/Fundamental_Theorem_Of_Arithmetic/claude_ugly_number_analysis/#statement-3-k-z-such-that-n-5k","title":"Statement 3: <code>\u2203k \u2208 \u2124 such that n = 5k</code>","text":"<p>Reading: \"There exists an integer k such that n equals 5 times k\"</p> <p>Symbol Breakdown:</p> <ul> <li><code>\u2203</code> = \"there exists\"</li> <li><code>k \u2208 \u2124</code> = \"k is an element of the integers\" (k is a whole number: ..., -2, -1, 0, 1, 2, ...)</li> <li><code>n = 5k</code> = \"n equals 5 times k\"</li> </ul> <p>Meaning: n can be written as some integer multiplied by 5 (i.e., n is a multiple of 5)</p>"},{"location":"Algorithms/Math/Fundamental_Theorem_Of_Arithmetic/claude_ugly_number_analysis/#why-these-are-all-equivalent","title":"Why These Are All Equivalent","text":"<p>Let me prove these equivalences with examples:</p>"},{"location":"Algorithms/Math/Fundamental_Theorem_Of_Arithmetic/claude_ugly_number_analysis/#example-1-n-20","title":"Example 1: n = 20","text":"<p>Statement 1: <code>20 \u2261 0 (mod 5)</code> - 20 \u00f7 5 = 4 remainder 0 \u2713</p> <p>Statement 2: <code>20 % 5 == 0</code> - 20 % 5 = 0 \u2713</p> <p>Statement 3: <code>\u2203k \u2208 \u2124 such that 20 = 5k</code> - k = 4 works: 20 = 5 \u00d7 4 \u2713</p>"},{"location":"Algorithms/Math/Fundamental_Theorem_Of_Arithmetic/claude_ugly_number_analysis/#example-2-n-17","title":"Example 2: n = 17","text":"<p>Statement 1: <code>17 \u2261 0 (mod 5)</code> - 17 \u00f7 5 = 3 remainder 2 \u2260 0 \u2717</p> <p>Statement 2: <code>17 % 5 == 0</code> - 17 % 5 = 2 \u2260 0 \u2717</p> <p>Statement 3: <code>\u2203k \u2208 \u2124 such that 17 = 5k</code> - No integer k works: 17/5 = 3.4 (not an integer) \u2717</p>"},{"location":"Algorithms/Math/Fundamental_Theorem_Of_Arithmetic/claude_ugly_number_analysis/#the-mathematical-connection","title":"The Mathematical Connection","text":""},{"location":"Algorithms/Math/Fundamental_Theorem_Of_Arithmetic/claude_ugly_number_analysis/#division-algorithm","title":"Division Algorithm","text":"<p>For any integers n and 5, we can write: </p><pre><code>n = 5q + r    where 0 \u2264 r &lt; 5\n</code></pre> - q is the quotient - r is the remainder<p></p>"},{"location":"Algorithms/Math/Fundamental_Theorem_Of_Arithmetic/claude_ugly_number_analysis/#the-three-statements-are-saying","title":"The Three Statements Are Saying:","text":"<ol> <li>Modular: The remainder r equals 0</li> <li>Programming: The remainder operation gives 0  </li> <li>Algebraic: We can write n as 5 times some integer (when r = 0, then n = 5q)</li> </ol>"},{"location":"Algorithms/Math/Fundamental_Theorem_Of_Arithmetic/claude_ugly_number_analysis/#practical-examples","title":"Practical Examples","text":"n n \u00f7 5 Quotient (q) Remainder (r) n \u2261 0 (mod 5)? n % 5 k exists? n = 5k 0 0 \u00f7 5 0 0 \u2713 0 \u2713 (k=0) 0 = 5\u00d70 5 5 \u00f7 5 1 0 \u2713 0 \u2713 (k=1) 5 = 5\u00d71 10 10 \u00f7 5 2 0 \u2713 0 \u2713 (k=2) 10 = 5\u00d72 12 12 \u00f7 5 2 2 \u2717 2 \u2717 12 \u2260 5k -15 -15 \u00f7 5 -3 0 \u2713 0 \u2713 (k=-3) -15 = 5\u00d7(-3)"},{"location":"Algorithms/Math/Fundamental_Theorem_Of_Arithmetic/claude_ugly_number_analysis/#key-insight","title":"Key Insight","text":"<p>All three expressions are different ways of asking the same fundamental question:</p> <p>\"Is n a multiple of 5?\"</p> <ul> <li>Modular arithmetic asks: \"Does n have remainder 0 when divided by 5?\"</li> <li>Programming asks: \"Does the modulo operation return 0?\"  </li> <li>Algebraic asks: \"Can we write n as 5 times some integer?\"</li> </ul> <p>They're all equivalent ways of expressing divisibility!</p>"},{"location":"Algorithms/Merge_Sort/","title":"Merge Sort","text":""},{"location":"Algorithms/Merge_Sort/#merge-sort","title":"Merge Sort","text":""},{"location":"Algorithms/Merge_Sort/#sorting-an-array","title":"Sorting an array","text":"<p>tl;dr</p> <p>Think of Merge Sort as a binary recursive application of 2-way merge where the left and right recursive calls sort the left and right subarrays and we merge the two halves in the post order position.  </p> <p>Thats it.</p> <p>The actual sorting is a result of the merge function applied in a bottom up fashion from smallest subarray lengths.The actual sorting is a result of the merge function applied in a bottom up fashion from smallest subarray lengths.</p> <p>As for the implementation details :</p> <p>Instead of passing arrays to the merge method, pass pointers. Three pointers are sufficient : lo , mid and hi. The subarrays to be merged are always adjacent: arr[lo...mid] and arr[mid+1...hi]</p> <p>Base case is hit when lo = hi . A subarray of size one cannot be divided an further and is sorted by definition.</p> <p>Merging cannot be done in place. So merge sort does require extra space. Instead of allocating extra temp space in each recursive call, just allocate extra space as external variable, equal to length of array. This temp space can be used to perform 2-way merge using the lo, mid and hi pointers passed to merge method. </p> Code Merge Sort on Array <pre><code>\"\"\"\nMerge Sort Solution for https://leetcode.com/problems/sort-an-array/description/\n\"\"\"\nclass Solution:\n    def merge(self, lo,mid,hi) :\n        \"\"\"\n        Merge self.nums[lo:mid+1] and self.nums[mid+1,hi+1] into self.temp[lo:hi+1] \n        Write back self.temp[lo:hi+1] into self.nums[lo:hi+1]\n        \"\"\"\n        print(\"merge \" ,lo,mid,hi)\n        for i in range(lo,hi+1) :\n            self.temp[i] = self.nums[i]\n\n        p1 = lo \n        p2 = mid+1\n        p3 = lo \n\n        while p1 &lt;= mid and p2 &lt;= hi :\n            if self.temp[p1] &lt; self.temp[p2] :\n                self.nums[p3] =  self.temp[p1]\n                p1+=1\n            else : \n                self.nums[p3] =  self.temp[p2]\n                p2+=1\n            p3+=1\n        while p1 &lt;= mid :\n            self.nums[p3] = self.temp[p1]\n            p1+=1\n            p3+=1\n\n        while p2 &lt;= hi :\n            self.nums[p3] = self.temp[p2]\n            p2+=1\n            p3+=1\n\n    def sort(self, lo,hi) :\n        \"\"\"\n        DRY RUN UNTIL FIRST BASE CASE IS HIT :\n\n         0           2            5  \n        [5,    1,    1,  2,  0  ,0]\n        lo           m           hi\n        lo     m     hi\n        lomhi \n        ^BASE CASE HIT  \n        \"\"\"\n        if lo &gt;=  hi  :\n            return\n        # print(self.nums, self.temp)\n        mid = (lo+hi)//2\n        self.sort(lo,mid)\n        self.sort(mid+1,hi)\n        self.merge(lo,mid,hi)\n\n    def sortArray(self, nums: List[int]) -&gt; List[int]:\n        #For writing intermediate merge results \n        self.temp = [0]*len(nums)\n        #For accessing array which needs to be sorted\n        self.nums = nums\n        self.sort(0,len(nums)-1)\n        return self.nums\n</code></pre>"},{"location":"Algorithms/Merge_Sort/#sorting-a-linked-list","title":"Sorting a linked list","text":"<p>Sorting a linked list is easier. We can actually break up the linked list in two halves in each recursive call. Sort the two halves and finally merge them. The base case is when the linked list is empty or only has a single element.</p> <p>Pay attention to the difference in the recursive implementation for array and linked list.</p> <p>For arrays, we pass pointers to the merge function to denote subarrays to be merged. For linked lists, we actually split the list and pass two separate lists.</p> Code Merge Sort on Linked List <pre><code>\"\"\"\nSolution for : https://leetcode.com/problems/sort-list/description/\nRecursively  :\n  Split the list in two halves .\n  Sort first half\n  Sort second half\n  Merge the two SORTED halves.\n\"\"\"\n# Definition for singly-linked list.\n class ListNode:\n     def __init__(self, val=0, next=None):\n         self.val = val\n         self.next = next\n\nfrom typing import Optional\n\nclass Solution:\n    def split_list(self, head: Optional[ListNode]) -&gt; (Optional[ListNode],Optional[ListNode]) :\n        \"\"\"\n        Splitting the list into two :\n        dummy-&gt;1-&gt; 2 -&gt;3-&gt;None\n            sf\n               s   f\n                   s      f  \n        s.next -&gt; second half  # s.next = None\n        dummy.next --&gt; first half\n        \"\"\"\n        dummy = ListNode(0)\n        dummy.next = head\n        s,f = dummy,dummy \n        while f and f.next :\n            f = f.next.next\n            s = s.next\n        #slow pointer stops at the PREVIOUS NODE of second half\n        second_half = s.next\n        s.next = None\n        first_half = dummy.next\n        # print(\"first half\" , first_half)\n        # print(\"second half\", second_half)\n        return first_half,second_half\n\n    def merge(self,list1 : Optional[ListNode] , list2 : Optional[ListNode]) -&gt; Optional[ListNode] : \n        p1, p2  = list1,list2\n        # print(\"merging\", p1, \" and \", p2)\n        dummy = ListNode(0)\n        p3 = dummy\n\n        while p1 and p2 :\n            if p1.val &lt; p2.val:\n                p3.next = p1\n                p1 = p1.next\n            else: \n                p3.next = p2\n                p2 = p2.next\n            p3 = p3.next\n\n        if p1 :\n            p3.next = p1\n        if p2 :\n            p3.next = p2\n        # print(\"merged\" , dummy.next)\n        return dummy.next \n\n    def _sort(self,head) :\n        if not head or not head.next :\n            return head\n        #We first SPLIT the list halves\n        first_half, second_half = self.split_list(head)\n        #Then we SORT both halves\n        #Sorting is not in place\n        #So, you have to pass the sorted lists to merge\n        sorted_first_half = self._sort(first_half)\n        sorted_second_half = self._sort(second_half)\n        #Merge the sorted halves\n        return self.merge(sorted_first_half,sorted_second_half)\n\n\n    def sortList(self, head: Optional[ListNode]) -&gt; Optional[ListNode]:\n        return self._sort(head)\n</code></pre>"},{"location":"Algorithms/Merge_Sort/#important-things-to-note","title":"Important things to note","text":"<p>As we saw earlier, quicksort is just an application of array partitioning where we can think of a partition happening at the preorder position in the traversal of a binary tree. Similarly : </p> <p>tl;dr (Again!)</p> <p>Merge sort is an application of 2-way merge where we can think of a merge happening in the post order position of a binary tree traversal.</p>"},{"location":"Algorithms/Misc_Array_Problems/Copying_and_allocating/","title":"Allocate, Copy and Transpose 2D Matrix","text":""},{"location":"Algorithms/Misc_Array_Problems/Copying_and_allocating/#allocate-copy-and-transpose-2d-matrix","title":"Allocate, Copy and Transpose 2D Matrix","text":"<p>Avoid shallow copy bugs and stick to list comprehensions when copying and allocating 2D arrays.</p>"},{"location":"Algorithms/Misc_Array_Problems/Copying_and_allocating/#allocating-a-2d-array","title":"Allocating a 2D array","text":"<p>Allocate target Matrix of num_rows* num_cols intialized to 0 </p> <p>Allocate a 2D matrix</p> <p>target = [ [0]*num_cols for _ in range(num_rows) ]</p>"},{"location":"Algorithms/Misc_Array_Problems/Copying_and_allocating/#copying-a-2d-matrix","title":"Copying a 2D Matrix","text":"<p>Create copy of 2D array</p> <pre><code>class Solution:\n  def get_copy(self,matrix: List[List[int]] ) -&gt; List[List[int]]  : \n      num_rows, num_cols = len(matrix), len(matrix[0])\n      # Using Nested List Comprehension.\n      copy = [[matrix[r][c] for c in range(num_cols)] for r in range(num_rows)]\n      return copy\n</code></pre>"},{"location":"Algorithms/Misc_Array_Problems/Copying_and_allocating/#transposing-a-2d-matrix","title":"Transposing a 2D matrix","text":"<p>In a transposed matrix, the rows become columns and columns become rows. Which means :</p> <p><code>src[r][c] == target[c][r]</code> </p> <p>OR</p> <p><code>target[r][c] == src[c][r]</code></p> <p>Transpose of a matrix</p> <pre><code>\"\"\"\nSolution for : https://leetcode.com/problems/transpose-matrix/\nYou could alternately preallocate an n*m target matrix and copy over each element from src to target matrix using a nested loop over the original matrix.\n\"\"\"\nclass Solution:\n    def transpose(self, matrix: List[List[int]]) -&gt; List[List[int]]:\n        num_rows = len(matrix)\n        num_cols = len(matrix[0])\n        # Using Nested List Comprehension. Number of Columns equal number of rows.\n        # Remember : target[r][c] = src[c][r]\n        transpose = [[matrix[c][r] for c in range(num_rows)] for r in range(num_cols)]\n        return transpose\n</code></pre>"},{"location":"Algorithms/Misc_Array_Problems/Diagonal_and_Antidiagonal/","title":"Diagonal and Antidiagonal traversal of Square Matrix","text":""},{"location":"Algorithms/Misc_Array_Problems/Diagonal_and_Antidiagonal/#diagonal-and-antidiagonal-traversal-of-square-matrix","title":"Diagonal and Antidiagonal traversal of Square Matrix","text":"<p>The important thing to note here is that the diagonal and antidiagonal overlap only in cases when the square matrix has an odd number of rows/columns and the overlapping cell will always have the same row and column index.</p> <p>Another thing to not is that the number of elements in the diagonal and antidiagonal of an n*n matrix is the same as number of rows/columns.</p> <p>As for the diagonal traversal it is easy:</p> <pre><code>Iterate n times :\n   start from [0,0] and keep adding one to both r and c.\n</code></pre> <p>For the antidiagonal, if you start from the bottom left,  the column index remains the same as that of the diagonal and the row index is n-1 - row_index of diagonal.</p> <p>So, we can iterate over the same antidiagonal:</p> <pre><code>Iterate n times :\n   start from [n-1,0] and keep adding one to c and subtracting 1 from r.\n</code></pre> <p>Of course, we only need a single loop to iterate over both. Just take care of the overlapping cell in case you are doing arithmetic on the elements.</p>"},{"location":"Algorithms/Misc_Array_Problems/Diagonal_and_Antidiagonal/#practice","title":"Practice","text":"Matrix Diagonal Sum <pre><code>\"\"\"\nSolution for : https://leetcode.com/problems/matrix-diagonal-sum/\n\nEyeball this and you will get the pattern :\n\n00 01 02\n10 11 12\n20 21 22\n\"\"\"\nclass Solution:\n    def diagonalSum(self, mat: List[List[int]]) -&gt; int:\n        total = 0\n        n = len(mat) \n        for i in range(len(mat)) :\n            total+=mat[i][i] \n            # Do not count if this is an overlapping cell\n            if (n-1) - i == i : continue\n            #Traverse Anti Diagonal from bottom left to top right\n            total+=mat[(n-1)-i][i]\n        return total \n</code></pre>"},{"location":"Algorithms/Misc_Array_Problems/In_Place_Transpose_of_a_Square_Matrix/","title":"In Place Transpose of a Square Matrix","text":""},{"location":"Algorithms/Misc_Array_Problems/In_Place_Transpose_of_a_Square_Matrix/#in-place-transpose-of-a-square-matrix","title":"In Place Transpose of a Square Matrix","text":"<p>A square matrix can be transposed in place by just swapping elements along the diagonals. The trick is to traverse only one side of the diagonal.</p> <p>The code only iterates over the lower triangle ( lower half over the diagonal excluding the diagonal itself). When you iterate over columns, iterate over [o,curr_row).</p> <p>For upper triangle iterate over [curr_row+1,n) </p> <pre><code>[\n  00  01   02  03\n *10  11   12  13\n *20 *21   22  23\n *30 *31  *32  33\n]\n</code></pre> Transposing a square matrix <pre><code>def transpose(matrix) :\n    \"\"\"\n    Transpose an n*n matrix\n    \"\"\"\n    n = len(matrix)\n    try :\n        assert n == len(matrix[0])\n    except AssertionError :\n        print(\"Not a square matrix\")\n        raise\n\n    for r in range(n) :\n        #for c in range(r+1, n) : # Iterate over the upper triangle\n        for c in range(r) : # Iterate over the lower triangle\n            print(f\"Swapping ({r},{c}) with ({c},{r})\")\n            matrix[r][c], matrix[c][r] = matrix[c][r],matrix[r][c]\n\n    return matrix\n\nif __name__ == \"__main__\" :\n    matrix = [\n        [1, 2, 3],\n        [4, 5, 6],\n        [7, 8, 9]\n    ]\n\n    print(\"Original Matrix:\")\n    for row in matrix:\n        print(row)\n\n    transposed_matrix = transpose(matrix)\n    print(\"\\nTransposed Matrix:\")\n    for row in transposed_matrix:\n        print(row)\n</code></pre> <p>If the matrix is not square, then for an input array of m*n, you will have to allocate a separate target array of dimensions n*m and copy over each element to the target array : <code>src[r][c] = target[c][r]</code></p>"},{"location":"Algorithms/Quickselect/","title":"QuickSelect","text":""},{"location":"Algorithms/Quickselect/#quickselect","title":"QuickSelect","text":""},{"location":"Algorithms/Quickselect/#defining-the-problem","title":"Defining the problem.","text":"<p>Given an array, find the sorted value in non decreasing order at the kth index (Lets assume indices are counted from zero). </p> <p>In other words what would be the value at the kth index if the array were sorted in non decreasing order?</p> <p>This is also called the kth order statistic.</p>"},{"location":"Algorithms/Quickselect/#the-algorithm","title":"The Algorithm","text":"<p>The core of the quickselect algorithm is the partitioning scheme. There are different ways of implementing quickselet based on whether the partitioning scheme puts the pivot in its sorted position or not. Here we will consider only the implementation which uses partition schemes which do, because this is simpler and more intuitive.</p> <p>Assume, you have a partitioning scheme which returns some partition index / pivot index s.t. the value at the pivot index is in its sorted position. The quickeselect algorithm makes one recursive call based on the relative position of k to the pivot index. The base case is when the pivot index is k.</p> <p>Algorithm</p> <pre><code>def quickselect(arr,lo,hi, k ) :\n    p = partition(arr,lo,hi)\n    if p == k :\n       return arr[k]\n    elif  k&lt; p :\n       return quickselect(arr,lo,p-1,k)\n    else :\n       return quickselect(arr,p+1,hi,k)\n</code></pre> <p>Here is the implementation using Hoare partitionining.Notice the difference that random pivot selection makes in the comments :</p> Code Hoare Partition with Weak Condition and fixed pivotHoare Partition with Weak Condition and random pivot <pre><code>        from typing import List\n        import random\n\n        def quickselect(arr: List[int],lo: int,hi: int,k : int) -&gt; int :\n            \"\"\"\n            Quickselect using Hoare partition with a weak condition and fixed pivot. \n\n            This is the implementation which you will find in textbooks using do..while style loops which in python becomes while True loops.\n            I prefer avoiding white True loops. \n            It's still using Hoare partition with a weak condition. But the pivot selection is not random.\n            Notice how the performance degrades to O(n^2) for sorted input array because the pivot selection is not random.\n            Compare this with next version which uses a random pivot.\n            \"\"\"\n            v = arr[lo]\n            i = lo\n            j = hi+1\n            while True:\n                while True:\n                    i += 1\n                    if not (i &lt; hi and arr[i] &lt; v):\n                        break\n                while True:\n                    j -= 1\n                    if not (j &gt; lo and arr[j] &gt; v):\n                        break\n                if i &gt;= j:\n                    break\n                arr[i], arr[j] = arr[j], arr[i]\n            arr[lo], arr[j] = arr[j], arr[lo]\n\n            print(f\"Pivot Value: {v}  ,Partitioned list : {arr}, Partition Index = {j} , le = {lo}, hi = {hi}, k = {1}\")\n            if k -1  == j :\n                print(f\"Returning index {j}\")\n                return nums[j]\n            elif k -1 &gt; j : \n                return quickselect(nums,j+1,hi,k)\n            else :\n                return quickselect(nums,lo,j-1,k)\n\n        if __name__ == \"__main__\" :\n            # nums = [5,2,1,1,1,1,1,1,1,1,1,5,5,-3,1,1,1,1,1,1,1,1,1,1,1-2,-5]               # O(nlogn)\n            nums = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]  # O(nlogn)\n            # nums = [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24]  #degrades to O(n^2)\n            # nums = [24,23,22,21,20,19,18,17,16,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1]  #O(nlogn)\n            print( \n                   f\"kth_largest = { quickselect(nums, 0, len(nums)-1 ,23) }\" \n                  )\n\n            # nums = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1] # O(nlogn)\n            # print( \n            #        f\"kth_largest = { quickselect(nums, 0, len(nums)-1 ,1) }\" \n            #       )\n</code></pre> <pre><code>         from typing import List\n         import random\n\n\n         def hoare_partition(arr, lo, hi):\n             \"\"\"\n             Entropy Optimal Hoare partition.\n             Produces balanced partitions when there are large number of duplicates.\n             [lo,right] contains elements less than or equal to pivot.\n             (right,hi]  contains elements greater than or equal to pivot.\n             Invariants for the while loop :\n             [lo] has pivot  \n             [lo, left)  &lt;= pivot    #Has values &lt;= pivot\n             (right, hi] &gt;= pivot\n             [pivot|--- &lt;=pivot-----|-----Undetermined-------|---&gt;=pivot----]\n                                     left               right\n             After execution of while loop : \n             [pivot|----&lt;=pivot------|-----&gt;=pivot------]\n             lo                  right                  hi\n             [lo,right] &lt;= pivot\n             After Putting pivot in sorted position : \n             [----&lt;=pivot----|pivot|-----&gt;=pivot------]\n             lo               right                  hi\n             Finally, return right.\n             \"\"\"\n             #PIVOT SELECTION\n             #Pick a random pivot index and always keep pivot at index lo\n             #NB:  random.randint(0,0) is 0.\n             pivot_index = random.randint(lo,hi)\n             arr[lo],arr[pivot_index] =  arr[pivot_index],arr[lo]\n             #read pivot value\n             pivot = arr[lo]\n\n             #PARTITIONING\n             #partition [lo+1,hi] ; \n             #NB : when lo == hi , while loop will not be executed\n             left,right = lo+1, hi\n             while left&lt;=right:\n                 #Move left ahead if arr[left] is strictly less than pivot value\n                 while left &lt;= right and arr[left] &lt; pivot :\n                     left+=1\n                 #Move right to the left if it is strictly higher than pivot\n                 while left &lt;= right  and arr[right] &gt; pivot :\n                     right-=1\n                 #Swap left and right and move pointers\n                 #If both values are equal to pivot this will do a swap,move pointers and effectively leave pivot values where they are. \n                 if left &lt;=  right :\n                     arr[left], arr[right] = arr[right], arr[left]\n                     right-=1\n                     left+=1\n             #Put pivot in sorted position\n             arr[lo], arr[right] = arr[right], arr[lo]\n             return right\n\n         def quickselect(arr: List[int],lo: int,hi: int,k : int) -&gt; int :\n             \"\"\"\n             Quicselect using Hoare partition with weak condition and random pivot.\n             Return vlaue at Kth SMALLEST Index,\n             Returns the non decreasingly sorted value at kth Index with indices starting from lo. [lo ,hi] is inclusive.\n             This uses Hoare's partition scheme with a weak condition and which also puts pivot in its sorted position.\n             \"\"\" \n             pivot_index = hoare_partition(arr,lo,hi)\n             print(f\"Pivot Value: {arr[pivot_index]}  ,Partitioned list : {arr}, Partition Index = {pivot_index} , le = {lo}, hi = {hi}, k = {k}\")\n             if k == pivot_index :\n                 return arr[k]\n             if k &lt; pivot_index :\n                 return quickselect(arr,lo, pivot_index-1,k)\n             elif k &gt; pivot_index :  \n                  return quickselect(arr,pivot_index+1,hi,k)\n\n\n\n         if __name__ == \"__main__\" :\n             # nums = [5,2,1,1,1,1,1,1,1,1,1,5,5,-3,1,1,1,1,1,1,1,1,1,1,1-2,-5]                 # O(nlogn)\n             # nums = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]  # Close to O(nlogn)\n             nums = [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24]  #STill close to O(nlogn) . WIthout random pivot degrades to O(n^2)\n             # nums = [24,23,22,21,20,19,18,17,16,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1]  #O(nlogn)\n             print( \n                    f\"kth_largest = { quickselect(nums, 0, len(nums)-1 ,23) }\" \n                   )\n</code></pre>"},{"location":"Algorithms/Quickselect/#additional-resources","title":"Additional Resources","text":"<ol> <li>https://en.wikipedia.org/wiki/Median_of_medians</li> </ol>"},{"location":"Algorithms/Quicksort/","title":"QuickSort","text":""},{"location":"Algorithms/Quicksort/#quicksort","title":"QuickSort","text":""},{"location":"Algorithms/Quicksort/#defining-the-problem","title":"Defining the problem.","text":"<p>Given an array, sort it in non decreasing order.</p>"},{"location":"Algorithms/Quicksort/#the-algorithm","title":"The Algorithm","text":"<p>Once you understand partitioning, quickselect and quicksort are just recursive applications of it. In quickselect, we make a single recursive call, while in quicksort, we make two recursive calls to eventually put all elements in their sorted position. Quick Sort can be visualized as a preorder traversal of a binary tree, where in the preorder position, you call the partitioning function and recursively call quicksort on both partitions. The base case is when lo &gt;= hi.</p> <p>Assume, you have a partitioning scheme which returns some partition index / pivot index s.t. the value at the pivot index is in its sorted position.</p> <p>The algorithm is :</p> <p>Algorithm</p> <pre><code>def quicksort(arr,lo,hi, k ) :\n    if lo &gt;= hi  :\n       return \n    p = partition(arr,lo,hi)\n    quicksort(arr,lo,p-1,k)\n    quicksort(arr,p+1,hi,k)\n</code></pre> <p>Implementation using Hoare Partitioning :</p> Code Hoare Partition with Weak Condition and random pivot <pre><code>         from typing import List\n         import random\n\n\n         def hoare_partition(arr, lo, hi):\n             \"\"\"\n             Entropy Optimal Hoare partition.\n             Produces balanced partitions when there are large number of duplicates.\n             [lo,right] contains elements less than or equal to pivot.\n             (right,hi]  contains elements greater than or equal to pivot.\n             Invariants for the while loop :\n             [lo] has pivot  \n             [lo, left)  &lt;= pivot    #Has values &lt;= pivot\n             (right, hi] &gt;= pivot\n             [pivot|--- &lt;=pivot-----|-----Undetermined-------|---&gt;=pivot----]\n                                     left               right\n             After execution of while loop : \n             [pivot|----&lt;=pivot------|-----&gt;=pivot------]\n             lo                  right                  hi\n             [lo,right] &lt;= pivot\n             After Putting pivot in sorted position : \n             [----&lt;=pivot----|pivot|-----&gt;=pivot------]\n             lo               right                  hi\n             Finally, return right.\n             \"\"\"\n             #PIVOT SELECTION\n             #Pick a random pivot index and always keep pivot at index lo\n             #NB:  random.randint(0,0) is 0.\n             pivot_index = random.randint(lo,hi)\n             arr[lo],arr[pivot_index] =  arr[pivot_index],arr[lo]\n             #read pivot value\n             pivot = arr[lo]\n\n             #PARTITIONING\n             #partition [lo+1,hi] ; \n             #NB : when lo == hi , while loop will not be executed\n             left,right = lo+1, hi\n             while left&lt;=right:\n                 #Move left ahead if arr[left] is strictly less than pivot value\n                 while left &lt;= right and arr[left] &lt; pivot :\n                     left+=1\n                 #Move right to the left if it is strictly higher than pivot\n                 while left &lt;= right  and arr[right] &gt; pivot :\n                     right-=1\n                 #Swap left and right and move pointers\n                 #If both values are equal to pivot this will do a swap,move pointers and effectively leave pivot values where they are. \n                 if left &lt;=  right :\n                     arr[left], arr[right] = arr[right], arr[left]\n                     right-=1\n                     left+=1\n             #Put pivot in sorted position\n             arr[lo], arr[right] = arr[right], arr[lo]\n             return right\n\n         def quicksort(arr: List[int],lo: int,hi: int) -&gt; int :\n             \"\"\"\n             Quicksort using Hoare partition with weak condition and random pivot.\n             This uses Hoare's partition scheme with a weak condition and which also puts pivot in its sorted position.\n             \"\"\" \n             if lo &gt;= hi :\n                 return\n             p = hoare_partition(arr,lo,hi)\n             quicksort(arr,lo, p-1)\n             quicksort(arr,p+1,hi)\n\n\n\n         if __name__ == \"__main__\" :\n             nums = [24,23,22,21,20,19,18,17,16,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1]\n             quicksort(nums,0,len(nums)-1)\n             print (nums)\n</code></pre>"},{"location":"Algorithms/Quicksort/#additional-references","title":"Additional References :","text":"<p>https://algs4.cs.princeton.edu/23quicksort/</p>"},{"location":"Algorithms/Trees/Introduction/","title":"What exactly is a Tree?","text":""},{"location":"Algorithms/Trees/Introduction/#what-exactly-is-a-tree","title":"What exactly is a Tree?","text":"<p>For quite some time, there used to be great confusion in my mind about the exact nature of a tree.  Some sources defined trees as undirected graphs and others said they were directed graphs. Some said that trees must have a root and others said, not really. It turns out that there are in fact two different species of 'trees'.</p> <p>Generally, when we talk about trees we are talking about rooted trees and to be even more specific, rooted out-trees. These are linked recursive data structures in which there is a strict parent-child relationship between nodes. The root node in a rooted tree does not have a parent. All other nodes do. There are no cycles and edges are directed from parent down to its children.</p> <p>A rooted out-tree in terms of graph theory is a digraph ( directed graph) such that :</p> <ul> <li>One of the vertices is a root node with no in edges.</li> <li>All other vertices have only one in-edge</li> <li>There is a path from root to all other vertices. </li> </ul> <p></p> <p>A (rooted) binary tree is one in which a node can have at most two children. If there is no such restriction, then we have a (rooted) multi-way or N-ary tree</p> <p>Technically, trees can also be undirected and thereby unrooted. These are defined as connected (there is a path between any two nodes), acyclic (a path cannot visit any  node twice) and undirected (an edge is two way connected) graphs. In other words, they are undirected graphs in which for any two vertices x and y, there is a unique simple path from x to y. A simple path is one in which vertices are not repeated. It is better to think of these as specific types of graphs and not confuse them with the usual rooted trees. They are also called free trees by the way.</p>"},{"location":"Algorithms/Trees/Introduction/#references","title":"References:","text":"<ol> <li> <p>Roooted Trees</p> </li> <li> <p>Unrooted Trees</p> </li> <li> <p>Rooted vs Unrooted</p> </li> <li> <p>Paths, Walks and Trails</p> </li> <li> <p>Some theorems on Trees</p> </li> <li> <p>Unique Simple Path Theorem</p> </li> <li> <p>In-Tree vs Out-Tree</p> </li> </ol>"},{"location":"Algorithms/Trees/Properties/","title":"Properties and Terms","text":""},{"location":"Algorithms/Trees/Properties/#properties-and-terms","title":"Properties and Terms","text":""},{"location":"Algorithms/Trees/Properties/#types-of-nodes","title":"Types of Nodes","text":"<p>The nodes in a tree can be classified into different types based on their properties :</p> <ul> <li>Root Node : Has no parent.</li> <li>Child Node : Is linked to a parent.</li> <li>Leaf node : Has No children.</li> <li>Parent Node : Has at least one child.</li> <li>Sibling Nodes : Nodes that share a parent.</li> </ul>"},{"location":"Algorithms/Trees/Properties/#what-is-an-edge","title":"What is an edge?","text":"<p>An edge represents the relationship between two nodes/vertices. It can be directed or undirected. A directed edge constrains traversal to only one direction as opposed to an undirected edge. In rooted tree an edge represents a parent child relationship between two nodes by virtue of the traversal being constrained from parent to child.</p>"},{"location":"Algorithms/Trees/Properties/#what-is-a-path","title":"What is a Path?","text":"<p>Do not confuse a path with a SET of nodes. There is a notion of ordering in a path. It is a sequence of nodes which are connected by edges. </p> <p>A path in a graph is a sequence of edges, where the endpoint of each edge is the starting point of the next edge.  We can have undirected paths in an undirected graph or directed paths in a directed graph. [Ref. 5]</p> <p>In a rooted tree an edge is always directed from parent to child. (This is the subtle reason why backtracking to the root node is necessary in depth first tree traversals to visit the right subtree after having visited the left subtree)</p>"},{"location":"Algorithms/Trees/Properties/#properties","title":"Properties:","text":"<ul> <li>Depth (of a node): The number of edges between the node and the root. Root node has a depth of 0.</li> <li>Level (of a node): Same as depth, but conventionally we start counting levels from 1. Root node is at level 1. Used more in the context of level order traversals.</li> <li>Height (of a tree or subtree): The length of the path between the root node (of a tree or subtree) and the deepest descendant of the root node. Height of a node is simply the height of the subtree rooted at that node. The height of a tree is the height of the root node.</li> <li>Length (of a path) :  Number of edges in a path</li> <li>Degree (of a node) : Total number of children of a node.</li> </ul>"},{"location":"Algorithms/Trees/Traversals/","title":"Depth First Binary Tree Traversals","text":""},{"location":"Algorithms/Trees/Traversals/#depth-first-binary-tree-traversals","title":"Depth First Binary Tree Traversals","text":"<p>There are two fundamental ways of traversing trees :</p> <ul> <li>Depth First </li> <li>Level Order or Breadth First</li> </ul> <p>Here , we will mainly concern ourselves with depth first traversals. When traversing binary trees depth first, you can do it in three different ways depending on when you visit the root.</p>"},{"location":"Algorithms/Trees/Traversals/#inorder","title":"Inorder","text":"<p>Left - Visit - Right(LVR) A.K.A Left - Node - Right(LNR) </p> <p>Begins at the left-most node, ends at the rightmost node. The left subtree is explored first, the root is visited next and the right subtree is explored last.</p> <pre><code>                        4\n                      /   \\\n                     /     \\  \n                    2       6 \n                   / \\     / \\ \n                  /   \\   /   \\\n                 1     3  5    7\n</code></pre> Recursive solution using implicit stack <pre><code>\"\"\"\nIn-Order Traversal : \nLeft - Root - RIght\n\"\"\"\n# Definition for a binary tree node.\n# class TreeNode:\n#     def __init__(self, val=0, left=None, right=None):\n#         self.val = val\n#         self.left = left\n#         self.right = right\nclass Solution:\n    def inorderTraversal(self, root: Optional[TreeNode]) -&gt; List[int]:\n\n        res = []\n\n        def traverse_inorder(root) :\n            #Base Case\n            if not root :\n                return  \n\n            # Explore Left Subtree\n            traverse_inorder(root.left)\n\n            #Visit the root\n            res.append(root.val)\n\n            #Explore Right Subtree\n            traverse_inorder(root.right)\n\n        traverse_inorder(root)\n        return res \n</code></pre> Iterative solution using explicit stack <pre><code># Definition for a binary tree node.\n# class TreeNode:\n#     def __init__(self, val=0, left=None, right=None):\n#         self.val = val\n#         self.left = left\n#         self.right = right2\n\"\"\"\nSolution for : https://leetcode.com/problems/binary-tree-inorder-traversal/description\n\nInorder traversal is :\nLeft - Visit - Right\n\nIterative solution using explicit stack.\nThis method simulates the recursive call stack with an explicit stack.\nThe core idea is:\n1. Keep traversing left and pushing nodes onto the stack until you can't go left anymore.\n2. Pop a node, visit it (add its value to the result).\n3. Move to the right child of the popped node and repeat the process.\n\"\"\"\nclass Solution:\n    def inorderTraversal(self, root: Optional[TreeNode]) -&gt; List[int]:\n        stack=[]\n        res=[]\n        current = root\n        # Loop as long as there are nodes to process \n        while current or stack:\n            #Explore the left subtree.If current is not NULL, we  go left till the leftmost leaf node\n            #If current is NULL, stack must be non empty. With stack top either having the parent node of a previous visit or a leaf node which we can consider to be the parent of a NULL node. In this case, we will not enter this loop and proceed to the ascent phase in the next step.\n            while current :\n                stack.append(current)\n                current = current.left \n            # Backtrack to the parent\n            parent = stack.pop()\n            #Visit the parent\n            res.append(parent.val)\n            #Explore the right subtree\n            current = parent.right   \n        return res\n</code></pre> <p>The recursive version is usually well understood. However, a detailed explanation of the iterative implementation is warranted here.  The iterative version essentially simulates the function call stack using an explicit stack.</p> <p>The best way to understand the algorithm is in terms of a two state machine which is controlled by the predicate <code>while current or stack</code>. Think of current as the root of the subtree which is currently being explored. This is initialzed to the root of the entire tree. The stack represents the suspended execution context of a Node which is to be resumed and processed later.</p> <p>The state machine is in either one of two states. </p> <p>State 1: Descent Phase (current != null)</p> <ul> <li>Action: Push the current node onto the stack and advance the pointer to its left child (current = current.left).</li> <li>Purpose: To traverse the leftmost spine of the current subtree. This phase corresponds to recursively exploring the Left component of the LNR pattern. The stack stores the execution context (parent nodes) to be resumed.</li> </ul> <p>State 2: Ascent &amp; Visit Phase or backtracking (current == null and stack is not empty)</p> <p>Action: Pop a node from the stack. This node's left subtree is now fully processed. Action: Visit the popped node (append its value to the result list). This is the Node (or Root) component of LNR. Action: Set current to the popped node's right child to initiate a new descent phase into the Right subtree.</p> <p>Invariant: A node is only visited (popped and its value recorded) after its left subtree has been completely exhausted, perfectly enforcing the Left-Node-Right sequence.</p> <p>The terminal state of the machine is reached when current is null and stack is empty. This means there is no active subtree to explore and suspended parent context to resume.</p>"},{"location":"Algorithms/Trees/Traversals/#preorder","title":"Preorder","text":"<p>Visit - Left - Right(VLR) A.K.A. Node-Left-Right(NLR) </p> <p>Begins at root and ends at the right most node. The root node is visited first and then left and right subtrees are explored recursively.</p> <pre><code>                        1\n                      /   \\\n                     /     \\  \n                    2       5 \n                   / \\     / \\ \n                  /   \\   /   \\\n                 3     4  6    7\n</code></pre> Recursive solution using implicit stack <pre><code># Definition for a binary tree node.\n# class TreeNode:\n#     def __init__(self, val=0, left=None, right=None):\n#         self.val = val\n#         self.left = left\n#         self.right = right\n\n\"\"\"\nSolution for : https://leetcode.com/problems/binary-tree-preorder-traversal/\nPreOrder traversal : Visit - Left - Right\nRecursive Solution\n\"\"\"\nclass Solution:\n    def preorderTraversal(self, root: Optional[TreeNode]) -&gt; List[int]:\n\n        res = [] \n\n        def traverse_preorder(root) : \n            #Base case\n            if not root : return\n\n            #Visit the root node\n            res.append(root.val)\n\n            #Explore Left Subtree\n            traverse_preorder(root.left) \n\n            #Explore Right Subtree\n            traverse_preorder(root.right)\n\n        traverse_preorder(root)\n        return res\n</code></pre> Iterative solution using explicit stack <pre><code># Definition for a binary tree node.\n# class TreeNode:\n#     def __init__(self, val=0, left=None, right=None):\n#         self.val = val\n#         self.left = left\n#         self.right = right\n\n\"\"\"\nSolution for : https://leetcode.com/problems/binary-tree-preorder-traversal/\nPreOrder traversal : Visit - Left - Right\nIterative Solution\n\"\"\"\nclass Solution:\n    def preorderTraversal(self, root: Optional[TreeNode]) -&gt; List[int]:\n\n        stack = []\n        res = [] \n        current = root\n        while current or stack:\n            #Visit the root of this subtree and explore its left subtree \n            while current :\n                res.append(current.val)\n                stack.append(current)\n                current = current.left\n            # Backtrack to Parent and explore its right subtree    \n            parent = stack.pop()\n            current = parent.right\n\n        return res\n</code></pre> <p>The iterative solution for preorder traversal uses the same two-state machine, but performs the \"visit\" action before exploring the left subtree.</p> <p>State 1: Visit &amp; Descent Phase (current != null)</p> <ul> <li> <p>Action: Visit the current node immediately (append its value to the result list). This is the Node (or Root) component of NLR.</p> </li> <li> <p>Action: Push the current node onto the stack to save its context (specifically, its right child) for later.</p> </li> <li> <p>Action: Advance the pointer to its left child (current = current.left). This explores the Left component.</p> </li> </ul> <p>State 2: Ascent Phase or Backtracking (current == null and stack is not empty)</p> <ul> <li>Action: Pop a node from the stack. Its left subtree has been fully processed.</li> <li>Action: Set current to the popped node's right child to initiate a new visit &amp; descent phase into the Right subtree.</li> </ul> <p>Invariant: A node is visited the moment it is first encountered, before any descent into its children, enforcing the Node-Left-Right sequence.</p>"},{"location":"Algorithms/Trees/Traversals/#postorder","title":"Postorder","text":"<p>Left - Right - Visit(LRV) A.K.A Left - Right - Node (LRN)</p> <p>Begins at the left most node and ends at the root node. </p> <pre><code>                        7\n                      /   \\\n                     /     \\  \n                    3       6 \n                   / \\     / \\ \n                  /   \\   /   \\\n                 1     2  4    5\n</code></pre> Recursive solution using implicit stack <pre><code># Definition for a binary tree node.\n# class TreeNode:\n#     def __init__(self, val=0, left=None, right=None):\n#         self.val = val\n#         self.left = left\n#         self.right = right\nclass Solution:\n    def postorderTraversal(self, root: Optional[TreeNode]) -&gt; List[int]:\n        res = []\n        def traverse_postorder(root) :\n            #base case\n            if not root : return \n\n            # left\n            traverse_postorder(root.left)\n            # right\n            traverse_postorder(root.right)\n            # visit\n            res.append(root.val)\n        traverse_postorder(root)\n        return res\n</code></pre> Iterative solution using explicit stack <pre><code>   # Definition for a binary tree node.\n   # class TreeNode:\n   #     def __init__(self, val=0, left=None, right=None):\n   #         self.val = val\n   #         self.left = left\n   #         self.right = right\n   class Solution:\n       def postorderTraversal(self, root: Optional[TreeNode]) -&gt; List[int]:\n\n           stack = []\n           res = []\n           last_visited = None\n           current = root\n\n           while current or stack :\n               #State 1 : Descent \n               #Explore the left subtree\n               while current :\n                   stack.append(current)\n                   current = current.left\n               peek_node = stack[-1]\n               #State 2 : Ascent \n               #Decision A\n               # The right subtree is not explored yet\n               # Tip : Don't forget to check that peek_node.right is not None or you will end up in an infinite loop by comparing None with None.\n               if   peek_node.right and peek_node.right != last_visited : \n                   current = peek_node.right # Descend into right subtree\n               #Decision B\n               # Either We already processed the right subtree or the right subtree is NULL. \n               # So, it is time to visit its parent\n               else :\n                   parent = stack.pop()\n                   res.append(parent.val)\n                   last_visited = parent\n                   # Continue ascent by keep current as NULL\n\n           return res\n</code></pre> <p>Iterative postorder traversal is the most complex of the three because it requires us to delay visiting a node until both of its children have been fully processed. This introduces a new challenge: when we return to a node from a subtree, we must know whether we came from the left (and must now go right) or came from the right (and can finally visit the node). The most common and efficient one-stack solution uses a last_visited_node pointer to check if the right subtree has already been processed.</p> <p>The iterative algorithm using a single stack operates as a two-state machine, but its second state is more complex, involving a critical decision-making step.</p> <p>State 1: Descent Phase (current != null)</p> <ul> <li>Action: Push the current node onto the stack and advance the pointer to its left child (current = current.left).</li> <li>Purpose: To traverse and stack the leftmost spine of any given subtree. This corresponds to exploring the Left component of the LRN pattern. The node is pushed to suspend its execution.</li> </ul> <p>State 2: Ascent &amp; Decision Phase (current == null and stack is not empty)</p> <ul> <li>Action: Peek at the node on top of the stack (peek_node). Do not pop it yet.</li> <li>Purpose: The algorithm must now decide its next move. We are at peek_node because its left subtree is fully processed. The question is: has its right subtree also been processed?</li> <li>Decision Logic: If peek_node has a right child AND that child is NOT the last_visited_node: This means the Right subtree must be explored. The algorithm transitions back to the Descent Phase by setting current = peek_node.right. Else (if peek_node has no right child OR its right child was the last_visited_node): This confirms that both the Left and Right subtrees are fully processed. The algorithm can now perform the visit.</li> <li>Action: Pop the node from the stack.</li> <li>Action: Visit the node by appending its value to the result list. This is the Node component of LRN.</li> <li>Action: Update last_visited_node to this newly visited node.</li> <li>Action: Keep current = None. This is crucial. It ensures the while loop continues in the Ascent &amp; Decision Phase, processing the next node on the stack.</li> </ul> <p>Invariant: A node is only visited (popped and its value recorded) after the algorithm confirms that both its left child (by completing the initial descent) and its right child (by checking last_visited_node) have been fully processed, strictly enforcing the Left-Right-Node sequence.</p> Iterative solution using modified preorder traversal <pre><code># Definition for a binary tree node.\n# class TreeNode:\n#     def __init__(self, val=0, left=None, right=None):\n#         self.val = val\n#         self.left = left\n#         self.right = right\nclass Solution:\n    def postorderTraversal(self, root: Optional[TreeNode]) -&gt; List[int]:\n        stack = []\n        res = []\n        current = root\n\n        while current or stack :\n            #Visit and descent\n            #Vist current node and descend into the right subtree\n            while current : \n                #visit \n                res.append(current.val)\n                #store execution state on stack for backtracking to left subtree later\n                stack.append(current)\n                #descend into the right subtree\n                current = current.right\n\n            #ascent\n            #backtrack \n            parent = stack.pop()\n            #and descend to the left subtree\n            current = parent.left\n\n        res.reverse()\n        return res\n</code></pre> <p>You could avoid all this headache by exploiting the relationship between Preorder and Postorder traversals. </p> <ul> <li>Preorder : Node - Left - Right</li> <li>Postorder : Left - Right - Node</li> </ul> <p>Notice that if we reverse the preorder sequence of actions we get Right - Left - Node. But what if we did the preorder traversal like this : Node - Right - Left?</p> <p>Now, the reversal of this sequence is Left - Right - Node which is exactly the same as postorder traversal.</p> <p>Is is as simple as that! You can do all three iterative traversals using the simpler state machine approach.</p>"},{"location":"Algorithms/Trees/Traversals/#a-note-on-the-role-of-the-stack","title":"A Note on the role of the stack","text":"<p>In all three iterative traversals, the stack's fundamental role is to simulate the function call stack of recursion by saving the execution context of a node so its processing can be suspended and resumed later. This is done by backtracking which corresponds to popping a node from the stack to return to a previously suspended execution context.</p> <p>The key difference lies in what specific part of the execution context is being saved:</p> <ul> <li> <p>Inorder (LNR): When a node is pushed, you are saving the context to first visit this node, then explore its right subtree. This entire block of work is deferred until after its left subtree is fully processed.</p> </li> <li> <p>Preorder (NLR): Since you visit the node before pushing, you are only saving the context to explore its right subtree. The \"visit\" part of its context is already complete.</p> </li> <li> <p>Postorder (LRN): When a node is pushed, you are saving the most complex context: the obligation to first explore its left and right subtrees, and then finally visit the node itself. Its execution is suspended until both children's subtrees are fully processed.</p> </li> </ul>"},{"location":"Algorithms/Two_Way_Merge/","title":"Merging Sorted Containers","text":""},{"location":"Algorithms/Two_Way_Merge/#merging-sorted-containers","title":"Merging Sorted Containers","text":"<p>Before considering K-way merge, let us first look at the simpler version of the problem.</p>"},{"location":"Algorithms/Two_Way_Merge/#the-2-way-merge-problem","title":"The 2-Way merge problem.","text":""},{"location":"Algorithms/Two_Way_Merge/#merging-two-sorted-arrays","title":"Merging two sorted arrays","text":"<p>Given 2 integer arrays, sorted in non decreasing order, merge them into a single sorted array. </p> Code Two Way Merge on ArraysUnit Tests for Two Merge on Arrays <pre><code>from typing import List\n\ndef merge_two(arr1 :  List[int], arr2 : List[int]) -&gt; List[int] :\n    \"\"\"\n    Given two arrays sorted in non decreasing order, return a merged array \n    \"\"\"\n\n    m, n = len(arr1) , len(arr2)\n\n    res = [0]*(m+n)\n\n    p1,p2,p3 = 0,0,0\n\n    while p1 &lt; m  and p2 &lt; n  :\n\n        if arr1[p1] &lt; arr2[p2] : \n            res[p3] = arr1[p1]\n            p3+=1\n            p1+=1\n        else :\n            res[p3] = arr2[p2]\n            p3+=1\n            p2+=1\n\n\n    while p1 &lt; m :\n        res[p3] = arr1[p1]\n        p1+=1\n        p3+=1\n\n    while p2 &lt; n :\n        res[p3] = arr2[p2]\n        p2+=1\n        p3+=1\n\n    return res\n\n\nif __name__ == \"__main__\" :\n    print(merge_two([4,5,7] , [3,6,7,8]))\n    print(merge_two([4,5,7] , [3,6]))\n    print(merge_two([] , [1,2,3]))\n    print( merge_two( [1, 2, 2, 5] , [2, 3, 4, 4] ) ) # [1, 2, 2, 2, 3, 4, 4, 5]\n    print( merge_two( [1, 3, 5, 7] , [2, 4, 6, 8] ) ) # [1, 2, 3, 4, 5, 6, 7, 8]\n</code></pre> <pre><code>\"\"\"\nUnit Tests for Two way merge.\n\n# Aside : See these for an introduction to pytest : https://realpython.com/pytest-python-testing/\n              https://gist.github.com/kwmiebach/3fd49612ef7a52b5ce3a\n\"\"\"\nimport pytest\nfrom two_way_merge import merge_two \n\n\n# --- Test Cases ---\n\ndef test_merge_basic():\n    \"\"\"Tests merging two non-empty lists.\"\"\"\n    arr1 = [1, 3, 5, 7]\n    arr2 = [2, 4, 6, 8]\n    expected = [1, 2, 3, 4, 5, 6, 7, 8]\n    # Note: The original buggy code might return [1, 2, 3, 4, 5, 6, 7, 9]\n    assert merge_two(arr1, arr2) == expected\n\ndef test_merge_empty_first():\n    \"\"\"Tests merging when the first list is empty.\"\"\"\n    arr1 = []\n    arr2 = [2, 4, 6]\n    expected = [2, 4, 6]\n    assert merge_two(arr1, arr2) == expected\n    # Test swap logic too\n    assert merge_two(arr2, arr1) == expected\n\ndef test_merge_empty_second():\n    \"\"\"Tests merging when the second list is empty.\"\"\"\n    arr1 = [1, 3, 5]\n    arr2 = []\n    expected = [1, 3, 5]\n    assert merge_two(arr1, arr2) == expected\n    # Test swap logic too\n    assert merge_two(arr2, arr1) == expected\n\ndef test_merge_both_empty():\n    \"\"\"Tests merging when both lists are empty.\"\"\"\n    arr1 = []\n    arr2 = []\n    expected = []\n    assert merge_two(arr1, arr2) == expected\n\ndef test_merge_duplicates():\n    \"\"\"Tests merging lists with duplicate numbers.\"\"\"\n    arr1 = [1, 2, 2, 5]\n    arr2 = [2, 3, 4, 4]\n    expected = [1, 2, 2, 2, 3, 4, 4, 5]\n    # Note: Original buggy code might return [1, 2, 2, 2, 3, 4, 4, 9]\n    assert merge_two(arr1, arr2) == expected\n\ndef test_merge_duplicates_across_lists():\n    \"\"\"Tests merging lists where duplicates exist between lists.\"\"\"\n    arr1 = [1, 3, 5]\n    arr2 = [1, 3, 5]\n    expected = [1, 1, 3, 3, 5, 5]\n    # Note: Original buggy code might return [1, 1, 3, 3, 5, 9]\n    assert merge_two(arr1, arr2) == expected\n\ndef test_merge_interleaved():\n    \"\"\"Tests merging lists with highly interleaved numbers.\"\"\"\n    arr1 = [1, 4, 5, 8]\n    arr2 = [2, 3, 6, 7]\n    expected = [1, 2, 3, 4, 5, 6, 7, 8]\n    # Note: Original buggy code might return [1, 2, 3, 4, 5, 6, 7, 9]\n    assert merge_two(arr1, arr2) == expected\n\ndef test_merge_one_list_much_shorter():\n    \"\"\"Tests merging when one list is significantly shorter.\"\"\"\n    arr1 = [1, 2, 10, 11, 12]\n    arr2 = [3, 4]\n    expected = [1, 2, 3, 4, 10, 11, 12]\n    # Note: Original buggy code might return [1, 2, 3, 4, 10, 11, 9]\n    assert merge_two(arr1, arr2) == expected\n    # Test swap logic too\n    assert merge_two(arr2, arr1) == expected # Should give same result\n\ndef test_merge_all_elements_smaller():\n    \"\"\"Tests merging when all elements of one list are smaller than the other.\"\"\"\n    arr1 = [1, 2, 3]\n    arr2 = [4, 5, 6]\n    expected = [1, 2, 3, 4, 5, 6]\n    # Note: Original buggy code might return [1, 2, 3, 4, 5, 9]\n    assert merge_two(arr1, arr2) == expected\n    # Test swap logic too\n    assert merge_two(arr2, arr1) == expected # Should give same result\n\ndef test_merge_negative_numbers():\n    \"\"\"Tests merging lists with negative numbers.\"\"\"\n    arr1 = [-5, -1, 0, 10]\n    arr2 = [-3, -2, 8, 12]\n    expected = [-5, -3, -2, -1, 0, 8, 10, 12]\n    # Note: Original buggy code might return [-5, -3, -2, -1, 0, 8, 10, 9]\n    assert merge_two(arr1, arr2) == expected\n\ndef test_merge_single_element_lists():\n    \"\"\"Tests merging lists with single elements.\"\"\"\n    arr1 = [5]\n    arr2 = [2]\n    expected = [2, 5]\n    # Note: Original buggy code might return [2, 9] due to off-by-one loop\n    assert merge_two(arr1, arr2) == expected\n    assert merge_two(arr2, arr1) == expected\n</code></pre>"},{"location":"Algorithms/Two_Way_Merge/#merging-two-sorted-lists","title":"Merging two sorted lists","text":"<p>The same thing for a linked list :</p> Code Two Way Merge on Lists <pre><code>\"\"\"\nSolution for  https://leetcode.com/problems/merge-two-sorted-lists/description/\n\"\"\"\nfrom typing import Optional\nclass ListNode:\n    def __init__(self, val=0, next=None):\n        self.val = val\n        self.next = next\n\nclass Solution:\n    def mergeTwoLists(self, list1: Optional[ListNode], list2: Optional[ListNode]) -&gt; Optional[ListNode]:\n        p1, p2  = list1,list2\n        dummy = ListNode(0)\n        p3 = dummy\n\n        while p1 and p2 :\n            if p1.val &lt; p2.val:\n                p3.next = p1\n                p1 = p1.next\n            else: \n                p3.next = p2\n                p2 = p2.next\n            p3 = p3.next\n\n        if p1 :\n            p3.next = p1\n        else :\n            p3.next = p2\n\n        return dummy.next \n</code></pre> <p>So, how are we solving the problem of two way merging?</p> <p>We have a write pointer p3 and we have two read pointers, p1 and p2. In each iteration of the while loop, we pick the smaller read pointer to write to the write location and move both pointers (write pointer and one of the read pointers) ahead.</p> <p>If either p1 or p2 have still not reached the end of their read containers, we just write the rest of that read container at the write pointer.</p>"},{"location":"Algorithms/Two_Way_Merge/#practice","title":"Practice","text":"Merge sorted arrays backwards <pre><code>\"\"\"\nSolution for https://leetcode.com/problems/merge-sorted-array/\n\nA slightly different variation of the two-way merge problem. \nThe challenge here is to merge one array into another array \nbackwards into extra space allocated at the end of one of the arrays.\n\nTo accomplish this at each step we pick the greatest among the the two choices and put it at the end of the target array.\n\np3  -&gt; write pointer\np1, p2 -&gt; read pointers\nnums1 -&gt; target array\n\n\n\np3 will never overwrite p1 because :\n\nInitially p3 - p1 = n,\nIf p2 is decremented, the gap p3 - p1 reduces by 1, because p3 is incremented and p1 remains the same.\nIf p1 is decremented, the gap p3 - p1 remains the same \n0&lt;= p3-p1 &lt;= n # The gap can at most become zero.\nWhen,\np3 -p1 = 0 &lt;-&gt; p2  = -1\nWhich means all of nums2 has been written into nums1, the while loop will not execute.\nTherefore p3 will never overwrite p1.\n\n\nDry Run these to see how it works out :\n\nnums1 :[2,3,0]\nnums2 :[4]\n\nnums1 :[2,3,0]\nnums2 : [-1]\n\"\"\"\nclass Solution:\n    def merge(self, nums1: List[int], m: int, nums2: List[int], n: int) -&gt; None:\n        \"\"\"\n        Do not return anything, modify nums1 in-place instead.\n        \"\"\"\n        p1,p2,p3 = m-1, n-1, len(nums1) -1 \n        while p2 &gt;= 0 :\n            if p1 &gt;=0 and nums1[p1] &gt; nums2[p2] : \n                nums1[p3] = nums1[p1]\n                p1-=1\n                p3-=1\n            else :\n                nums1[p3] = nums2[p2]\n                p2-=1\n                p3-=1\n\n        return nums1\n</code></pre> Merge Strings Alternately <pre><code>\"\"\"\nA simpler version of the two way merge problem.\nAll you have to do is pick the next element alternately among the two choices.\nThe EFFECT can be achieved by simply picking a letter from each array one after the other.\nWrap this up in s while loop runs until either one of the arrays have not finished being read. \n\"\"\"\nclass Solution:\n    def mergeAlternately(self, word1: str, word2: str) -&gt; str:\n\n        res = []\n        p1,p2 =0,0\n\n        while p1 &lt; len(word1) or p2 &lt; len(word2) :\n            if p1 &lt; len(word1) : \n                res.append(word1[p1]) \n                p1+=1\n            if p2 &lt; len(word2) : \n                res.append(word2[p2])\n                p2+=1\n        return \"\".join(res)\n</code></pre>"},{"location":"blog/","title":"Blog","text":""},{"location":"blog/#blog","title":"Blog","text":""},{"location":"blog/depth_first_binary_tree_traversals/","title":"Depth First Binary Tree Traversals","text":"<p>Let us build a rock solid mental model of iterative implementation of depth first binary tree traversals based on state machines.</p> <p></p> <p>There are two fundamental ways of traversing trees :</p> <ul> <li>Depth First </li> <li>Level Order or Breadth First</li> </ul> <p>Here , we will mainly concern ourselves with depth first traversals. When traversing binary trees depth first, you can do it in three different ways depending on when you visit the root.</p>","tags":["Data_Structures","Algorithms"]},{"location":"blog/depth_first_binary_tree_traversals/#inorder","title":"Inorder","text":"<p>Left - Visit - Right(LVR) A.K.A Left - Node - Right(LNR) </p> <p>Begins at the left-most node, ends at the rightmost node. The left subtree is explored first, the root is visited next and the right subtree is explored last.</p> <pre><code>                        4\n                      /   \\\n                     /     \\  \n                    2       6 \n                   / \\     / \\ \n                  /   \\   /   \\\n                 1     3  5    7\n</code></pre> Recursive solution using implicit stack <pre><code>\"\"\"\nIn-Order Traversal : \nLeft - Root - RIght\n\"\"\"\n# Definition for a binary tree node.\n# class TreeNode:\n#     def __init__(self, val=0, left=None, right=None):\n#         self.val = val\n#         self.left = left\n#         self.right = right\nclass Solution:\n    def inorderTraversal(self, root: Optional[TreeNode]) -&gt; List[int]:\n\n        res = []\n\n        def traverse_inorder(root) :\n            #Base Case\n            if not root :\n                return  \n\n            # Explore Left Subtree\n            traverse_inorder(root.left)\n\n            #Visit the root\n            res.append(root.val)\n\n            #Explore Right Subtree\n            traverse_inorder(root.right)\n\n        traverse_inorder(root)\n        return res \n</code></pre> Iterative solution using explicit stack <pre><code># Definition for a binary tree node.\n# class TreeNode:\n#     def __init__(self, val=0, left=None, right=None):\n#         self.val = val\n#         self.left = left\n#         self.right = right2\n\"\"\"\nSolution for : https://leetcode.com/problems/binary-tree-inorder-traversal/description\n\nInorder traversal is :\nLeft - Visit - Right\n\nIterative solution using explicit stack.\nThis method simulates the recursive call stack with an explicit stack.\nThe core idea is:\n1. Keep traversing left and pushing nodes onto the stack until you can't go left anymore.\n2. Pop a node, visit it (add its value to the result).\n3. Move to the right child of the popped node and repeat the process.\n\"\"\"\nclass Solution:\n    def inorderTraversal(self, root: Optional[TreeNode]) -&gt; List[int]:\n        stack=[]\n        res=[]\n        current = root\n        # Loop as long as there are nodes to process \n        while current or stack:\n            #Explore the left subtree.If current is not NULL, we  go left till the leftmost leaf node\n            #If current is NULL, stack must be non empty. With stack top either having the parent node of a previous visit or a leaf node which we can consider to be the parent of a NULL node. In this case, we will not enter this loop and proceed to the ascent phase in the next step.\n            while current :\n                stack.append(current)\n                current = current.left \n            # Backtrack to the parent\n            parent = stack.pop()\n            #Visit the parent\n            res.append(parent.val)\n            #Explore the right subtree\n            current = parent.right   \n        return res\n</code></pre> <p>The recursive version is usually well understood. However, a detailed explanation of the iterative implementation is warranted here.  The iterative version essentially simulates the function call stack using an explicit stack.</p> <p>The best way to understand the algorithm is in terms of a two state machine which is controlled by the predicate <code>while current or stack</code>. Think of current as the root of the subtree which is currently being explored. This is initialzed to the root of the entire tree. The stack represents the suspended execution context of a Node which is to be resumed and processed later.</p> <p>The state machine is in either one of two states. </p> <p>State 1: Descent Phase (current != null)</p> <ul> <li>Action: Push the current node onto the stack and advance the pointer to its left child (current = current.left).</li> <li>Purpose: To traverse the leftmost spine of the current subtree. This phase corresponds to recursively exploring the Left component of the LNR pattern. The stack stores the execution context (parent nodes) to be resumed.</li> </ul> <p>State 2: Ascent &amp; Visit Phase or backtracking (current == null and stack is not empty)</p> <p>Action: Pop a node from the stack. This node's left subtree is now fully processed. Action: Visit the popped node (append its value to the result list). This is the Node (or Root) component of LNR. Action: Set current to the popped node's right child to initiate a new descent phase into the Right subtree.</p> <p>Invariant: A node is only visited (popped and its value recorded) after its left subtree has been completely exhausted, perfectly enforcing the Left-Node-Right sequence.</p> <p>The terminal state of the machine is reached when current is null and stack is empty. This means there is no active subtree to explore and suspended parent context to resume.</p>","tags":["Data_Structures","Algorithms"]},{"location":"blog/depth_first_binary_tree_traversals/#preorder","title":"Preorder","text":"<p>Visit - Left - Right(VLR) A.K.A. Node-Left-Right(NLR) </p> <p>Begins at root and ends at the right most node. The root node is visited first and then left and right subtrees are explored recursively.</p> <pre><code>                        1\n                      /   \\\n                     /     \\  \n                    2       5 \n                   / \\     / \\ \n                  /   \\   /   \\\n                 3     4  6    7\n</code></pre> Recursive solution using implicit stack <pre><code># Definition for a binary tree node.\n# class TreeNode:\n#     def __init__(self, val=0, left=None, right=None):\n#         self.val = val\n#         self.left = left\n#         self.right = right\n\n\"\"\"\nSolution for : https://leetcode.com/problems/binary-tree-preorder-traversal/\nPreOrder traversal : Visit - Left - Right\nRecursive Solution\n\"\"\"\nclass Solution:\n    def preorderTraversal(self, root: Optional[TreeNode]) -&gt; List[int]:\n\n        res = [] \n\n        def traverse_preorder(root) : \n            #Base case\n            if not root : return\n\n            #Visit the root node\n            res.append(root.val)\n\n            #Explore Left Subtree\n            traverse_preorder(root.left) \n\n            #Explore Right Subtree\n            traverse_preorder(root.right)\n\n        traverse_preorder(root)\n        return res\n</code></pre> Iterative solution using explicit stack <pre><code># Definition for a binary tree node.\n# class TreeNode:\n#     def __init__(self, val=0, left=None, right=None):\n#         self.val = val\n#         self.left = left\n#         self.right = right\n\n\"\"\"\nSolution for : https://leetcode.com/problems/binary-tree-preorder-traversal/\nPreOrder traversal : Visit - Left - Right\nIterative Solution\n\"\"\"\nclass Solution:\n    def preorderTraversal(self, root: Optional[TreeNode]) -&gt; List[int]:\n\n        stack = []\n        res = [] \n        current = root\n        while current or stack:\n            #Visit the root of this subtree and explore its left subtree \n            while current :\n                res.append(current.val)\n                stack.append(current)\n                current = current.left\n            # Backtrack to Parent and explore its right subtree    \n            parent = stack.pop()\n            current = parent.right\n\n        return res\n</code></pre> <p>The iterative solution for preorder traversal uses the same two-state machine, but performs the \"visit\" action before exploring the left subtree.</p> <p>State 1: Visit &amp; Descent Phase (current != null)</p> <ul> <li> <p>Action: Visit the current node immediately (append its value to the result list). This is the Node (or Root) component of NLR.</p> </li> <li> <p>Action: Push the current node onto the stack to save its context (specifically, its right child) for later.</p> </li> <li> <p>Action: Advance the pointer to its left child (current = current.left). This explores the Left component.</p> </li> </ul> <p>State 2: Ascent Phase or Backtracking (current == null and stack is not empty)</p> <ul> <li>Action: Pop a node from the stack. Its left subtree has been fully processed.</li> <li>Action: Set current to the popped node's right child to initiate a new visit &amp; descent phase into the Right subtree.</li> </ul> <p>Invariant: A node is visited the moment it is first encountered, before any descent into its children, enforcing the Node-Left-Right sequence.</p>","tags":["Data_Structures","Algorithms"]},{"location":"blog/depth_first_binary_tree_traversals/#postorder","title":"Postorder","text":"<p>Left - Right - Visit(LRV) A.K.A Left - Right - Node (LRN)</p> <p>Begins at the left most node and ends at the root node. </p> <pre><code>                        7\n                      /   \\\n                     /     \\  \n                    3       6 \n                   / \\     / \\ \n                  /   \\   /   \\\n                 1     2  4    5\n</code></pre> Recursive solution using implicit stack <pre><code># Definition for a binary tree node.\n# class TreeNode:\n#     def __init__(self, val=0, left=None, right=None):\n#         self.val = val\n#         self.left = left\n#         self.right = right\nclass Solution:\n    def postorderTraversal(self, root: Optional[TreeNode]) -&gt; List[int]:\n        res = []\n        def traverse_postorder(root) :\n            #base case\n            if not root : return \n\n            # left\n            traverse_postorder(root.left)\n            # right\n            traverse_postorder(root.right)\n            # visit\n            res.append(root.val)\n        traverse_postorder(root)\n        return res\n</code></pre> Iterative solution using explicit stack <pre><code>   # Definition for a binary tree node.\n   # class TreeNode:\n   #     def __init__(self, val=0, left=None, right=None):\n   #         self.val = val\n   #         self.left = left\n   #         self.right = right\n   class Solution:\n       def postorderTraversal(self, root: Optional[TreeNode]) -&gt; List[int]:\n\n           stack = []\n           res = []\n           last_visited = None\n           current = root\n\n           while current or stack :\n               #State 1 : Descent \n               #Explore the left subtree\n               while current :\n                   stack.append(current)\n                   current = current.left\n               peek_node = stack[-1]\n               #State 2 : Ascent \n               #Decision A\n               # The right subtree is not explored yet\n               # Tip : Don't forget to check that peek_node.right is not None or you will end up in an infinite loop by comparing None with None.\n               if   peek_node.right and peek_node.right != last_visited : \n                   current = peek_node.right # Descend into right subtree\n               #Decision B\n               # Either We already processed the right subtree or the right subtree is NULL. \n               # So, it is time to visit its parent\n               else :\n                   parent = stack.pop()\n                   res.append(parent.val)\n                   last_visited = parent\n                   # Continue ascent by keep current as NULL\n\n           return res\n</code></pre> <p>Iterative postorder traversal is the most complex of the three because it requires us to delay visiting a node until both of its children have been fully processed. This introduces a new challenge: when we return to a node from a subtree, we must know whether we came from the left (and must now go right) or came from the right (and can finally visit the node). The most common and efficient one-stack solution uses a last_visited_node pointer to check if the right subtree has already been processed.</p> <p>The iterative algorithm using a single stack operates as a two-state machine, but its second state is more complex, involving a critical decision-making step.</p> <p>State 1: Descent Phase (current != null)</p> <ul> <li>Action: Push the current node onto the stack and advance the pointer to its left child (current = current.left).</li> <li>Purpose: To traverse and stack the leftmost spine of any given subtree. This corresponds to exploring the Left component of the LRN pattern. The node is pushed to suspend its execution.</li> </ul> <p>State 2: Ascent &amp; Decision Phase (current == null and stack is not empty)</p> <ul> <li>Action: Peek at the node on top of the stack (peek_node). Do not pop it yet.</li> <li>Purpose: The algorithm must now decide its next move. We are at peek_node because its left subtree is fully processed. The question is: has its right subtree also been processed?</li> <li>Decision Logic: If peek_node has a right child AND that child is NOT the last_visited_node: This means the Right subtree must be explored. The algorithm transitions back to the Descent Phase by setting current = peek_node.right. Else (if peek_node has no right child OR its right child was the last_visited_node): This confirms that both the Left and Right subtrees are fully processed. The algorithm can now perform the visit.</li> <li>Action: Pop the node from the stack.</li> <li>Action: Visit the node by appending its value to the result list. This is the Node component of LRN.</li> <li>Action: Update last_visited_node to this newly visited node.</li> <li>Action: Keep current = None. This is crucial. It ensures the while loop continues in the Ascent &amp; Decision Phase, processing the next node on the stack.</li> </ul> <p>Invariant: A node is only visited (popped and its value recorded) after the algorithm confirms that both its left child (by completing the initial descent) and its right child (by checking last_visited_node) have been fully processed, strictly enforcing the Left-Right-Node sequence.</p> Iterative solution using modified preorder traversal <pre><code># Definition for a binary tree node.\n# class TreeNode:\n#     def __init__(self, val=0, left=None, right=None):\n#         self.val = val\n#         self.left = left\n#         self.right = right\nclass Solution:\n    def postorderTraversal(self, root: Optional[TreeNode]) -&gt; List[int]:\n        stack = []\n        res = []\n        current = root\n\n        while current or stack :\n            #Visit and descent\n            #Vist current node and descend into the right subtree\n            while current : \n                #visit \n                res.append(current.val)\n                #store execution state on stack for backtracking to left subtree later\n                stack.append(current)\n                #descend into the right subtree\n                current = current.right\n\n            #ascent\n            #backtrack \n            parent = stack.pop()\n            #and descend to the left subtree\n            current = parent.left\n\n        res.reverse()\n        return res\n</code></pre> <p>You could avoid all this headache by exploiting the relationship between Preorder and Postorder traversals. </p> <ul> <li>Preorder : Node - Left - Right</li> <li>Postorder : Left - Right - Node</li> </ul> <p>Notice that if we reverse the preorder sequence of actions we get Right - Left - Node. But what if we did the preorder traversal like this : Node - Right - Left?</p> <p>Now, the reversal of this sequence is Left - Right - Node which is exactly the same as postorder traversal.</p> <p>Is is as simple as that! You can do all three iterative traversals using the simpler state machine approach.</p>","tags":["Data_Structures","Algorithms"]},{"location":"blog/depth_first_binary_tree_traversals/#a-note-on-the-role-of-the-stack","title":"A Note on the role of the stack","text":"<p>In all three iterative traversals, the stack's fundamental role is to simulate the function call stack of recursion by saving the execution context of a node so its processing can be suspended and resumed later. This is done by backtracking which corresponds to popping a node from the stack to return to a previously suspended execution context.</p> <p>The key difference lies in what specific part of the execution context is being saved:</p> <ul> <li> <p>Inorder (LNR): When a node is pushed, you are saving the context to first visit this node, then explore its right subtree. This entire block of work is deferred until after its left subtree is fully processed.</p> </li> <li> <p>Preorder (NLR): Since you visit the node before pushing, you are only saving the context to explore its right subtree. The \"visit\" part of its context is already complete.</p> </li> <li> <p>Postorder (LRN): When a node is pushed, you are saving the most complex context: the obligation to first explore its left and right subtrees, and then finally visit the node itself. Its execution is suspended until both children's subtrees are fully processed.</p> </li> </ul>","tags":["Data_Structures","Algorithms"]},{"location":"blog/What_is_a_tree/","title":"What exactly is a Tree","text":"<p>For quite some time, there used to be great confusion in my mind about the exact nature of a tree. </p> <p>Some sources defined trees as undirected graphs and others said they were directed graphs. Some said that trees must have a root and others said, not really. It turns out that there are in fact two different species of 'trees'.</p> <p>Generally, when we talk about trees we are talking about rooted trees and to be even more specific, rooted out-trees. These are linked recursive data structures in which there is a strict parent-child relationship between nodes. The root node in a rooted tree does not have a parent. All other nodes do. There are no cycles and edges are directed from parent down to its children.</p> <p>A rooted out-tree in terms of graph theory is a digraph ( directed graph) such that :</p> <ul> <li>One of the vertices is a root node with no in edges.</li> <li>All other vertices have only one in-edge</li> <li>There is a path from root to all other vertices. </li> </ul> <p></p> <p>A (rooted) binary tree is one in which a node can have at most two children. If there is no such restriction, then we have a (rooted) multi-way or N-ary tree</p> <p>Technically, trees can also be undirected and thereby unrooted. These are defined as connected (there is a path between any two nodes), acyclic (a path cannot visit any  node twice) and undirected (an edge is two way connected) graphs. In other words, they are undirected graphs in which for any two vertices x and y, there is a unique simple path from x to y. A simple path is one in which vertices are not repeated. It is better to think of these as specific types of graphs and not confuse them with the usual rooted trees. They are also called free trees by the way.</p>","tags":["Data_Structures"]},{"location":"blog/What_is_a_tree/#references","title":"References:","text":"<ol> <li> <p>Roooted Trees</p> </li> <li> <p>Unrooted Trees</p> </li> <li> <p>Rooted vs Unrooted</p> </li> <li> <p>Paths, Walks and Trails</p> </li> <li> <p>Some theorems on Trees</p> </li> <li> <p>Unique Simple Path Theorem</p> </li> <li> <p>In-Tree vs Out-Tree</p> </li> </ol>","tags":["Data_Structures"]},{"location":"blog/Anatomy%20of%20an%20AWS%20Policy/","title":"Anatomy of an AWS Policy","text":"","tags":["AWS","IAM"]},{"location":"blog/Anatomy%20of%20an%20AWS%20Policy/#anatomy-of-an-aws-policy","title":"Anatomy of an AWS Policy","text":"<p>AWS Policies are json documents which declare the access permissions for certain services or resources at a granular level. They can be attached to roles, users or resources. Let us understand how this works by looking at an example policy. </p>","tags":["AWS","IAM"]},{"location":"blog/Anatomy%20of%20an%20AWS%20Policy/#the-philosophy-of-aws-security","title":"The philosophy of AWS Security","text":"<p>AWS IAM operates on the principle of the shared responsibility model. This means AWS provides the MEANS of implementing security but YOU are responsible for using them effectively. This means having to manage lots of policies.</p> <p>Another important principle is the principle of LEAST PRIVILEGE. In case of a hypthetical security breach, the BLAST RADIUS should be as small as possible.</p> <p>Blast Radius for the following policy is a single Lambda Function being invoked: </p> <pre><code>{\n          \"Version\": \"2012-10-17\",\n          \"Statement\": [\n            {\n              \"Sid\": \"VisualEditor1\",\n              \"Effect\": \"Allow\",\n              \"Action\": \"lambda:InvokeFunction\",\n              \"Resource\": [\n                \"arn:aws:lambda:&lt;your_account_id&gt;:&lt;us-east-1&gt;:function:MyFunction\"\n              ]\n            }\n          ]\n}\n</code></pre> <p>Now , if you want to use a generic role to invoke ANY lambda function in the acccount, you would use a wild card in the Resource : \"arn:aws:lambda:::function:*\" .</p> <p>Now the blast radius is invocation of any function.</p> <p>And if you want to have a generic role which can pretty much do anything with lambda functions like create, modify, delete and invoke lambdas, you would use a wildcard to define the Action as \"lambda:*\". So, the blast radius just increased to include anything which can be done to or with lambda functions. Somebody could just list and delete all your lambda functions.</p>","tags":["AWS","IAM"]},{"location":"blog/Anatomy%20of%20an%20AWS%20Policy/#when-does-iam-apply","title":"When does IAM apply?","text":"<p>Whenever you request anything through an AWS API its going to go through IAM. But but, there is a case when it does not apply : When you have an HTTP server running on an AWS EC2 instance and http requests are directly made to the elastic ip address, there is no AWS service in between, so no IAM policy evaluation is done in this case.</p> <pre><code>{\n          \"Version\": \"2012-10-17\",\n          \"Statement\": [\n            {\n              \"Sid\": \"VisualEditor1\",\n              \"Effect\": \"Allow\",\n              \"Action\": \"lambda:InvokeFunction\",\n              \"Resource\": [\n                \"arn:aws:lambda:&lt;your_account_id&gt;:&lt;us-east-1&gt;:function:MyFunction\"\n              ]\n            }\n          ]\n}\n</code></pre>","tags":["AWS","IAM"]},{"location":"blog/Anatomy%20of%20an%20AWS%20Policy/#trust-policy","title":"TRUST POlICY","text":"<p>A trust policy is an IAM policy attached to a Role which specifies who (which Principal) can assume it. This is typically used to allow services to assume the role. A typical example is API Gateway assuming a role which Has permissions to invoke a Lambda function.</p> <p>https://aws.amazon.com/blogs/security/how-to-use-trust-policies-with-iam-roles/</p>","tags":["AWS","IAM"]},{"location":"blog/Anatomy%20of%20an%20AWS%20Policy/#invocation-role-vs-execution-role","title":"Invocation Role vs Execution Role","text":"<p>A question may arise: Does the invoking role/user need all the permisions of the execution role? Say, a user does not have direct permission to delete an item from an s3 bucket, but he does have access to invoke a lambda function which can perform this action through its execution role. Can the user invoke the lambda and delete files? The answer is yes. Once the Lambda is invoked it autmoatically assumes the execution roles and performs whatever actions it does with the execution role. </p> <p>\"The policies that are attached to the credentials that made the original call to AssumeRole are not evaluated by AWS when making the \"allow\" or \"deny\" authorization decision. The user temporarily gives up its original permissions in favor of the permissions assigned by the assumed role. In the case of the AssumeRoleWithSAML and AssumeRoleWithWebIdentity API operations, there are no policies to evaluate because the caller of the API is not an AWS identity.\" [Ref 5]</p>","tags":["AWS","IAM"]},{"location":"blog/Anatomy%20of%20an%20AWS%20Policy/#resource-policies","title":"Resource Policies","text":"","tags":["AWS","IAM"]},{"location":"blog/Anatomy%20of%20an%20AWS%20Policy/#a-special-note-on-s3-policies","title":"A special Note on S3 Policies","text":"<p>When it comes to S3 buckets there two types of ARNs 1. ARN to match the bucket : arn:aws:s3:::my-bucket 2. ARN to match objects: arn:aws:s3:::my-bucket/ , arn:aws:s3:::my-bucket/somepath/ ,arn:aws:s3:::my-bucket/somepath/*, arn:aws:s3:::my-bucket/somepath/somekey.txt</p> <p>There are also two types of actions : Bucket level and Object level. </p> <pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\"s3:GetObject\", \"s3:ListBucket\"],\n            \"Resource\": [\n                \"arn:aws:s3:::mybucket\",\n                \"arn:aws:s3:::mybucket/resports/diabetes/*\"\n            ]\n        }\n    ]\n}\n</code></pre> <p>In the policy above, s3:ListBucket applies to the Bucket ARN and s3:GetObject applies to the arn matching objects with this pattern mybucket/resports/diabetes/*. A clearer way to write the same policy is using two separate statemnets, one for bucket level actions and another for Object level actions :</p> <pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\"s3:ListBucket\"],\n            \"Resource\": \"arn:aws:s3:::mybucket\"\n        }.\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": \"s3:GetObject\",\n            \"Resource\": \"arn:aws:s3:::mybucket/reports/diabetes/*\"\n        }\n    ]\n}\n</code></pre> <p>The most common example of Bucket Level action is ListBucket. PutObject, GetObject are examples of  Object Level actions.</p> <p>A common mistake when writing bucket policies is not including the bucket arn as a Resource in the policy. Then you will be left wondowering why I cannot list operations fail even though I have s3:Listbucket action in my policy.</p>","tags":["AWS","IAM"]},{"location":"blog/Anatomy%20of%20an%20AWS%20Policy/#cross-account-resource-access","title":"Cross account resource access","text":"<p>Consider this quote from S3 ducumentation : </p> <p>\"If the requester is an IAM principal, Amazon S3 must determine if the parent AWS account to which the principal belongs has granted the principal necessary permission to perform the operation. In addition, if the request is for a bucket operation, such as a request to list the bucket content, Amazon S3 must verify that the bucket owner has granted permission for the requester to perform the operation. To perform a specific operation on a resource, an IAM principal needs permission from both the parent AWS account to which it belongs and the AWS account that owns the resource\"  [ Ref 7 ]</p> <p>This applies in general to any resource. What this means in practice is :</p> <ol> <li> <p>If the principal and the resource belong to the same account: Either the resource policy OR the policy attached to the principal (i.e. user/role) should explicitly Allow the action on the target resource.</p> </li> <li> <p>If they belong to different accounts: Both, the resource policy AND the policy attached to the principal (i.e. user/role) MUST explicitly Allow the action on the target resource.</p> </li> </ol>","tags":["AWS","IAM"]},{"location":"blog/Anatomy%20of%20an%20AWS%20Policy/#user-groups","title":"User Groups","text":"<p>A user group is a collection of IAM users. User groups are used to specify permissions to a collection of users. Groups simplify permission management by letting us grant, change, and remove permissions for multiple users at once.</p>","tags":["AWS","IAM"]},{"location":"blog/Anatomy%20of%20an%20AWS%20Policy/#references","title":"References","text":"<ol> <li> <p>https://docs.aws.amazon.com/pdfs/whitepapers/latest/aws-fault-isolation-boundaries/aws-fault-isolation-boundaries.pdf</p> </li> <li> <p>https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_aws-services-that-work-with-iam.html</p> </li> <li> <p>https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_condition-keys.html</p> </li> <li> <p>https://docs.aws.amazon.com/awscloudtrail/latest/userguide/view-cloudtrail-events-cli.html</p> </li> <li> <p>https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_temp_control-access_assumerole.html</p> </li> <li> <p>https://docs.aws.amazon.com/service-authorization/latest/reference/reference_policies_actions-resources-contextkeys.html</p> </li> <li> <p>https://docs.aws.amazon.com/IAM/latest/UserGuide/reference-arns.html#supported-arns</p> </li> <li> <p>https://docs.aws.amazon.com/AmazonS3/latest/userguide/how-s3-evaluates-access-control.html</p> </li> </ol>","tags":["AWS","IAM"]},{"location":"blog/test/","title":"Building Serverless ETL Pipelines on AWS","text":"","tags":["tag1","tag2"]},{"location":"blog/test/#building-serverless-etl-pipelines-on-aws","title":"Building Serverless ETL Pipelines on AWS","text":"<p>In this post, I'll share my experience building a serverless ETL pipeline to extract and map medical entities using various AWS services including Lambda, SNS, Textract, Translate, Comprehend Medical, Glue, and Athena.</p>","tags":["tag1","tag2"]},{"location":"blog/test/#the-challenge","title":"The Challenge","text":"<p>Working at Genentech, I faced the challenge of extracting medical entities from multilingual clinical notes that came in various formats including handwritten images, PDFs, and DOCX files. These entities needed to be mapped to standardized ICD-10 codes for analysis.</p>","tags":["tag1","tag2"]},{"location":"blog/test/#architecture-overview","title":"Architecture Overview","text":"<p>Our solution leveraged a serverless architecture with these key components:</p> <ol> <li>Document Ingestion: AWS S3 for storage with event triggers</li> <li>Text Extraction: AWS Textract for converting documents to machine-readable text</li> <li>Translation: AWS Translate for handling multilingual content</li> <li>Entity Recognition: AWS Comprehend Medical for identifying medical terms</li> <li>Code Mapping: Custom Lambda function to map to ICD-10 codes</li> <li>Data Storage: Processed results stored in S3 and cataloged in Glue</li> <li>Analysis: Athena for SQL-based querying of the processed data</li> </ol>","tags":["tag1","tag2"]},{"location":"blog/test/#implementation-details","title":"Implementation Details","text":"<p>[Content continues with technical implementation details]</p>","tags":["tag1","tag2"]},{"location":"blog/test/#benefits-and-results","title":"Benefits and Results","text":"<p>The serverless approach provided several advantages:</p> <ol> <li>Cost Efficiency: Pay-per-use model reduced operational costs by 40%</li> <li>Scalability: Automatic scaling handled varying document loads</li> <li>Maintenance: Reduced operational overhead compared to EC2-based solutions</li> <li>Accuracy: Achieved 92% accuracy in entity recognition and mapping</li> </ol>","tags":["tag1","tag2"]},{"location":"blog/test/#lessons-learned","title":"Lessons Learned","text":"<p>[Content continues with lessons learned and best practices]</p>","tags":["tag1","tag2"]},{"location":"blog/test/#conclusion","title":"Conclusion","text":"<p>Serverless ETL pipelines offer significant advantages for processing unstructured medical data at scale. By leveraging AWS's managed services, we created a solution that was both cost-effective and powerful.</p> <p>Feel free to reach out if you have questions about implementing similar solutions in your organization!</p>","tags":["tag1","tag2"]},{"location":"blog/Fun%20Facts%20about%20two%27s%20complement%20encoding/","title":"Fun Facts about two's complement encoding","text":"<p>While revisiting my old notes on Standard C , I came across this fun fact: In two\u2019s complement, values like 0xF, 0xFF, and 0xFFFF all represent -1. Let's see why this is and explore some other properties of 2's complement encoding. </p>","tags":["C","Basics"]},{"location":"blog/Fun%20Facts%20about%20two%27s%20complement%20encoding/#signed-vs-unsigned-encoding-for-integers","title":"Signed Vs Unsigned encoding for integers","text":"<p>Most computers use two\u2019s complement to represent negative values. In this system, the most significant bit (MSB) is treated as having a negative weight.</p> <p>Unsigned Example: 1111 \u2192 8 + 4 + 2 + 1 = 15</p> <p>Signed Example: 1111 \u2192 -8 + 4 + 2 + 1 = -1</p> <p>This means: - 0xF = -1 - 0xFF = -1 - 0xFFF = -1 - 0xFFFF = -1 \u2026and so on, for all bits set in any size.</p> <p>For a byte, numbers from 0x00 to 0x7F are positive and have the same encoding in both signed and unsigned (two\u2019s complement) form.</p>","tags":["C","Basics"]},{"location":"blog/Fun%20Facts%20about%20two%27s%20complement%20encoding/#range-for-n-bit-signed-integers","title":"Range for N-bit Signed Integers","text":"<p>Let us dig a little bit deeper. What could be the the range for an N-Bit Signed Integer.</p> <p>The Lowest value would be if the Most Significant Bit is set and no other bit is set.</p> <p>For a 4- bit Integer: </p> <pre><code>BIT representation        :   1     0     0      0\nPostition Value of bits   :  2^3   2^2   2^1    2^0 \nSign                      :  Negative since MSB is set \nDecimal Value             : -8                     \n</code></pre> <p>The lowest value will always be -(2^(N-1))</p> <p>Conversely, the highest positive value will be when the MSB is not set and all other bits are set. This is equal to 2^(N-1) -1.</p> <p>For a 4- bit Integer: </p> <pre><code>BIT representation        :   0     1     1      1\nPostition Value of bits   :  2^3   2^2   2^1    2^0 \nSign                      :  Positive since MSB is not set \nDecimal Value             :  7                     \n</code></pre> <p>Did you also notice that -(2^(N-1)) + (2^(N-1)-1) = -1?</p> <p>This is why 0xFFFFF... is always -1 in 2's complement.</p>","tags":["C","Basics"]},{"location":"blog/Fun%20Facts%20about%20two%27s%20complement%20encoding/#extension","title":"Extension","text":"<p>In fact, you can represent any N-bit signed integers in N+x bits bits simply by extending the Most Significant Bit x times.</p> <pre><code>    4-bit       8-bit       16-bit              Decimal\n    0111        00000111    0000000000000111    +7\n    1110        11111110    1111111111111110    -2\n</code></pre>","tags":["C","Basics"]},{"location":"blog/Fun%20Facts%20about%20two%27s%20complement%20encoding/#truncation","title":"Truncation","text":"<p>An N-bit signed integer can be truncated to N-x bits and hold the same value by removing the Highest Order x Bits excluding the  MSB as long as they match the MSB (Sign Bit).</p> <pre><code>11100  is -4\n1100   is -4\n100    is -4\n10     is NOT -4 . It is -2.   \n</code></pre>","tags":["C","Basics"]},{"location":"blog/Fun%20Facts%20about%20two%27s%20complement%20encoding/#getting-twos-complement-of-a-number","title":"Getting Two's Complement of a Number","text":"<p>For getting 2's complement of a number you  flip the bits and add 1.</p> <pre><code>4-bit   Decimal \n0011      3       \n1100     -4          \n1101     -3          #Two's complement of 0011\n</code></pre> <p>[ As two why this works see Ref 2 below ] </p>","tags":["C","Basics"]},{"location":"blog/Fun%20Facts%20about%20two%27s%20complement%20encoding/#overflow-detection","title":"Overflow Detection","text":"<p>In 2's complement overflow is not possible when adding numbers with opposite signs. When adding numbers of the same sign, if there is an overflow you will get the wrong sign bit. </p> <ul> <li> <p>If both numbers are positive and you get a negative result.</p> </li> <li> <p>If both numbers are negative and you get a positive result.</p> </li> </ul> <p>[Ref 1]</p>","tags":["C","Basics"]},{"location":"blog/Fun%20Facts%20about%20two%27s%20complement%20encoding/#why-2s-complement","title":"Why 2's complement?","text":"<p>It's convenient. The same Adders in Hardware can be used for doing addition and subtraction. All you have to do is flip bits and add 1 to get the negative representation of a positive number and vice-versa.</p> <p>It is also Bijective, which is to say that it can uniquely represent negative integers, positive integers and zero.</p>","tags":["C","Basics"]},{"location":"blog/Fun%20Facts%20about%20two%27s%20complement%20encoding/#additional-references","title":"Additional References","text":"<ol> <li>https://web.archive.org/web/20131031093811/http://www.cs.uwm.edu/~cs151/Bacon/Lecture/HTML/ch03s09.html</li> <li>https://www.cs.cornell.edu/~tomf/notes/cps104/twoscomp.html</li> <li>https://wiki.sei.cmu.edu/confluence/display/c/INT32-C.+Ensure+that+operations+on+signed+integers+do+not+result+in+overflow</li> </ol>","tags":["C","Basics"]},{"location":"resources/","title":"Resources","text":""},{"location":"resources/#resources","title":"Resources","text":"<p>A curated collection of learning materials, documentation, and references.</p>"},{"location":"resources/#learning-platforms-playgrounds","title":"Learning Platforms &amp; Playgrounds","text":""},{"location":"resources/#data-engineering","title":"Data Engineering","text":"<ul> <li>DataTalks Club</li> </ul>"},{"location":"resources/#containers-linux","title":"Containers &amp; Linux","text":"<ul> <li>Container &amp; Linux Learning Labs</li> </ul>"},{"location":"resources/#sql","title":"SQL","text":"<ul> <li>Mode SQL Tutorial</li> <li>SQL Bolt</li> </ul>"},{"location":"resources/#spark","title":"Spark","text":"<ul> <li>Spark Playground</li> </ul>"},{"location":"resources/#mlai","title":"ML/AI","text":"<ul> <li>AFAIK.io</li> </ul>"},{"location":"resources/#documentation-white-papers","title":"Documentation &amp; White Papers","text":""},{"location":"resources/#aws","title":"AWS","text":"<ul> <li>AWS Fault Isolation Boundaries</li> <li>Using Trust Policies with IAM Roles</li> <li>S3 Object Lifecycle Management</li> </ul>"},{"location":"resources/#azure","title":"Azure","text":"<ul> <li>Azure Architecture Center</li> <li>AWS IAM to Azure Concepts Mapping</li> <li>Azure for AWS Professionals - Security &amp; Identity</li> <li>AWS &amp; Azure AD Security Integration</li> <li>Big Data Architecture Style</li> </ul>"},{"location":"resources/#snowflake","title":"Snowflake","text":"<ul> <li>Snowflake Documentation</li> </ul>"},{"location":"resources/#trino","title":"Trino","text":"<ul> <li>Trino Documentation</li> </ul>"},{"location":"resources/#devops","title":"DevOps","text":"<ul> <li>Terraform Language Documentation</li> <li>Helm Documentation</li> <li>Terraform CLI Workspaces Guide</li> <li>Terraform Configuration Organization</li> </ul>"},{"location":"resources/#technical-blog-posts-articles","title":"Technical Blog Posts &amp; Articles","text":""},{"location":"resources/#sql_1","title":"SQL","text":"<ul> <li>Mastering EXPLAIN ANALYZE</li> <li>N+1 Query Problem</li> <li>GraphQL N+1 Problem</li> </ul>"},{"location":"resources/#dataframe-data-processing","title":"DataFrame &amp; Data Processing","text":"<ul> <li>SQL vs DataFrames Discussion</li> <li>10 Minutes to Dask</li> <li>TPC-H Benchmarks</li> <li>Dask Join Order Optimization</li> <li>Spark MapReduce Internals</li> <li>JDBC - a short guide</li> </ul>"},{"location":"resources/#cloud-computing","title":"Cloud Computing","text":"<ul> <li>AWS App Runner Discussion</li> <li>AWS App Runner Issues</li> <li>Cloud Platform Comparison for Startups</li> </ul>"},{"location":"resources/#aiml","title":"AI/ML","text":"<ul> <li>LLM Document Summarization</li> <li>Text Chunking Strategies</li> <li>Podcast Transcripts Summarization</li> </ul>"},{"location":"resources/#books-long-form-content","title":"Books &amp; Long-form Content","text":""},{"location":"resources/#data-engineering_1","title":"Data Engineering","text":"<ul> <li>How Query Engines Work</li> <li>Data Engineering Design Patterns</li> </ul>"},{"location":"resources/#cloud-computing_1","title":"Cloud Computing","text":"<ul> <li>AWS SAP Certification Notes</li> <li>Azure Data Lakehouse Toolkit</li> </ul>"},{"location":"resources/#version-control","title":"Version Control","text":"<ul> <li>Pro Git Book</li> </ul>"},{"location":"resources/#web-development","title":"Web Development","text":"<ul> <li>HTTP Crash Course</li> </ul>"},{"location":"resources/#networking","title":"Networking","text":"<ul> <li>Networking Crash Course</li> </ul>"},{"location":"resources/#computer-engineering","title":"Computer Engineering","text":"<p>What Every Computer Scientist should know about Floating Point Arithmetic</p>"},{"location":"resources/#technical-blogs","title":"Technical Blogs","text":""},{"location":"resources/#data-engineering_2","title":"Data Engineering","text":"<ul> <li>AS Rathore's Blog</li> </ul>"},{"location":"resources/#sql_2","title":"SQL","text":"<ul> <li>Erland Sommarskog's SQL Blog</li> </ul>"},{"location":"resources/#ai-engineering","title":"AI Engineering","text":"<ul> <li>Amit Chaudhary's Blog</li> <li>Coding The Smart Way</li> <li>Rik AI</li> </ul>"},{"location":"resources/#java","title":"Java","text":"<ul> <li>Marco Behler's Blog</li> </ul>"},{"location":"resources/#cheat-sheets","title":"Cheat Sheets","text":"<ul> <li>PySpark RDD Cheat Sheet</li> <li>Trino Cheat Sheet</li> </ul>"},{"location":"resources/Dimensional_Modeling_Cheatseet/","title":"Dimensional Data Modeling Cheatsheet","text":""},{"location":"resources/Dimensional_Modeling_Cheatseet/#dimensional-data-modeling-cheatsheet","title":"Dimensional Data Modeling Cheatsheet","text":"<p>AI Assisted (Claude Sonnet 4)</p>"},{"location":"resources/Dimensional_Modeling_Cheatseet/#core-concepts","title":"Core Concepts","text":""},{"location":"resources/Dimensional_Modeling_Cheatseet/#what-is-dimensional-modeling","title":"What is Dimensional Modeling?","text":"<p>A design technique for data warehouses that organizes data into facts (measurable events) and dimensions (descriptive context) to support fast analytical queries and business intelligence.</p>"},{"location":"resources/Dimensional_Modeling_Cheatseet/#key-benefits","title":"Key Benefits","text":"<ul> <li>Query Performance: Optimized for analytical workloads</li> <li>Business User Friendly: Intuitive structure matching business processes</li> <li>Scalable: Handles large data volumes efficiently</li> <li>Flexible: Supports drilling down and rolling up through hierarchies</li> </ul>"},{"location":"resources/Dimensional_Modeling_Cheatseet/#building-blocks","title":"Building Blocks","text":""},{"location":"resources/Dimensional_Modeling_Cheatseet/#fact-tables","title":"Fact Tables","text":"<p>Central tables containing quantitative, measurable data about business events.</p> <p>Characteristics: - Large number of rows (transactions, events) - Foreign keys to dimension tables - Additive measures (sales amount, quantity) - Sparse data (many null values acceptable)</p> <p>Types of Facts: - Transactional: Individual business events - Periodic Snapshot: Regular intervals (daily, monthly) - Accumulating Snapshot: Process lifecycle tracking</p>"},{"location":"resources/Dimensional_Modeling_Cheatseet/#dimension-tables","title":"Dimension Tables","text":"<p>Provide descriptive context for facts - the \"who, what, when, where, why\" of business events.</p> <p>Characteristics: - Relatively small number of rows - Wide tables with many descriptive attributes - Primary keys (often surrogate keys) - Denormalized for query performance</p> <p>Common Dimensions: - Customer: Demographics, preferences, segments - Product: Categories, brands, specifications - Time: Hierarchies, fiscal calendars, holidays - Geography: Regions, territories, locations</p>"},{"location":"resources/Dimensional_Modeling_Cheatseet/#schema-patterns","title":"Schema Patterns","text":""},{"location":"resources/Dimensional_Modeling_Cheatseet/#star-schema","title":"Star Schema","text":"<p>Structure: Central fact table directly connected to dimension tables </p><pre><code>    Customer\n        |\nProduct \u2014 Fact \u2014 Store\n        |\n      Date\n</code></pre><p></p> <p>Advantages: - Simple structure, easy to understand - Faster query performance (fewer joins) - Optimized for business intelligence tools</p> <p>Use When: - Query performance is priority - Business users need simple models - Storage redundancy is acceptable</p>"},{"location":"resources/Dimensional_Modeling_Cheatseet/#snowflake-schema","title":"Snowflake Schema","text":"<p>Structure: Normalized dimensions broken into multiple related tables </p><pre><code>    Customer\n        |\n     City \u2014 State \u2014 Country\n        |\nProduct \u2014 Fact \u2014 Store\n   |              |\nCategory       Region\n   |\n Brand\n</code></pre><p></p> <p>Advantages: - Reduced data redundancy - Better data integrity - Smaller storage footprint</p> <p>Use When: - Storage efficiency is critical - Large dimension tables need normalization - Data integrity is paramount</p>"},{"location":"resources/Dimensional_Modeling_Cheatseet/#galaxy-schema","title":"Galaxy Schema","text":"<p>Structure: Multiple fact tables sharing common dimensions </p><pre><code>Dimension1 \u2014 Fact1 \u2014 Dimension2\n    |           |\n    |         Fact2 \u2014 Dimension3\n    |           |\nDimension4 \u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\n</code></pre><p></p> <p>Use When: - Multiple business processes - Complex enterprise requirements - Shared dimensions across processes</p>"},{"location":"resources/Dimensional_Modeling_Cheatseet/#essential-design-principles","title":"Essential Design Principles","text":""},{"location":"resources/Dimensional_Modeling_Cheatseet/#grain-definition","title":"Grain Definition","text":"<p>Most Critical Decision: What does each row in the fact table represent?</p> <p>Examples: - One row per sales transaction - One row per daily inventory snapshot - One row per customer interaction</p> <p>Rules: - Must be consistent throughout fact table - Determines what questions can be answered - All measures must be at same grain level</p>"},{"location":"resources/Dimensional_Modeling_Cheatseet/#slowly-changing-dimensions-scd","title":"Slowly Changing Dimensions (SCD)","text":"<p>Handle changes to dimensional attributes over time.</p> <p>Type 1: Overwrite - Replace old value with new value - No history preserved - Use for: Corrections, low-importance changes     </p><pre><code>UPDATE Customer SET City = 'New York' WHERE CustomerID = 123\n</code></pre><p></p> <p>Type 2: Add New Record - Create new record with effective dates - Preserves complete history - Use for: Important attribute changes     </p><pre><code>-- Original record\nCustomerKey | CustomerID | Name | City | EffectiveDate | ExpirationDate\n501         | 123        | John | Boston | 2020-01-01  | 2023-12-31\n\n-- New record after move\n502         | 123        | John | NYC    | 2024-01-01  | 9999-12-31\n</code></pre><p></p> <p>Type 3: Add New Column - Keep both old and new values - Limited history (usually current + previous) - Use for: Predictable changes, limited tracking     </p><pre><code>CustomerKey | Name | CurrentCity | PreviousCity\n501         | John | NYC         | Boston\n</code></pre><p></p>"},{"location":"resources/Dimensional_Modeling_Cheatseet/#surrogate-keys","title":"Surrogate Keys","text":"<p>Artificial primary keys (usually integers) for dimension records.</p> <p>Benefits: - Stable references when business keys change - Better join performance - Handle multiple source systems - Support slowly changing dimensions</p> <p>Implementation: </p><pre><code>-- Instead of natural key\nProductKey | ProductCode | ProductName | Category\n201        | SKU-12345   | iPhone 15   | Electronics\n\n-- Fact table references surrogate key\nSalesKey | ProductKey | CustomerKey | SalesAmount\n1001     | 201        | 501         | 999.00\n</code></pre><p></p>"},{"location":"resources/Dimensional_Modeling_Cheatseet/#common-patterns","title":"Common Patterns","text":""},{"location":"resources/Dimensional_Modeling_Cheatseet/#conformed-dimensions","title":"Conformed Dimensions","text":"<p>Shared dimensions across multiple fact tables ensuring consistent analysis.</p> <p>Example: - Customer dimension used by Sales, Support, and Marketing facts - Date dimension shared across all business processes - Product dimension used by Sales, Inventory, and Returns</p> <p>Benefits: - Consistent metrics across business areas - Integrated reporting capabilities - Simplified ETL processes</p>"},{"location":"resources/Dimensional_Modeling_Cheatseet/#degenerate-dimensions","title":"Degenerate Dimensions","text":"<p>Dimensional data stored directly in fact table (no separate dimension table).</p> <p>Common Examples: - Order numbers, invoice numbers - Transaction IDs, ticket numbers - Serial numbers, batch codes</p> <p>When to Use: - Only identifier exists (no descriptive attributes) - High cardinality with little analytical value - Performance optimization for grouping</p> <pre><code>-- OrderNumber stored in fact table\nSalesID | CustomerKey | ProductKey | OrderNumber | SalesAmount\n1001    | 501         | 201        | ORD-12345   | 125.99\n1002    | 501         | 203        | ORD-12345   | 89.50\n</code></pre>"},{"location":"resources/Dimensional_Modeling_Cheatseet/#junk-dimensions","title":"Junk Dimensions","text":"<p>Consolidate low-cardinality flags and indicators.</p> <p>Example: </p><pre><code>-- Instead of separate dimensions for each flag\nJunkKey | PaymentType | ShippingType | GiftWrap | Priority\n901     | Credit      | Standard     | No       | Normal\n902     | Credit      | Express      | Yes      | High\n903     | Debit       | Standard     | No       | Normal\n</code></pre><p></p> <p>Benefits: - Reduces fact table width - Eliminates tiny dimension tables - Groups related operational flags</p>"},{"location":"resources/Dimensional_Modeling_Cheatseet/#role-playing-dimensions","title":"Role-Playing Dimensions","text":"<p>Same dimension used multiple times in single fact table.</p> <p>Example: </p><pre><code>-- Date dimension used for multiple date types\nSalesID | CustomerKey | OrderDateKey | ShipDateKey | DeliveryDateKey\n1001    | 501         | 20240115     | 20240116    | 20240118\n</code></pre><p></p> <p>Implementation: - Create views or aliases for each role - Use descriptive names in fact table - Maintain same dimension structure</p>"},{"location":"resources/Dimensional_Modeling_Cheatseet/#design-best-practices","title":"Design Best Practices","text":""},{"location":"resources/Dimensional_Modeling_Cheatseet/#fact-table-design","title":"Fact Table Design","text":"<ol> <li>Choose Atomic Grain: Store data at lowest level of detail</li> <li>Additive Measures: Prefer measures that can be summed</li> <li>Consistent Grain: All measures at same level of detail</li> <li>Minimize NULLs: Use dimension attributes instead of fact NULLs</li> </ol>"},{"location":"resources/Dimensional_Modeling_Cheatseet/#dimension-design","title":"Dimension Design","text":"<ol> <li>Descriptive Attributes: Include all relevant business context</li> <li>User-Friendly Names: Use business terminology</li> <li>Hierarchies: Support drill-down analysis paths</li> <li>Surrogate Keys: Always use for dimension primary keys</li> </ol>"},{"location":"resources/Dimensional_Modeling_Cheatseet/#performance-optimization","title":"Performance Optimization","text":"<ol> <li> <p>Indexing Strategy:</p> <ul> <li>Clustered indexes on fact table date columns</li> <li>Foreign key indexes on fact tables</li> <li>Covering indexes for common query patterns</li> </ul> </li> <li> <p>Partitioning:</p> <ul> <li>Partition large fact tables by date</li> <li>Consider customer-based partitioning for very large customers</li> <li>Implement partition elimination in queries</li> </ul> </li> <li> <p>Aggregation:</p> <ul> <li>Pre-aggregate common summary levels</li> <li>Create OLAP cubes for frequent analysis patterns</li> <li>Maintain incremental refresh processes</li> </ul> </li> </ol>"},{"location":"resources/Dimensional_Modeling_Cheatseet/#implementation-checklist","title":"Implementation Checklist","text":""},{"location":"resources/Dimensional_Modeling_Cheatseet/#planning-phase","title":"Planning Phase","text":"<ul> <li> Define business requirements and KPIs</li> <li> Identify source systems and data quality</li> <li> Determine fact table grain and measures</li> <li> Design dimension hierarchies and attributes</li> <li> Plan for slowly changing dimensions</li> </ul>"},{"location":"resources/Dimensional_Modeling_Cheatseet/#development-phase","title":"Development Phase","text":"<ul> <li> Create staging area for data cleansing</li> <li> Implement surrogate key generation</li> <li> Build ETL processes with error handling</li> <li> Create data quality validation rules</li> <li> Implement SCD processing logic</li> </ul>"},{"location":"resources/Dimensional_Modeling_Cheatseet/#testing-phase","title":"Testing Phase","text":"<ul> <li> Validate business rules and calculations</li> <li> Test slowly changing dimension scenarios</li> <li> Verify referential integrity</li> <li> Performance test with realistic data volumes</li> <li> User acceptance testing with business users</li> </ul>"},{"location":"resources/Dimensional_Modeling_Cheatseet/#deployment-phase","title":"Deployment Phase","text":"<ul> <li> Set up production environments</li> <li> Implement monitoring and alerting</li> <li> Create user documentation and training</li> <li> Establish maintenance procedures</li> <li> Plan for ongoing support and enhancements</li> </ul>"},{"location":"resources/Dimensional_Modeling_Cheatseet/#common-pitfalls-to-avoid","title":"Common Pitfalls to Avoid","text":""},{"location":"resources/Dimensional_Modeling_Cheatseet/#design-mistakes","title":"Design Mistakes","text":"<ul> <li>Inconsistent Grain: Mixing transaction and summary data in same fact</li> <li>Over-normalization: Creating snowflake when star would suffice</li> <li>Missing Business Keys: Not storing original system identifiers</li> <li>Inadequate SCD Planning: Not considering how dimensions change</li> </ul>"},{"location":"resources/Dimensional_Modeling_Cheatseet/#performance-issues","title":"Performance Issues","text":"<ul> <li>Too Many Dimensions: Overly wide fact tables</li> <li>Large Dimension Tables: Not using mini-dimensions for large cardinality</li> <li>Poor Partitioning: Not aligning with query patterns</li> <li>Missing Indexes: Inadequate indexing strategy</li> </ul>"},{"location":"resources/Dimensional_Modeling_Cheatseet/#data-quality-problems","title":"Data Quality Problems","text":"<ul> <li>Duplicate Records: Lack of proper matching and deduplication</li> <li>Orphaned Facts: Facts without corresponding dimensions</li> <li>Inconsistent Formatting: Different formats across source systems</li> <li>Late Arriving Data: Not handling out-of-sequence data loads</li> </ul>"},{"location":"resources/Dimensional_Modeling_Cheatseet/#quick-reference","title":"Quick Reference","text":""},{"location":"resources/Dimensional_Modeling_Cheatseet/#when-to-use-star-vs-snowflake","title":"When to Use Star vs Snowflake","text":"<p>Star Schema: - Performance is critical - Business users directly query - Storage cost is not major concern - Simpler maintenance requirements</p> <p>Snowflake Schema: - Storage efficiency important - Large dimension tables - Strong normalization requirements - Complex data relationships</p>"},{"location":"resources/Dimensional_Modeling_Cheatseet/#scd-type-selection","title":"SCD Type Selection","text":"<p>Type 1: Corrections, unimportant changes Type 2: Critical business changes requiring history Type 3: Predictable changes with limited history needs</p>"},{"location":"resources/Dimensional_Modeling_Cheatseet/#grain-selection-guidelines","title":"Grain Selection Guidelines","text":"<ul> <li>Start with most atomic level possible</li> <li>Consider storage and performance implications</li> <li>Ensure all business questions can be answered</li> <li>Plan for future analytical requirements</li> </ul>"},{"location":"resources/Dimensional_Modeling_Cheatseet/#e-commerce-star-schema-example","title":"E-commerce Star Schema Example","text":""},{"location":"resources/Dimensional_Modeling_Cheatseet/#business-scenario","title":"Business Scenario","text":"<p>Imagine you're working at an online retailer that sells consumer electronics. The business stakeholders come to you with a requirement: \"We need to analyze our sales performance to understand which products are selling well, who our best customers are, and how our different stores are performing.\"</p> <p>This is a classic dimensional modeling challenge. Let's walk through the thought process of designing a data warehouse to solve this problem.</p>"},{"location":"resources/Dimensional_Modeling_Cheatseet/#understanding-the-business-questions","title":"Understanding the Business Questions","text":"<p>Before we start designing tables, we need to understand what questions the business wants to answer. The stakeholders tell us they want to know:</p> <ul> <li>Which products are our top sellers by month?</li> <li>Who are our highest-value customers by region?</li> <li>How do weekend sales compare to weekday sales?</li> <li>Which stores generate the most revenue?</li> <li>What's the average order value by customer segment?</li> </ul> <p>These questions give us clues about what our dimensional model should look like. We can see that we need to track sales events (facts) and analyze them by product, customer, time, and store (dimensions).</p>"},{"location":"resources/Dimensional_Modeling_Cheatseet/#choosing-the-grain","title":"Choosing the Grain","text":"<p>The most critical decision in dimensional modeling is selecting the grain - what does each row in our fact table represent? We have several options:</p> <p>We could track at the order level (one row per order), but this wouldn't let us analyze individual products. We could track at the daily summary level (one row per product per day), but this might lose important detail.</p> <p>For our e-commerce scenario, we choose line item level - one row per product in each order. This atomic grain gives us maximum flexibility. We can always aggregate up to order level or daily level, but we can't drill down if we start with summarized data.</p>"},{"location":"resources/Dimensional_Modeling_Cheatseet/#designing-our-star-schema","title":"Designing Our Star Schema","text":"<pre><code>erDiagram\n    FACT_SALES {\n        bigint sales_key\n        int customer_key\n        int product_key\n        int date_key\n        int store_key\n        varchar order_number\n        int quantity\n        decimal unit_price\n        decimal discount_amount\n        decimal total_amount\n    }\n\n    DIM_CUSTOMER {\n        int customer_key\n        varchar customer_id\n        varchar customer_name\n        varchar email\n        varchar city\n        varchar state\n        varchar country\n        varchar customer_segment\n        date registration_date\n    }\n\n    DIM_PRODUCT {\n        int product_key\n        varchar product_id\n        varchar product_name\n        varchar category\n        varchar brand\n        decimal cost\n        decimal list_price\n        varchar status\n    }\n\n    DIM_DATE {\n        int date_key\n        date full_date\n        int year\n        int quarter\n        int month\n        varchar month_name\n        int day\n        varchar day_name\n        char is_weekend\n        char is_holiday\n    }\n\n    DIM_STORE {\n        int store_key\n        varchar store_id\n        varchar store_name\n        varchar store_type\n        varchar city\n        varchar state\n        varchar region\n        varchar manager\n    }\n\n    DIM_CUSTOMER ||--o{ FACT_SALES : \"purchases\"\n    DIM_PRODUCT ||--o{ FACT_SALES : \"sold\"\n    DIM_DATE ||--o{ FACT_SALES : \"on_date\"\n    DIM_STORE ||--o{ FACT_SALES : \"sold_at\"</code></pre>"},{"location":"resources/Dimensional_Modeling_Cheatseet/#fact-table-design-reasoning","title":"Fact Table Design Reasoning","text":"<p>Our fact table contains the measurable events - the actual sales transactions. Let's think through each component:</p> <p>Surrogate Key: <code>sales_key</code> is an artificial primary key. We don't use the natural order number because multiple line items can exist per order.</p> <p>Foreign Keys: Each foreign key points to a dimension table. Notice how <code>customer_key</code>, <code>product_key</code>, <code>date_key</code>, and <code>store_key</code> connect our fact to the descriptive information.</p> <p>Degenerate Dimension: <code>order_number</code> stays in the fact table because while it's dimensional (we want to group by order), it has no descriptive attributes that warrant a separate table.</p> <p>Measures: These are the numbers we want to analyze: - <code>quantity</code>: How many items were sold - <code>unit_price</code>: Price per item (may differ from list price due to promotions) - <code>discount_amount</code>: Discount applied to this line item - <code>total_amount</code>: Final amount for this line item</p> <p>Notice that all measures are at the same grain (per line item) and are mostly additive - we can sum them across different dimensions to get meaningful totals.</p>"},{"location":"resources/Dimensional_Modeling_Cheatseet/#dimension-design-reasoning","title":"Dimension Design Reasoning","text":""},{"location":"resources/Dimensional_Modeling_Cheatseet/#customer-dimension-the-who","title":"Customer Dimension - The \"Who\"","text":"<p>We chose SCD Type 1 (overwrite changes) for simplicity. In a real scenario, you might want SCD Type 2 if tracking customer moves is important for regional analysis.</p> <p>The dimension includes geographic information (city, state, country) because the business wants regional analysis. Customer segmentation helps with targeted analysis.</p>"},{"location":"resources/Dimensional_Modeling_Cheatseet/#product-dimension-the-what","title":"Product Dimension - The \"What\"","text":"<p>Products are organized in a simple hierarchy: Category \u2192 Brand \u2192 Product. This denormalized approach (keeping all levels in one table) follows star schema principles - it's faster to query than a normalized snowflake approach.</p> <p>We include both cost and list price because: - Cost enables profit analysis - List price vs. unit price in facts shows promotional impact</p>"},{"location":"resources/Dimensional_Modeling_Cheatseet/#date-dimension-the-when","title":"Date Dimension - The \"When\"","text":"<p>The date dimension is crucial for time-based analysis. We include: - Calendar attributes (year, quarter, month) - Day-of-week information for weekday vs. weekend analysis - Holiday flags for seasonal analysis</p> <p>Each date gets its own row (one row per day), and we'll have thousands of rows spanning several years.</p>"},{"location":"resources/Dimensional_Modeling_Cheatseet/#store-dimension-the-where","title":"Store Dimension - The \"Where\"","text":"<p>This covers both physical stores and online channels. The <code>store_type</code> field distinguishes between \"Physical\", \"Online\", and \"Mobile\" channels.</p> <p>Geographic hierarchy (city, state, region) enables territorial analysis for operations and sales management.</p>"},{"location":"resources/Dimensional_Modeling_Cheatseet/#how-the-model-answers-business-questions","title":"How the Model Answers Business Questions","text":"<p>Let's trace through how our design answers the original business questions:</p> <p>\"Which products are our top sellers by month?\" </p><pre><code>SELECT \n    d.year, d.month_name, p.product_name,\n    SUM(f.quantity) as units_sold,\n    SUM(f.total_amount) as revenue\nFROM fact_sales f\nJOIN dim_date d ON f.date_key = d.date_key\nJOIN dim_product p ON f.product_key = p.product_key\nWHERE d.year = 2024\nGROUP BY d.year, d.month_name, p.product_name\nORDER BY d.month, revenue DESC;\n</code></pre><p></p> <p>\"How do weekend sales compare to weekday sales?\" </p><pre><code>SELECT \n    CASE WHEN d.is_weekend = 'Y' THEN 'Weekend' ELSE 'Weekday' END as day_type,\n    COUNT(*) as transaction_count,\n    SUM(f.total_amount) as total_sales,\n    AVG(f.total_amount) as avg_transaction_value\nFROM fact_sales f\nJOIN dim_date d ON f.date_key = d.date_key\nGROUP BY CASE WHEN d.is_weekend = 'Y' THEN 'Weekend' ELSE 'Weekday' END;\n</code></pre><p></p> <p>Notice how the star schema makes these queries intuitive - we join the fact table to the dimensions we need and aggregate the measures.</p>"},{"location":"resources/Dimensional_Modeling_Cheatseet/#design-trade-offs-and-alternatives","title":"Design Trade-offs and Alternatives","text":"<p>Why Star Instead of Snowflake? We could have normalized the product dimension into separate Category and Brand tables. However, the star schema approach offers: - Simpler queries (fewer joins) - Better performance for analytical workloads - Easier understanding for business users - Most BI tools are optimized for star schemas</p> <p>Why This Grain? Line-item level gives us maximum analytical flexibility. The trade-off is larger fact table size, but storage is usually less expensive than the loss of analytical capability.</p> <p>SCD Type Decisions We chose Type 1 for simplicity, but in production you might want: - Type 2 for Customer if regional sales history matters when customers move - Type 2 for Product if price history analysis is important - Type 1 for Store (store characteristics rarely change significantly)</p>"},{"location":"resources/Dimensional_Modeling_Cheatseet/#extending-the-model","title":"Extending the Model","text":"<p>As business requirements evolve, this model can grow:</p> <p>Additional Dimensions: Promotion, Sales Rep, Payment Method Additional Facts: Returns, Inventory, Customer Service interactions Enhanced Attributes: Customer demographics, product specifications, store performance metrics</p> <p>The key is starting with a solid foundation that answers the core business questions, then evolving the model based on new requirements.</p>"},{"location":"resources/Dimensional_Modeling_Cheatseet/#sample-data-to-visualize-the-concept","title":"Sample Data to Visualize the Concept","text":"<pre><code>-- Sample fact data showing the grain\nsales_key | customer_key | product_key | date_key | store_key | order_number | quantity | unit_price | total_amount\n1001      | 501          | 201         | 20240315 | 101       | ORD-12345    | 2        | 99.99      | 199.98\n1002      | 501          | 203         | 20240315 | 101       | ORD-12345    | 1        | 49.99      | 49.99\n1003      | 502          | 201         | 20240316 | 102       | ORD-12346    | 1        | 99.99      | 99.99\n\n-- Customer dimension sample\ncustomer_key | customer_id | customer_name | city    | state | customer_segment\n501          | CUST-001    | John Smith    | Seattle | WA    | VIP\n502          | CUST-002    | Jane Doe      | Portland| OR    | Regular\n\n-- Product dimension sample  \nproduct_key | product_id | product_name | category    | brand  | list_price\n201         | SKU-001    | iPhone 15    | Electronics | Apple  | 999.99\n203         | SKU-003    | AirPods Pro  | Electronics | Apple  | 249.99\n</code></pre> <p>Notice how the same order (ORD-12345) appears in multiple fact records because John Smith bought two different products. This is the line-item grain in action.</p>"},{"location":"resources/Dimensional_Modeling_Cheatseet/#key-takeaways-for-interviews","title":"Key Takeaways for Interviews","text":"<p>When discussing this model in an interview context, emphasize:</p> <ol> <li>Grain Selection: Explain why line-item level was chosen and what alternatives existed</li> <li>Business Alignment: Show how the design directly supports stated business requirements  </li> <li>Query Performance: Demonstrate how star schema enables fast, intuitive queries</li> <li>Scalability: Discuss how the model can evolve with new requirements</li> <li>Trade-offs: Acknowledge the storage vs. flexibility trade-off and why it's worth it</li> </ol> <p>This example demonstrates solid dimensional modeling fundamentals while remaining simple enough to explain clearly in an interview setting.</p>"},{"location":"resources/Dimensional_Modeling_Cheatseet/#clinical-trial-galaxy-schema-example","title":"Clinical Trial Galaxy Schema Example","text":""},{"location":"resources/Dimensional_Modeling_Cheatseet/#business-scenario_1","title":"Business Scenario","text":"<p>You're working as a data architect for a pharmaceutical company conducting a Phase III clinical trial for a new diabetes medication. The study involves 500 patients across 25 clinical sites over 18 months. The regulatory team, biostatisticians, and clinical operations team have come to you with complex analytical requirements.</p> <p>This scenario differs fundamentally from typical business intelligence challenges because clinical trials operate under strict regulatory requirements (FDA, EMA), require sophisticated statistical analysis, and involve patient safety monitoring. Let's explore how dimensional modeling adapts to these unique challenges.</p>"},{"location":"resources/Dimensional_Modeling_Cheatseet/#understanding-clinical-trial-complexity","title":"Understanding Clinical Trial Complexity","text":"<p>Clinical trials generate multiple types of data that need to be analyzed together:</p> <p>Efficacy Data: How well does the drug work? Primary endpoint is reduction in HbA1c (blood sugar control). Secondary endpoints include weight loss, blood pressure improvement, and quality of life scores.</p> <p>Safety Data: What adverse events occur? Every headache, nausea, or serious event must be tracked, categorized, and analyzed for patterns. Regulatory agencies require detailed safety reporting.</p> <p>Operational Data: Are we following the protocol? Visit scheduling, protocol deviations, and site performance all impact trial success and regulatory approval.</p> <p>The key insight is that we have fundamentally different types of business events happening - patient measurements and adverse events - that need separate fact tables but must be analyzed together. This leads us to a galaxy schema design.</p>"},{"location":"resources/Dimensional_Modeling_Cheatseet/#designing-the-galaxy-schema","title":"Designing the Galaxy Schema","text":"<pre><code>erDiagram\n    FACT_PATIENT_MEASUREMENT {\n        bigint measurement_key\n        int patient_key\n        int visit_key\n        int measurement_type_key\n        int date_key\n        int site_key\n        varchar measurement_id\n        decimal measurement_value\n        varchar measurement_unit\n        decimal baseline_value\n        decimal change_from_baseline\n        char within_normal_range\n        char measurement_quality_flag\n    }\n\n    FACT_ADVERSE_EVENT {\n        bigint adverse_event_key\n        int patient_key\n        int visit_key\n        int event_type_key\n        int date_key\n        int severity_key\n        varchar event_id\n        varchar event_description\n        int duration_days\n        char related_to_study_drug\n        char serious_event_flag\n        char resolved_flag\n    }\n\n    DIM_PATIENT {\n        int patient_key\n        varchar patient_id\n        varchar site_patient_id\n        int age_at_enrollment\n        varchar gender\n        varchar race\n        varchar ethnicity\n        decimal baseline_hba1c\n        decimal baseline_weight\n        int diabetes_duration_years\n        varchar enrollment_status\n        date enrollment_date\n        date completion_date\n    }\n\n    DIM_VISIT {\n        int visit_key\n        varchar visit_code\n        varchar visit_name\n        varchar visit_type\n        int visit_number\n        int study_day\n        varchar visit_window\n        char baseline_visit_flag\n        char primary_endpoint_flag\n        varchar visit_status\n    }\n\n    DIM_MEASUREMENT_TYPE {\n        int measurement_type_key\n        varchar measurement_code\n        varchar measurement_name\n        varchar measurement_category\n        varchar body_system\n        decimal normal_range_min\n        decimal normal_range_max\n        varchar standard_unit\n        char primary_endpoint_flag\n        char safety_parameter_flag\n    }\n\n    DIM_SITE {\n        int site_key\n        varchar site_id\n        varchar site_name\n        varchar principal_investigator\n        varchar city\n        varchar state\n        varchar country\n        varchar site_type\n        date site_activation_date\n        char site_status\n    }\n\n    DIM_DATE {\n        int date_key\n        date full_date\n        int year\n        int quarter\n        int month\n        varchar month_name\n        int week\n        int day_of_week\n        varchar day_name\n        int study_day\n        varchar study_phase\n        char is_visit_window\n    }\n\n    DIM_EVENT_TYPE {\n        int event_type_key\n        varchar event_code\n        varchar event_term\n        varchar system_organ_class\n        varchar preferred_term\n        varchar event_category\n        char expected_event_flag\n        varchar regulatory_classification\n    }\n\n    DIM_SEVERITY {\n        int severity_key\n        varchar severity_code\n        varchar severity_name\n        varchar severity_description\n        int severity_rank\n        char hospitalization_required\n        char life_threatening_flag\n    }\n\n    DIM_PATIENT ||--o{ FACT_PATIENT_MEASUREMENT : \"measured_for\"\n    DIM_VISIT ||--o{ FACT_PATIENT_MEASUREMENT : \"during_visit\"\n    DIM_MEASUREMENT_TYPE ||--o{ FACT_PATIENT_MEASUREMENT : \"type_of\"\n    DIM_DATE ||--o{ FACT_PATIENT_MEASUREMENT : \"on_date\"\n    DIM_SITE ||--o{ FACT_PATIENT_MEASUREMENT : \"at_site\"\n\n    DIM_PATIENT ||--o{ FACT_ADVERSE_EVENT : \"experienced_by\"\n    DIM_VISIT ||--o{ FACT_ADVERSE_EVENT : \"during_visit\"\n    DIM_EVENT_TYPE ||--o{ FACT_ADVERSE_EVENT : \"type_of\"\n    DIM_DATE ||--o{ FACT_ADVERSE_EVENT : \"on_date\"\n    DIM_SEVERITY ||--o{ FACT_ADVERSE_EVENT : \"severity_level\"</code></pre>"},{"location":"resources/Dimensional_Modeling_Cheatseet/#why-galaxy-schema-instead-of-star","title":"Why Galaxy Schema Instead of Star?","text":"<p>The fundamental decision here is using multiple fact tables instead of trying to force everything into a single fact. Here's the reasoning:</p> <p>Different Grains: Patient measurements happen at scheduled visits with precise protocols. Adverse events can happen anytime and are reported whenever discovered. Trying to combine these into one fact table would create a confusing grain.</p> <p>Different Measures: Efficacy measurements are quantitative (HbA1c = 7.2%) while adverse events are qualitative (severity = \"Moderate\"). The measures don't make sense together.</p> <p>Different Analysis Patterns: Efficacy analysis focuses on statistical change over time. Safety analysis focuses on event rates and patterns. These require different analytical approaches.</p> <p>Regulatory Requirements: FDA submissions require separate efficacy and safety sections with different statistical methodologies. Our data model should align with regulatory reporting needs.</p>"},{"location":"resources/Dimensional_Modeling_Cheatseet/#fact-table-design-deep-dive","title":"Fact Table Design Deep Dive","text":""},{"location":"resources/Dimensional_Modeling_Cheatseet/#patient-measurement-fact-the-efficacy-story","title":"Patient Measurement Fact - The Efficacy Story","text":"<p>This fact table captures every measurement taken on every patient at every visit. The grain is: one row per measurement type per patient per visit.</p> <p>Pre-calculated Change from Baseline: Rather than calculating this at query time, we store <code>baseline_value</code> and <code>change_from_baseline</code>. This is crucial because: - Baseline calculations are complex (what if a patient missed their baseline visit?) - Statistical software expects pre-calculated changes - Performance is critical for large datasets with frequent analysis</p> <p>Data Quality Flags: <code>within_normal_range</code> and <code>measurement_quality_flag</code> capture data quality issues that are crucial for regulatory submissions. Outlier values need flagging and explanation.</p> <p>Why This Grain: We could aggregate to visit level, but measurement-level detail enables: - Analysis of specific biomarkers - Missing data pattern analysis - Quality control at the measurement level</p>"},{"location":"resources/Dimensional_Modeling_Cheatseet/#adverse-event-fact-the-safety-story","title":"Adverse Event Fact - The Safety Story","text":"<p>This fact table captures every adverse event reported during the study. The grain is: one row per adverse event occurrence.</p> <p>Relationship to Study Drug: <code>related_to_study_drug</code> is a critical regulatory field. Investigators must assess whether each event is related to the study medication.</p> <p>Duration Tracking: <code>duration_days</code> enables analysis of how long events last, which impacts quality of life and drug tolerability.</p> <p>Regulatory Classification: Different event types have different regulatory reporting requirements. Serious events must be reported to authorities within 24 hours.</p>"},{"location":"resources/Dimensional_Modeling_Cheatseet/#dimension-design-for-clinical-context","title":"Dimension Design for Clinical Context","text":""},{"location":"resources/Dimensional_Modeling_Cheatseet/#patient-dimension-the-study-population","title":"Patient Dimension - The Study Population","text":"<p>Unlike typical customer dimensions, patient characteristics are mostly fixed at enrollment. We use SCD Type 1 because: - Demographics don't change during the study - Medical history is captured at baseline - Any changes (like diabetes duration) are captured through measurements, not dimension updates</p> <p>Baseline Medical Information: <code>baseline_hba1c</code>, <code>baseline_weight</code>, and <code>diabetes_duration_years</code> enable subgroup analysis. Regulatory agencies want to know if the drug works differently in different patient populations.</p> <p>Enrollment Tracking: <code>enrollment_status</code> tracks patients through the study lifecycle (Screened, Enrolled, Completed, Withdrawn). This is crucial for statistical analysis and regulatory compliance.</p>"},{"location":"resources/Dimensional_Modeling_Cheatseet/#visit-dimension-the-protocol-framework","title":"Visit Dimension - The Protocol Framework","text":"<p>Clinical trials follow strict visit schedules defined in the protocol. This dimension captures that structure:</p> <p>Study Day: Calculated from enrollment date, enabling analysis aligned with the study timeline rather than calendar dates.</p> <p>Visit Windows: Protocols allow visits within acceptable windows (e.g., Week 12 visit can occur between Day 80-90). <code>visit_window</code> enables protocol compliance analysis.</p> <p>Primary Endpoint Visits: <code>primary_endpoint_flag</code> identifies visits where the main efficacy measurement occurs. These are crucial for the primary statistical analysis.</p>"},{"location":"resources/Dimensional_Modeling_Cheatseet/#measurement-type-dimension-clinical-standardization","title":"Measurement Type Dimension - Clinical Standardization","text":"<p>This dimension standardizes all the different measurements taken in the study:</p> <p>Clinical Hierarchy: <code>body_system</code> and <code>measurement_category</code> organize measurements (Endocrine \u2192 Glucose Control \u2192 HbA1c).</p> <p>Reference Ranges: <code>normal_range_min</code> and <code>normal_range_max</code> enable automated flagging of abnormal values for safety monitoring.</p> <p>Study Relevance: <code>primary_endpoint_flag</code> and <code>safety_parameter_flag</code> distinguish between different analysis categories for regulatory reporting.</p>"},{"location":"resources/Dimensional_Modeling_Cheatseet/#event-type-dimension-medical-coding","title":"Event Type Dimension - Medical Coding","text":"<p>Adverse events must be coded using standardized medical terminology (MedDRA - Medical Dictionary for Regulatory Activities):</p> <p>Hierarchy: <code>system_organ_class</code> \u2192 <code>preferred_term</code> \u2192 <code>event_term</code> follows MedDRA structure. For example: \"Gastrointestinal disorders\" \u2192 \"Nausea\" \u2192 \"Nausea after meals\".</p> <p>Regulatory Classification: <code>expected_event_flag</code> indicates whether the event is listed in the drug's safety profile. Unexpected events require special reporting.</p>"},{"location":"resources/Dimensional_Modeling_Cheatseet/#complex-analysis-examples","title":"Complex Analysis Examples","text":""},{"location":"resources/Dimensional_Modeling_Cheatseet/#primary-efficacy-analysis","title":"Primary Efficacy Analysis","text":"<pre><code>-- Mean change from baseline in HbA1c by visit\nSELECT \n    v.visit_name,\n    v.study_day,\n    COUNT(DISTINCT pm.patient_key) as patient_count,\n    AVG(pm.change_from_baseline) as mean_change,\n    STDDEV(pm.change_from_baseline) as std_deviation\nFROM fact_patient_measurement pm\nJOIN dim_measurement_type mt ON pm.measurement_type_key = mt.measurement_type_key\nJOIN dim_visit v ON pm.visit_key = v.visit_key\nWHERE mt.measurement_code = 'HBA1C'\n    AND v.primary_endpoint_flag = 'Y'\nGROUP BY v.visit_name, v.study_day, v.visit_number\nORDER BY v.visit_number;\n</code></pre> <p>This query answers the fundamental question: \"Is the drug working?\" Statistical significance testing would be applied to these results.</p>"},{"location":"resources/Dimensional_Modeling_Cheatseet/#safety-signal-detection","title":"Safety Signal Detection","text":"<pre><code>-- Adverse events by body system - looking for concerning patterns\nSELECT \n    et.system_organ_class,\n    COUNT(*) as total_events,\n    COUNT(DISTINCT ae.patient_key) as patients_affected,\n    COUNT(DISTINCT ae.patient_key) * 100.0 / \n        (SELECT COUNT(DISTINCT patient_key) FROM dim_patient WHERE enrollment_status = 'Enrolled') as incidence_rate,\n    SUM(CASE WHEN ae.related_to_study_drug = 'Y' THEN 1 ELSE 0 END) as drug_related_events\nFROM fact_adverse_event ae\nJOIN dim_event_type et ON ae.event_type_key = et.event_type_key\nGROUP BY et.system_organ_class\nORDER BY incidence_rate DESC;\n</code></pre> <p>This analysis helps identify safety</p>"},{"location":"resources/SQL_Basics_Cheatsheet/","title":"SQL Basics Cheatsheet","text":""},{"location":"resources/SQL_Basics_Cheatsheet/#sql-basics-cheatsheet","title":"SQL Basics Cheatsheet","text":"<ol> <li> <p>Order of execution of SQL: <code>FROM</code> -&gt; <code>ON</code> -&gt; <code>JOIN</code> -&gt; <code>WHERE</code> -&gt; <code>GROUP BY</code> -&gt; <code>CUBE</code>/<code>ROLLUP</code> -&gt; <code>HAVING</code> -&gt; <code>SELECT</code> -&gt; <code>DISTINCT</code> -&gt; <code>ORDER BY</code> -&gt; <code>TOP</code>/<code>LIMIT</code>.</p> </li> <li> <p>Difference between WHERE and HAVING: <code>WHERE</code> filters individual rows before grouping, while <code>HAVING</code> filters groups after grouping.</p> </li> <li> <p>Use of GROUP BY: <code>GROUP BY</code> groups rows that have the same values in specified columns into a summary row, often used with aggregate functions.</p> </li> <li> <p>Types of joins in SQL:</p> <ul> <li>INNER JOIN: Returns rows when there is a match in both tables.</li> <li>LEFT (OUTER) JOIN: Returns all rows from the left table, and matching rows from the right table. If no match, NULLs for right table columns.</li> <li>RIGHT (OUTER) JOIN: Returns all rows from the right table, and matching rows from the left table. If no match, NULLs for left table columns.</li> <li>FULL (OUTER) JOIN: Returns all rows when there is a match in one of the tables. If no match, NULLs for the non-matching side.</li> <li>CROSS JOIN: Returns the Cartesian product of the two tables (every row from the first table combined with every row from the second).</li> </ul> </li> <li> <p>Triggers in SQL: Special types of stored procedures that automatically execute (fire) when a specific event (INSERT, UPDATE, DELETE) occurs on a table or view.</p> </li> <li> <p>Stored procedure in SQL: A prepared SQL code block that can be saved, reused, and executed multiple times. They can accept parameters and return values.</p> </li> <li> <p>Types of window functions (rank, row_num, dense_rank, lead &amp; lag):</p> <ul> <li><code>ROW_NUMBER()</code>: Assigns a unique, sequential integer to each row within its partition, starting from 1.</li> <li><code>RANK()</code>: Assigns a rank to each row within its partition, with gaps in the ranking for ties.</li> <li><code>DENSE_RANK()</code>: Assigns a rank to each row within its partition, with no gaps in the ranking for ties.</li> <li><code>LEAD(column, offset, default)</code>: Accesses data from a subsequent row in the same result set without using a self-join.</li> <li><code>LAG(column, offset, default)</code>: Accesses data from a preceding row in the same result set without using a self-join.</li> </ul> </li> <li> <p>Difference between DELETE and TRUNCATE:</p> <ul> <li><code>DELETE</code>: A DML command, removes rows one by one, logs each row deletion, can be rolled back, and can have a <code>WHERE</code> clause.</li> <li><code>TRUNCATE</code>: A DDL command, deallocates the data pages, does not log individual row deletions (logs deallocation), cannot be rolled back, and cannot have a <code>WHERE</code> clause. It's much faster for large tables.</li> </ul> </li> <li> <p>Difference between DML, DDL and DCL:</p> <ul> <li>DML (Data Manipulation Language): Used for managing data within schema objects (e.g., <code>SELECT</code>, <code>INSERT</code>, <code>UPDATE</code>, <code>DELETE</code>).</li> <li>DDL (Data Definition Language): Used for defining and modifying the database schema (e.g., <code>CREATE</code>, <code>ALTER</code>, <code>DROP</code>, <code>TRUNCATE</code>).</li> <li>DCL (Data Control Language): Used for controlling access to data and the database (e.g., <code>GRANT</code>, <code>REVOKE</code>).</li> </ul> </li> <li> <p>Aggregate functions and when do we use them: Functions that perform a calculation on a set of values and return a single summary value. We use them to summarize data.</p> <ul> <li>Examples:<ul> <li><code>COUNT(*)</code>: Number of rows.</li> <li><code>SUM(column)</code>: Sum of values in a column.</li> <li><code>AVG(column)</code>: Average of values in a column.</li> <li><code>MAX(column)</code>: Maximum value in a column.</li> <li><code>MIN(column)</code>: Minimum value in a column.</li> </ul> </li> </ul> </li> <li> <p>Which is faster between CTE and Subquery? Generally, there isn't a significant performance difference. The SQL optimizer often treats them similarly. CTEs are often preferred for readability and reusability, especially for complex queries.</p> </li> <li> <p>What are constraints and types of Constraints? Rules enforced on data columns in a table to limit the type of data that can go into a table, ensuring data accuracy and integrity.</p> <ul> <li>Types:<ul> <li><code>NOT NULL</code>: Ensures a column cannot have a NULL value.</li> <li><code>UNIQUE</code>: Ensures all values in a column are different.</li> <li><code>PRIMARY KEY</code>: A combination of <code>NOT NULL</code> and <code>UNIQUE</code>, uniquely identifies each record.</li> <li><code>FOREIGN KEY</code>: Links two tables together, referencing the <code>PRIMARY KEY</code> of another table.</li> <li><code>CHECK</code>: Ensures all values in a column satisfy a specific condition.</li> <li><code>DEFAULT</code>: Sets a default value for a column when no value is specified.</li> </ul> </li> </ul> </li> <li> <p>Types of Keys?</p> <ul> <li>Primary Key: Uniquely identifies each record in a table.</li> <li>Candidate Key: A super key without any redundant attributes.</li> <li>Super Key: A set of attributes that can uniquely identify a tuple in a relation.</li> <li>Foreign Key: A field(s) in one table that refers to the primary key of another table.</li> <li>Alternate Key: Candidate keys that are not chosen as the primary key.</li> <li>Composite Key: A primary key consisting of two or more attributes.</li> </ul> </li> <li> <p>Different types of Operators?</p> <ul> <li>Arithmetic Operators: <code>+</code>, <code>-</code>, <code>*</code>, <code>/</code>, <code>%</code></li> <li>Comparison Operators: <code>=</code>, <code>!=</code> (or <code>&lt;&gt;</code>), <code>&gt;</code>, <code>&lt;</code>, <code>&gt;=</code>, <code>&lt;=</code>, <code>BETWEEN</code>, <code>LIKE</code>, <code>IN</code>, <code>IS NULL</code></li> <li>Logical Operators: <code>AND</code>, <code>OR</code>, <code>NOT</code></li> <li>Bitwise Operators: <code>&amp;</code>, <code>|</code>, <code>^</code>, <code>~</code> (less common in typical SQL usage)</li> <li>Set Operators: <code>UNION</code>, <code>UNION ALL</code>, <code>INTERSECT</code>, <code>EXCEPT</code> (or <code>MINUS</code>)</li> </ul> </li> <li> <p>Difference between GROUP BY and WHERE? <code>WHERE</code> filters individual rows before grouping, while <code>GROUP BY</code> groups rows based on specified columns. <code>WHERE</code> cannot use aggregate functions directly unless they are in a subquery or a CTE, whereas <code>GROUP BY</code> is used in conjunction with aggregate functions.</p> </li> <li> <p>What are Views? A virtual table based on the result-set of a SQL query. A view contains rows and columns, just like a real table, but it does not store data itself. The data is stored in the underlying tables.</p> </li> <li> <p>What are different types of constraints? (Already answered in question 12). <code>NOT NULL</code>, <code>UNIQUE</code>, <code>PRIMARY KEY</code>, <code>FOREIGN KEY</code>, <code>CHECK</code>, <code>DEFAULT</code>.</p> </li> <li> <p>Difference between VARCHAR and NVARCHAR?</p> <ul> <li><code>VARCHAR</code>: Stores variable-length non-Unicode string data. Uses 1 byte per character.</li> <li><code>NVARCHAR</code>: Stores variable-length Unicode string data. Uses 2 bytes per character, allowing it to store characters from different languages.</li> </ul> </li> <li> <p>Similar for CHAR and NCHAR?</p> <ul> <li><code>CHAR</code>: Stores fixed-length non-Unicode string data. Pads with spaces to the defined length.</li> <li><code>NCHAR</code>: Stores fixed-length Unicode string data. Pads with spaces to the defined length.</li> </ul> </li> <li> <p>What are index and their types? An index is a database object that improves the speed of data retrieval operations on a database table. It's like an index in a book.</p> <ul> <li>Types:<ul> <li>Clustered Index: Determines the physical order of data in the table. A table can have only one clustered index.</li> <li>Non-Clustered Index: Does not alter the physical order of the table. It stores the data separately from the data rows and contains pointers to the actual data. A table can have multiple non-clustered indexes.</li> </ul> </li> </ul> </li> <li> <p>What is an index? Explain its different types. (Already answered in question 20).</p> </li> <li> <p>List the different types of relationships in SQL.</p> <ul> <li>One-to-One (1:1): Each record in table A is linked to at most one record in table B, and vice versa.</li> <li>One-to-Many (1:M): One record in table A can be linked to many records in table B, but one record in table B is linked to only one record in table A. (Most common)</li> <li>Many-to-Many (M:N): Many records in table A can be linked to many records in table B, and vice versa. This typically requires an intermediary \"junction\" or \"linking\" table.</li> </ul> </li> <li> <p>Differentiate between UNION and UNION ALL.</p> <ul> <li><code>UNION</code>: Combines the result sets of two or more <code>SELECT</code> statements and removes duplicate rows.</li> <li><code>UNION ALL</code>: Combines the result sets of two or more <code>SELECT</code> statements and includes all duplicate rows.</li> </ul> </li> <li> <p>How many types of clauses in SQL? There's no fixed \"number of types\" of clauses, as they serve different purposes. Some common and distinct clauses include: <code>SELECT</code>, <code>FROM</code>, <code>WHERE</code>, <code>GROUP BY</code>, <code>HAVING</code>, <code>ORDER BY</code>, <code>LIMIT</code>/<code>TOP</code>, <code>JOIN</code> (and its variations), <code>ON</code>.</p> </li> <li> <p>Difference between Primary Key and Secondary Key?</p> <ul> <li>Primary Key: Uniquely identifies each record in a table. Must be unique and <code>NOT NULL</code>. There can be only one primary key per table.</li> <li>Secondary Key (or Alternate Key): A candidate key that is not chosen as the primary key. It can also uniquely identify records, but it's not the designated primary identifier.</li> </ul> </li> <li> <p>Find the second highest salary of an employee?</p> <pre><code>SELECT MAX(Salary)\nFROM Employees\nWHERE Salary &lt; (SELECT MAX(Salary) FROM Employees);\n</code></pre> <p>OR using <code>DENSE_RANK</code>:</p> <pre><code>SELECT Salary\nFROM (\n    SELECT Salary, DENSE_RANK() OVER (ORDER BY Salary DESC) as rnk\n    FROM Employees\n) AS RankedSalaries\nWHERE rnk = 2;\n</code></pre> </li> <li> <p>Write retention query in SQL? (This is a broad topic and depends on how \"retention\" is defined, e.g., monthly cohort retention). Here's a basic example for month-over-month retention for users who joined in a specific month:</p> <p>Assume <code>Users</code> table with <code>user_id</code> and <code>join_date</code>. Assume <code>Activities</code> table with <code>user_id</code> and <code>activity_date</code>.</p> <pre><code>WITH UserCohorts AS (\n    SELECT\n        user_id,\n        DATE_TRUNC('month', join_date) AS cohort_month\n    FROM Users\n),\nMonthlyActivity AS (\n    SELECT\n        user_id,\n        DATE_TRUNC('month', activity_date) AS activity_month\n    FROM Activities\n    GROUP BY user_id, DATE_TRUNC('month', activity_date)\n)\nSELECT\n    uc.cohort_month,\n    COUNT(DISTINCT uc.user_id) AS total_cohort_users,\n    DATE_TRUNC('month', ma.activity_month) AS retention_month,\n    COUNT(DISTINCT ma.user_id) AS retained_users,\n    (COUNT(DISTINCT ma.user_id) * 100.0) / COUNT(DISTINCT uc.user_id) AS retention_rate\nFROM UserCohorts uc\nINNER JOIN MonthlyActivity ma ON uc.user_id = ma.user_id\nWHERE ma.activity_month &gt;= uc.cohort_month -- Ensure activity is on or after join month\nGROUP BY uc.cohort_month, DATE_TRUNC('month', ma.activity_month)\nORDER BY uc.cohort_month, retention_month;\n</code></pre> <p>Note: <code>DATE_TRUNC</code> syntax might vary based on SQL dialect (e.g., <code>DATE_TRUNC('month', date_column)</code> in PostgreSQL/Redshift, <code>FORMAT(date_column, 'yyyy-MM-01')</code> in SQL Server, <code>STRFTIME('%Y-%m-01', date_column)</code> in SQLite).</p> </li> <li> <p>Write year-on-year growth in SQL?     Assuming a table <code>Sales</code> with <code>sale_date</code> and <code>amount</code>.</p> <pre><code>WITH YearlySales AS (\n    SELECT\n        EXTRACT(YEAR FROM sale_date) AS sales_year,\n        SUM(amount) AS total_sales\n    FROM Sales\n    GROUP BY EXTRACT(YEAR FROM sale_date)\n)\nSELECT\n    ys.sales_year,\n    ys.total_sales,\n    LAG(ys.total_sales, 1, 0) OVER (ORDER BY ys.sales_year) AS previous_year_sales,\n    (ys.total_sales - LAG(ys.total_sales, 1, 0) OVER (ORDER BY ys.sales_year)) * 100.0 / LAG(ys.total_sales, 1, NULL) OVER (ORDER BY ys.sales_year) AS yoy_growth_percentage\nFROM YearlySales ys\nORDER BY ys.sales_year;\n</code></pre> <p>Note: <code>EXTRACT(YEAR FROM date_column)</code> is common; other functions like <code>YEAR(date_column)</code> or <code>DATEPART(year, date_column)</code> might be used depending on the SQL dialect.</p> </li> <li> <p>Write a query for cumulative sum in SQL?     Assuming a table <code>Orders</code> with <code>order_date</code> and <code>order_amount</code>.</p> <pre><code>SELECT\n    order_date,\n    order_amount,\n    SUM(order_amount) OVER (ORDER BY order_date ASC) AS cumulative_sum\nFROM Orders\nORDER BY order_date;\n</code></pre> </li> <li> <p>Difference between Function and Stored Procedure?</p> <ul> <li>Functions:<ul> <li>Must return a value.</li> <li>Can be called in <code>SELECT</code>, <code>WHERE</code>, <code>HAVING</code> clauses.</li> <li>Cannot perform DML operations (e.g., <code>INSERT</code>, <code>UPDATE</code>, <code>DELETE</code>) directly on tables.</li> <li>Can only have input parameters.</li> </ul> </li> <li>Stored Procedures:<ul> <li>Can return 0 or N values.</li> <li>Cannot be called in <code>SELECT</code>, <code>WHERE</code>, <code>HAVING</code> clauses directly.</li> <li>Can perform DML and DDL operations.</li> <li>Can have input, output, and input/output parameters.</li> </ul> </li> </ul> </li> <li> <p>Do we use variables in views? No, you cannot directly use user-defined variables (like <code>@variable</code> in SQL Server or <code>SET @variable</code> in MySQL) within a standard SQL view definition. Views are essentially stored queries. To achieve dynamic behavior, you might pass parameters to a stored procedure that then queries the view, or use table-valued functions.</p> </li> <li> <p>What are the limitations of views?</p> <ul> <li>Updatability: Not all views are updatable. Views involving joins, aggregate functions, <code>GROUP BY</code>, <code>DISTINCT</code>, or certain other complex operations are generally not updatable.</li> <li>Performance: A view itself doesn't improve performance. The underlying query is executed every time the view is accessed. Complex views can sometimes be slower than direct queries.</li> <li>Indexing: You cannot directly create indexes on a standard view (though some systems support \"indexed views\" or \"materialized views\" which are different).</li> <li>Complexity: Overly complex views can be hard to understand, debug, and maintain.</li> <li>Security: While views can simplify security by showing only specific columns/rows, misconfigured views can inadvertently expose sensitive data.</li> <li>No Parameters: Standard views do not accept parameters, limiting their dynamic use cases.</li> </ul> </li> </ol>"},{"location":"resources/Terraform_Data_Engineer_Cheatsheet/","title":"Terraform Cheatsheet","text":""},{"location":"resources/Terraform_Data_Engineer_Cheatsheet/#terraform-cheatsheet","title":"Terraform Cheatsheet","text":"<p>AI Assisted (Grok 3)</p> <p>A cheatsheet which is actually useful to setup production grade IaC project using Terraform and Gitlab covering :</p> <ul> <li>Multi-Environment setup</li> <li>CI/CD</li> <li>State Management</li> <li>Rollbacks</li> <li>Capturing and Logging Outputs</li> </ul> <p>And more..</p>"},{"location":"resources/Terraform_Data_Engineer_Cheatsheet/#1-core-concepts","title":"1. Core Concepts","text":"<ul> <li>Infrastructure as Code (IaC): Define  infrastructure (e.g., Lambdas, databases, data lakes, compute clusters) in <code>.tf</code> files.</li> <li>Providers: Plugins for cloud platforms (e.g., <code>aws</code>, <code>google</code>, <code>azurerm</code>) or tools (e.g., <code>snowflake</code>, <code>databricks</code>).</li> <li>Resources: Infrastructure components (e.g., S3 buckets, RDS instances, BigQuery datasets).</li> <li>Modules: Reusable, parameterized templates for common infrastructure patterns.</li> <li>State: Tracks infrastructure state, stored locally (<code>terraform.tfstate</code>) or remotely (e.g., S3, GCS).</li> <li>Workspaces: Isolate environments (e.g., <code>dev</code>, <code>staging</code>, <code>prod</code>) within a single configuration directory using multiple state files. Note: This project uses folder-based isolation (separate directories for each environment) instead of workspaces for better clarity and flexibility.</li> </ul>"},{"location":"resources/Terraform_Data_Engineer_Cheatsheet/#2-key-terraform-components-and-state-management","title":"2. Key Terraform Components and State Management","text":"<ul> <li>State Management: <ul> <li>The <code>terraform.tfstate</code> file records the current state of your infrastructure, mapping Terraform configurations to real-world resources, including dynamically generated attributes (e.g., S3 bucket ARNs, RDS endpoints).</li> <li>Local State: Stored locally by default; suitable for solo or small projects but risky for teams due to potential conflicts or loss.</li> <li>Remote State: Store state in a remote backend (e.g., S3, GCS, Azure Blob) with locking (e.g., DynamoDB) to prevent concurrent modifications and enable team collaboration.</li> <li>Environment Isolation: This project uses separate directories (<code>environments/dev</code>, <code>environments/staging</code>, <code>environments/prod</code>) with distinct state files (e.g., <code>data-infra/dev/terraform.tfstate</code>) instead of Terraform workspaces, providing clear separation and environment-specific configurations.</li> <li>Best Practices: Use remote backends for production, encrypt state files, restrict access via IAM, and regularly back up state files.</li> <li>State Drift: Occurs when manual changes bypass Terraform. Use <code>terraform refresh</code> to update state or <code>terraform import</code> to bring resources under management.</li> </ul> </li> <li> <p>outputs.tf: </p> <ul> <li>Defines output values exposed after <code>terraform apply</code>, such as resource IDs, endpoints, or computed values (e.g., S3 bucket ARN, RDS endpoint).</li> <li>How It Works: Outputs reference attributes stored in the state file, which captures dynamically generated values during resource creation. <code>outputs.tf</code> itself does not store values but declares what to extract from the state.</li> <li>Example:    <pre><code>output \"s3_bucket_arn\" {\n  value       = aws_s3_bucket.bucket.arn\n  description = \"ARN of the S3 bucket\"\n}\n</code></pre></li> <li>After <code>terraform apply</code>, the state file stores the bucket\u2019s ARN, and the output retrieves it.</li> <li>Updating Outputs: Edit <code>outputs.tf</code> in the repository to add, modify, or remove outputs. Commit changes to Git, and they take effect on the next <code>terraform apply</code> or <code>terraform output</code>.</li> <li>Usage: Outputs are displayed in the CLI (<code>terraform output</code>), exported to JSON (<code>terraform output -json</code>), used in CI/CD pipelines, or referenced by other modules via remote state.</li> <li>Best Practices: Use descriptive names and descriptions, avoid sensitive data in outputs, and capture outputs in pipelines for automation.</li> </ul> </li> <li> <p>variables.tf: Declares input variables with types, defaults, and validations to parameterize configurations.</p> </li> <li>terraform.tfvars: Provides default variable values, overridden by environment-specific <code>.tfvars</code> files or CLI flags.</li> <li>backend.tf: Configures the remote state backend, specifying where and how state is stored and locked.</li> </ul>"},{"location":"resources/Terraform_Data_Engineer_Cheatsheet/#3-repository-folder-structure","title":"3. Repository Folder Structure","text":"<p>A well-organized repository structure ensures clarity, modularity, and scalability for managing data infrastructure across multiple environments.</p> <pre><code>data-infra/\n\u251c\u2500\u2500 modules/                     # Reusable Terraform modules\n\u2502   \u251c\u2500\u2500 s3_bucket/               # S3 bucket module\n\u2502   \u2502   \u251c\u2500\u2500 main.tf\n\u2502   \u2502   \u251c\u2500\u2500 variables.tf\n\u2502   \u2502   \u2514\u2500\u2500 outputs.tf\n\u2502   \u251c\u2500\u2500 rds/                     # RDS module\n\u2502   \u2502   \u251c\u2500\u2500 main.tf\n\u2502   \u2502   \u251c\u2500\u2500 variables.tf\n\u2502   \u2502   \u2514\u2500\u2500 outputs.tf\n\u2502   \u251c\u2500\u2500 databricks/              # Databricks module\n\u2502   \u2502   \u251c\u2500\u2500 main.tf\n\u2502   \u2502   \u251c\u2500\u2500 variables.tf\n\u2502   \u2502   \u2514\u2500\u2500 outputs.tf\n\u2502   \u2514\u2500\u2500 glue_job/                # Glue job module\n\u2502       \u251c\u2500\u2500 main.tf\n\u2502       \u251c\u2500\u2500 variables.tf\n\u2502       \u2514\u2500\u2500 outputs.tf\n\u251c\u2500\u2500 environments/                # Environment-specific configurations\n\u2502   \u251c\u2500\u2500 dev/\n\u2502   \u2502   \u251c\u2500\u2500 main.tf            # Dev-specific resources\n\u2502   \u2502   \u251c\u2500\u2500 variables.tf\n\u2502   \u2502   \u251c\u2500\u2500 outputs.tf         # Environment-specific outputs\n\u2502   \u2502   \u251c\u2500\u2500 backend.tf\n\u2502   \u2502   \u2514\u2500\u2500 terraform.tfvars    # Dev variable values\n\u2502   \u251c\u2500\u2500 staging/\n\u2502   \u2502   \u251c\u2500\u2500 main.tf\n\u2502   \u2502   \u251c\u2500\u2500 variables.tf\n\u2502   \u2502   \u251c\u2500\u2500 outputs.tf\n\u2502   \u2502   \u251c\u2500\u2500 backend.tf\n\u2502   \u2502   \u2514\u2500\u2500 terraform.tfvars\n\u2502   \u2514\u2500\u2500 prod/\n\u2502       \u251c\u2500\u2500 main.tf\n\u2502       \u251c\u2500\u2500 variables.tf\n\u2502       \u251c\u2500\u2500 outputs.tf\n\u2502       \u251c\u2500\u2500 backend.tf\n\u2502       \u2514\u2500\u2500 terraform.tfvars\n\u251c\u2500\u2500 scripts/                     # Supporting scripts (e.g., PySpark for Glue)\n\u2502   \u2514\u2500\u2500 transform.py\n\u251c\u2500\u2500 .gitignore                   # Git ignore file\n\u251c\u2500\u2500 README.md                    # Project documentation\n\u2514\u2500\u2500 .gitlab-ci.yml               # CI/CD pipeline configuration\n</code></pre>"},{"location":"resources/Terraform_Data_Engineer_Cheatsheet/#explanation","title":"Explanation","text":"<ul> <li>modules/: Contains reusable modules for resources like S3, RDS, Databricks, and Glue jobs, each with its own <code>outputs.tf</code>.</li> <li>environments/: Separates configurations for <code>dev</code>, <code>staging</code>, and <code>prod</code>, each with its own <code>main.tf</code>, <code>variables.tf</code>, <code>outputs.tf</code>, <code>backend.tf</code>, and <code>terraform.tfvars</code>, providing isolation without workspaces.</li> <li>scripts/: Stores scripts (e.g., PySpark scripts for Glue jobs) uploaded to S3.</li> <li>.gitignore: Ignores <code>.terraform/</code>, <code>*.tfstate</code>, <code>*.tfstate.backup</code>, and sensitive files like <code>*.tfvars</code> with secrets.</li> <li>README.md: Documents setup instructions, module usage, CI/CD pipeline details, artifact review, rollback procedures, output management, and environment isolation.</li> </ul>"},{"location":"resources/Terraform_Data_Engineer_Cheatsheet/#4-basic-commands","title":"4. Basic Commands","text":"<pre><code>terraform init            # Initialize project, download providers\nterraform plan            # Preview changes\nterraform apply           # Apply changes\nterraform destroy         # Tear down infrastructure\nterraform state list      # View managed resources\nterraform state show &lt;resource&gt;   # Inspect resource state\nterraform output          # Display all output values\nterraform output -json    # Export outputs as JSON\n# Note: The following workspace commands are included for reference but are not used in this project, which uses folder-based environment isolation instead.\nterraform workspace list  # List workspaces (not used)\nterraform workspace select &lt;env&gt;  # Switch to environment (not used)\nterraform workspace new &lt;env&gt;     # Create new workspace (not used)\n</code></pre>"},{"location":"resources/Terraform_Data_Engineer_Cheatsheet/#5-provider-configuration","title":"5. Provider Configuration","text":"<pre><code># environments/dev/main.tf\nterraform {\n  required_providers {\n    aws = {\n      source  = \"hashicorp/aws\"\n      version = \"~&gt; 5.0\"\n    }\n    snowflake = {\n      source  = \"Snowflake-Labs/snowflake\"\n      version = \"~&gt; 0.76\"\n    }\n  }\n}\n\nprovider \"aws\" {\n  region = var.region\n  assume_role {\n    role_arn = \"arn:aws:iam::${var.account_id}:role/TerraformRole\"\n  }\n}\n\nprovider \"snowflake\" {\n  account = var.snowflake_account\n  role    = \"ACCOUNTADMIN\"\n}\n</code></pre>"},{"location":"resources/Terraform_Data_Engineer_Cheatsheet/#6-multi-environment-configuration","title":"6. Multi-Environment Configuration","text":""},{"location":"resources/Terraform_Data_Engineer_Cheatsheet/#variables","title":"Variables","text":"<pre><code># environments/dev/variables.tf\nvariable \"region\" {\n  type    = string\n  default = \"us-east-1\"\n}\n\nvariable \"account_id\" {\n  type = string\n}\n\nvariable \"environment\" {\n  type = string\n  validation {\n    condition     = contains([\"dev\", \"staging\", \"prod\"], var.environment)\n    error_message = \"Environment must be dev, staging, or prod.\"\n  }\n}\n</code></pre>"},{"location":"resources/Terraform_Data_Engineer_Cheatsheet/#environment-specific-tfvars","title":"Environment-Specific <code>.tfvars</code>","text":"<pre><code># environments/dev/terraform.tfvars\nregion      = \"us-east-1\"\naccount_id  = \"123456789012\"\nenvironment = \"dev\"\n</code></pre>"},{"location":"resources/Terraform_Data_Engineer_Cheatsheet/#applying-environment-config","title":"Applying Environment Config","text":"<pre><code>cd environments/dev\nterraform init\nterraform apply -var-file=terraform.tfvars\n</code></pre>"},{"location":"resources/Terraform_Data_Engineer_Cheatsheet/#7-remote-state-management","title":"7. Remote State Management","text":"<pre><code># environments/dev/backend.tf\nterraform {\n  backend \"s3\" {\n    bucket         = \"my-terraform-state\"\n    key            = \"data-infra/dev/terraform.tfstate\"\n    region         = \"us-east-1\"\n    dynamodb_table = \"terraform-locks\"\n  }\n}\n</code></pre>"},{"location":"resources/Terraform_Data_Engineer_Cheatsheet/#8-module-example-s3-bucket","title":"8. Module Example: S3 Bucket","text":"<pre><code># modules/s3_bucket/main.tf\nresource \"aws_s3_bucket\" \"bucket\" {\n  bucket = \"${var.environment}-${var.bucket_name}\"\n  acl    = \"private\"\n\n  versioning {\n    enabled = true\n  }\n\n  tags = {\n    Environment = var.environment\n  }\n}\n\n# modules/s3_bucket/variables.tf\nvariable \"bucket_name\" {\n  type = string\n}\n\nvariable \"environment\" {\n  type = string\n}\n\n# modules/s3_bucket/outputs.tf\noutput \"bucket_arn\" {\n  value       = aws_s3_bucket.bucket.arn\n  description = \"ARN of the S3 bucket\"\n}\n\noutput \"bucket_name\" {\n  value       = aws_s3_bucket.bucket.id\n  description = \"Name of the S3 bucket\"\n}\n</code></pre>"},{"location":"resources/Terraform_Data_Engineer_Cheatsheet/#using-the-module","title":"Using the Module","text":"<pre><code># environments/dev/main.tf\nmodule \"data_lake\" {\n  source      = \"../../modules/s3_bucket\"\n  bucket_name = \"data-lake\"\n  environment = var.environment\n}\n\n# environments/dev/outputs.tf\noutput \"data_lake_arn\" {\n  value       = module.data_lake.bucket_arn\n  description = \"ARN of the data lake S3 bucket\"\n}\n</code></pre>"},{"location":"resources/Terraform_Data_Engineer_Cheatsheet/#9-data-infrastructure-examples","title":"9. Data Infrastructure Examples","text":""},{"location":"resources/Terraform_Data_Engineer_Cheatsheet/#snowflake-database","title":"Snowflake Database","text":"<pre><code># Fetch existing Snowflake role\ndata \"snowflake_role\" \"admin\" {\n  name = \"ACCOUNTADMIN\"\n}\n\nresource \"snowflake_database\" \"data_warehouse\" {\n  name = \"${var.environment}_DATA_WAREHOUSE\"\n}\n\nresource \"snowflake_schema\" \"raw\" {\n  database = snowflake_database.data_warehouse.name\n  name     = \"RAW\"\n}\n\n# outputs.tf\noutput \"snowflake_database_name\" {\n  value       = snowflake_database.data_warehouse.name\n  description = \"Name of the Snowflake database\"\n}\n</code></pre>"},{"location":"resources/Terraform_Data_Engineer_Cheatsheet/#databricks-cluster","title":"Databricks Cluster","text":"<pre><code># Fetch existing Databricks workspace\ndata \"databricks_spark_version\" \"latest\" {\n  latest = true\n}\n\nresource \"databricks_cluster\" \"data_processing\" {\n  cluster_name            = \"${var.environment}-data-processing\"\n  spark_version           = data.databricks_spark_version.latest.id\n  node_type_id            = var.environment == \"prod\" ? \"i3.2xlarge\" : \"i3.xlarge\"\n  autotermination_minutes = 30\n}\n\n# outputs.tf\noutput \"databricks_cluster_id\" {\n  value       = databricks_cluster.data_processing.cluster_id\n  description = \"ID of the Databricks cluster\"\n}\n</code></pre>"},{"location":"resources/Terraform_Data_Engineer_Cheatsheet/#aws-glue-crawler","title":"AWS Glue Crawler","text":"<pre><code># Fetch existing Glue database\ndata \"aws_glue_catalog_database\" \"main\" {\n  name = \"${var.environment}_catalog\"\n}\n\nresource \"aws_glue_crawler\" \"s3_crawler\" {\n  name          = \"${var.environment}-s3-crawler\"\n  database_name = data.aws_glue_catalog_database.main.name\n  role          = aws_iam_role.glue_role.arn\n  s3_target {\n    path = module.data_lake.bucket\n  }\n}\n\n# outputs.tf\noutput \"glue_crawler_name\" {\n  value       = aws_glue_crawler.s3_crawler.name\n  description = \"Name of the Glue crawler\"\n}\n</code></pre>"},{"location":"resources/Terraform_Data_Engineer_Cheatsheet/#aws-rds-database","title":"AWS RDS Database","text":"<pre><code># Fetch existing VPC and subnets\ndata \"aws_vpc\" \"default\" {\n  default = true\n}\n\ndata \"aws_subnets\" \"default_subnets\" {\n  filter {\n    name   = \"vpc-id\"\n    values = [data.aws_vpc.default.id]\n  }\n}\n\n# modules/rds/main.tf\nresource \"aws_db_subnet_group\" \"rds_subnet_group\" {\n  name       = \"${var.environment}-rds-subnet-group\"\n  subnet_ids = var.subnet_ids\n  tags = {\n    Environment = var.environment\n  }\n}\n\nresource \"aws_db_instance\" \"rds_instance\" {\n  identifier              = \"${var.environment}-${var.db_name}\"\n  engine                  = var.db_engine\n  engine_version          = var.engine_version\n  instance_class          = var.environment == \"prod\" ? var.prod_instance_class : var.dev_instance_class\n  allocated_storage       = var.allocated_storage\n  username                = var.db_username\n  password                = var.db_password\n  db_subnet_group_name    = aws_db_subnet_group.rds_subnet_group.name\n  vpc_security_group_ids  = var.security_group_ids\n  multi_az                = var.environment == \"prod\" ? true : false\n  backup_retention_period = var.environment == \"prod\" ? 7 : 1\n  skip_final_snapshot     = var.environment != \"prod\"\n  tags = {\n    Environment = var.environment\n  }\n}\n\n# modules/rds/variables.tf\nvariable \"db_name\" {\n  type = string\n}\n\nvariable \"environment\" {\n  type = string\n}\n\nvariable \"db_engine\" {\n  type    = string\n  default = \"postgres\"\n}\n\nvariable \"engine_version\" {\n  type    = string\n  default = \"15.3\"\n}\n\nvariable \"prod_instance_class\" {\n  type    = string\n  default = \"db.m5.large\"\n}\n\nvariable \"dev_instance_class\" {\n  type    = string\n  default = \"db.t3.micro\"\n}\n\nvariable \"allocated_storage\" {\n  type    = number\n  default = 20\n}\n\nvariable \"db_username\" {\n  type = string\n}\n\nvariable \"db_password\" {\n  type      = string\n  sensitive = true\n}\n\nvariable \"subnet_ids\" {\n  type = list(string)\n}\n\nvariable \"security_group_ids\" {\n  type = list(string)\n}\n\n# modules/rds/outputs.tf\noutput \"rds_endpoint\" {\n  value       = aws_db_instance.rds_instance.endpoint\n  description = \"Endpoint of the RDS instance\"\n}\n\n# Using the RDS Module\n# environments/dev/main.tf\nmodule \"rds_postgres\" {\n  source             = \"../../modules/rds\"\n  db_name            = \"datawarehouse\"\n  environment        = var.environment\n  db_engine          = \"postgres\"\n  engine_version     = \"15.3\"\n  prod_instance_class = \"db.m5.large\"\n  dev_instance_class  = \"db.t3.micro\"\n  allocated_storage   = 100\n  db_username        = var.db_username\n  db_password        = var.db_password\n  subnet_ids         = data.aws_subnets.default_subnets.ids\n  security_group_ids = [aws_security_group.rds_sg.id]\n}\n\n# environments/dev/outputs.tf\noutput \"rds_postgres_endpoint\" {\n  value       = module.rds_postgres.rds_endpoint\n  description = \"Endpoint of the RDS PostgreSQL instance\"\n}\n</code></pre>"},{"location":"resources/Terraform_Data_Engineer_Cheatsheet/#aws-glue-pyspark-job","title":"AWS Glue PySpark Job","text":"<pre><code># Fetch S3 bucket from another configuration\ndata \"terraform_remote_state\" \"data_lake\" {\n  backend = \"s3\"\n  config = {\n    bucket = \"my-terraform-state\"\n    key    = \"data-infra/${var.environment}/terraform.tfstate\"\n    region = \"us-east-1\"\n  }\n}\n\n# modules/glue_job/main.tf\nresource \"aws_glue_job\" \"pyspark_job\" {\n  name              = \"${var.environment}-${var.job_name}\"\n  role_arn          = var.glue_role_arn\n  glue_version      = \"4.0\"\n  worker_type       = var.environment == \"prod\" ? \"G.2X\" : \"G.1X\"\n  number_of_workers = var.environment == \"prod\" ? 10 : 2\n  max_retries       = 1\n\n  command {\n    script_location = \"s3://${var.script_bucket}/${var.script_path}\"\n    python_version  = \"python3\"\n    name            = \"glueetl\"\n  }\n\n  default_arguments = {\n    \"--job-name\"              = \"${var.environment}-${var.job_name}\"\n    \"--TempDir\"               = \"s3://${var.temp_bucket}/temp/\"\n    \"--enable-metrics\"         = \"true\"\n    \"--enable-continuous-log\" = \"true\"\n    \"--environment\"           = var.environment\n  }\n\n  tags = {\n    Environment = var.environment\n  }\n}\n\n# modules/glue_job/variables.tf\nvariable \"job_name\" {\n  type = string\n}\n\nvariable \"environment\" {\n  type = string\n}\n\nvariable \"glue_role_arn\" {\n  type = string\n}\n\nvariable \"script_bucket\" {\n  type = string\n}\n\nvariable \"script_path\" {\n  type = string\n}\n\nvariable \"temp_bucket\" {\n  type = string\n}\n\n# modules/glue_job/outputs.tf\noutput \"glue_job_name\" {\n  value       = aws_glue_job.pyspark_job.name\n  description = \"Name of the Glue PySpark job\"\n}\n\n# Using the Glue Job Module\n# environments/dev/main.tf\nmodule \"glue_job\" {\n  source         = \"../../modules/glue_job\"\n  job_name       = \"data-transform\"\n  environment    = var.environment\n  glue_role_arn  = aws_iam_role.glue_role.arn\n  script_bucket  = data.terraform_remote_state.data_lake.outputs.bucket_name\n  script_path    = \"scripts/transform.py\"\n  temp_bucket    = data.terraform_remote_state.data_lake.outputs.bucket_name\n}\n\n# environments/dev/outputs.tf\noutput \"glue_job_name\" {\n  value       = module.glue_job.glue_job_name\n  description = \"Name of the Glue job\"\n}\n</code></pre>"},{"location":"resources/Terraform_Data_Engineer_Cheatsheet/#10-gitlab-cicd-pipeline-for-terraform","title":"10. GitLab CI/CD Pipeline for Terraform","text":"<p>A GitLab CI/CD pipeline automates validation, planning, and deployment of Terraform configurations across environments, with artifacts stored for review, auditing, and output capture. This project uses folder-based environment isolation (folders like <code>environments/dev</code>, <code>environments/staging</code>, etc.) instead of Terraform workspaces for clarity and flexibility.</p>"},{"location":"resources/Terraform_Data_Engineer_Cheatsheet/#terraform-artifacts","title":"Terraform Artifacts","text":"<p>Terraform artifacts are files generated during pipeline jobs (e.g., <code>terraform plan</code>, <code>terraform apply</code>, or <code>terraform output</code>) saved in GitLab for review, auditing, or downstream use. They include:</p> <ul> <li> <p>Plan Artifacts:</p> <ul> <li><code>tfplan</code>: Binary plan file from <code>terraform plan -out=tfplan</code>, used by <code>terraform apply</code>.</li> <li><code>tfplan.json</code>: JSON representation (<code>terraform show -json tfplan</code>) for programmatic analysis.</li> <li><code>tfplan.txt</code>: Human-readable text (<code>terraform show tfplan</code>) for manual review.</li> </ul> </li> <li> <p>Apply Artifacts:</p> <ul> <li><code>apply.log</code>: Log file capturing <code>terraform apply</code> output, detailing infrastructure changes.</li> </ul> </li> <li> <p>Output Artifacts:</p> <ul> <li><code>outputs.json</code>: JSON file capturing <code>terraform output -json</code>, containing dynamically generated values (e.g., S3 bucket ARNs, RDS endpoints).</li> </ul> </li> <li> <p>Purpose:</p> <ul> <li>Review: Plan artifacts for verifying changes before applying.</li> <li>Audit: Apply logs and outputs for compliance and debugging.</li> <li>Consistency: Binary <code>tfplan</code> ensures <code>apply</code> matches the reviewed plan.</li> <li>Automation: Outputs enable integration with other systems or modules.</li> <li>Retention:</li> <li>GitLab artifacts can be stored for a specified period (e.g., <code>1 year</code> or <code>forever</code>, subject to GitLab plan).</li> <li>For long-term audit, upload artifacts to S3 with custom retention policies.</li> </ul> </li> </ul>"},{"location":"resources/Terraform_Data_Engineer_Cheatsheet/#capturing-terraform-outputs","title":"Capturing Terraform Outputs","text":"<p>Outputs defined in <code>outputs.tf</code> are captured in the pipeline to expose dynamically generated values for review, automation, or integration with other systems.</p>"},{"location":"resources/Terraform_Data_Engineer_Cheatsheet/#steps-to-capture-outputs","title":"Steps to Capture Outputs","text":"<ol> <li>Generate Outputs: Run <code>terraform output -json &gt; outputs.json</code> in the <code>apply</code> job to save outputs as a JSON file.</li> <li>Store as Artifacts: Include <code>outputs.json</code> in the <code>artifacts</code> section of <code>.gitlab-ci.yml</code>.</li> <li>Review Outputs: Download <code>outputs.json</code> from the GitLab pipeline UI or parse it programmatically (e.g., <code>jq '.s3_bucket_arn.value' outputs.json</code>).</li> <li>Use Outputs: Reference <code>outputs.json</code> in downstream jobs (e.g., to configure applications) or upload to S3 for long-term storage.</li> </ol>"},{"location":"resources/Terraform_Data_Engineer_Cheatsheet/#example-workflow","title":"Example Workflow","text":"<ul> <li>After <code>apply_dev</code> runs, <code>outputs.json</code> is generated, containing values like <code>data_lake_arn</code> and <code>rds_postgres_endpoint</code>.</li> <li>The team downloads <code>outputs.json</code> to verify resource endpoints.</li> <li>Outputs are uploaded to S3 for audit or used in another job to configure a data pipeline.</li> </ul>"},{"location":"resources/Terraform_Data_Engineer_Cheatsheet/#pipeline-configuration","title":"Pipeline Configuration","text":"<pre><code># .gitlab-ci.yml\nstages:\n  - validate\n  - plan\n  - apply\n  - backup\n\nvariables:\n  AWS_REGION: \"us-east-1\"\n  TF_VAR_account_id: \"123456789012\"\n\n# Cache Terraform plugins\ncache:\n  key: ${CI_PROJECT_ID}\n  paths:\n    - .terraform/\n    - environments/dev/.terraform/\n    - environments/staging/.terraform/\n    - environments/prod/.terraform/\n\n# Base job template\n.terraform_base:\n  image: hashicorp/terraform:1.5.7\n  before_script:\n    - terraform --version\n    - apk add --no-cache aws-cli\n    - aws configure set aws_access_key_id $AWS_ACCESS_KEY_ID\n    - aws configure set aws_secret_access_key $AWS_SECRET_ACCESS_KEY\n    - aws configure set region $AWS_REGION\n\n# Validate Terraform configuration\nvalidate:\n  stage: validate\n  extends: .terraform_base\n  script:\n    - cd environments/dev\n    - terraform init -backend-config=backend.tf\n    - terraform validate\n    - cd ../staging\n    - terraform init -backend-config=backend.tf\n    - terraform validate\n    - cd ../prod\n    - terraform init -backend-config=backend.tf\n    - terraform validate\n\n# Plan for Dev\nplan_dev:\n  stage: plan\n  extends: .terraform_base\n  script:\n    - cd environments/dev\n    - terraform init -backend-config=backend.tf\n    - terraform plan -var-file=terraform.tfvars -out=tfplan\n    - terraform show -json tfplan &gt; tfplan.json\n    - terraform show tfplan &gt; tfplan.txt\n  artifacts:\n    paths:\n      - environments/dev/tfplan\n      - environments/dev/tfplan.json\n      - environments/dev/tfplan.txt\n    expire_in: 1 week\n  only:\n    - main\n    - merge_requests\n\n# Plan for Staging\nplan_staging:\n  stage: plan\n  extends: .terraform_base\n  script:\n    - cd environments/staging\n    - terraform init -backend-config=backend.tf\n    - terraform plan -var-file=terraform.tfvars -out=tfplan\n    - terraform show -json tfplan &gt; tfplan.json\n    - terraform show tfplan &gt; tfplan.txt\n  artifacts:\n    paths:\n      - environments/staging/tfplan\n      - environments/staging/tfplan.json\n      - environments/staging/tfplan.txt\n    expire_in: 1 week\n  only:\n    - main\n    - merge_requests\n\n# Plan for Prod\nplan_prod:\n  stage: plan\n  extends: .terraform_base\n  script:\n    - cd environments/prod\n    - terraform init -backend-config=backend.tf\n    - terraform plan -var-file=terraform.tfvars -out=tfplan\n    - terraform show -json tfplan &gt; tfplan.json\n    - terraform show tfplan &gt; tfplan.txt\n  artifacts:\n    paths:\n      - environments/prod/tfplan\n      - environments/prod/tfplan.json\n      - environments/prod/tfplan.txt\n    expire_in: 1 week\n  only:\n    - main\n    - merge_requests\n\n# Apply for Dev\napply_dev:\n  stage: apply\n  extends: .terraform_base\n  script:\n    - cd environments/dev\n    - terraform init -backend-config=backend.tf\n    - terraform apply -auto-approve tfplan &gt; apply.log 2&gt;&amp;1\n    - terraform output -json &gt; outputs.json\n    - aws s3 cp apply.log s3://my-audit-logs/terraform-apply/dev/$(date +%Y-%m-%d_%H-%M-%S).log || true\n    - aws s3 cp outputs.json s3://my-audit-logs/terraform-outputs/dev/$(date +%Y-%m-%d_%H-%M-%S).json || true\n  artifacts:\n    paths:\n      - environments/dev/apply.log\n      - environments/dev/outputs.json\n    expire_in: 1 year\n  dependencies:\n    - plan_dev\n  when: manual\n  only:\n    - main\n\n# Apply for Staging\napply_staging:\n  stage: apply\n  extends: .terraform_base\n  script:\n    - cd environments/staging\n    - terraform init -backend-config=backend.tf\n    - terraform apply -auto-approve tfplan &gt; apply.log 2&gt;&amp;1\n    - terraform output -json &gt; outputs.json\n    - aws s3 cp apply.log s3://my-audit-logs/terraform-apply/staging/$(date +%Y-%m-%d_%H-%M-%S).log || true\n    - aws s3 cp outputs.json s3://my-audit-logs/terraform-outputs/staging/$(date +%Y-%m-%d_%H-%M-%S).json || true\n  artifacts:\n    paths:\n      - environments/staging/apply.log\n      - environments/staging/outputs.json\n    expire_in: 1 year\n  dependencies:\n    - plan_staging\n  when: manual\n  only:\n    - main\n\n# Apply for Prod\napply_prod:\n  stage: apply\n  extends: .terraform_base\n  script:\n    - cd environments/prod\n    - terraform init -backend-config=backend.tf\n    - terraform apply -auto-approve tfplan &gt; apply.log 2&gt;&amp;1\n    - terraform output -json &gt; outputs.json\n    - aws s3 cp apply.log s3://my-audit-logs/terraform-apply/prod/$(date +%Y-%m-%d_%H-%M-%S).log || true\n    - aws s3 cp outputs.json s3://my-audit-logs/terraform-outputs/prod/$(date +%Y-%m-%d_%H-%M-%S).json || true\n  artifacts:\n    paths:\n      - environments/prod/apply.log\n      - environments/prod/outputs.json\n    expire_in: 1 year\n  dependencies:\n    - plan_prod\n  when: manual\n  only:\n      - main\n  environment:\n    name: production\n\n# Backup State for Dev\nbackup_state_dev:\nstage:\n backup\nextends:\n .terraform_base\nscript:\n  - cd environments/dev\n  - terraform init - backend-config=backend.tf\n  - aws s3 cp s3://my-terraform-state/data-infra/dev/terraform.tfstate s3://my-terraform-state-backup/data-infra/dev/$(date +%Y-%m-%d_%H-%M-%S).tfstate || true\nwhen:\n always\nonly:\n - main\ndependencies:\n - apply_dev\n\n# Backup State for Staging\nbackup_state:\n stage: backup\n extends: .terraform_base\n script:\n  - cd environments/staging\n  - terraform init -backend-config=backend.tf\n  - aws s3 cp s3://my-terraform-state/data-infra/staging/terraform.tfstate s3://my-terraform-state-backup/data-infra/staging/$(date +%Y-%m-%d_%H-%M-%S).tfstate || true\n when: always\n  only:\n      - main\n      dependencies:\n        - apply_staging\n\n# Backup State for Prod\nbackup_state_prod:\n stage: apply\n extends: .terraform_base\n  script:\n   - cd environments/prod\n   - terraform init -backend-config=backend.tf\n   - aws s3 cp s3://my-terraform-state/data-infra/prod/terraform.tfstate s3://my-terraform-state-backup/data-infra/prod/$(date +%Y-%m-%d_%H-%M-%S).tfstate || true\n  when: always\n  only:\n   - main\n  dependencies:\n   - apply_prod\n</code></pre>"},{"location":"resources/Terraform_Data_Engineer_Cheatsheet/#reviewing-plan-apply-and-output-artifacts","title":"Reviewing Plan, Apply, and Output Artifacts","text":"<p>Plan artifacts (<code>tfplan</code>, <code>tfplan.json</code>, <code>tfplan.txt</code>), apply logs (<code>apply.log</code>), and output files (<code>outputs.json</code>) are generated during the <code>plan</code> and <code>apply</code> stages and stored in GitLab for review, auditing, and automation.</p>"},{"location":"resources/Terraform_Data_Engineer_Cheatsheet/#steps-to-review","title":"Steps to Review","text":"<ol> <li> <p>Access Artifacts: In GitLab, navigate to the pipeline or merge request, go to the <code>plan_dev</code>, <code>plan_staging</code>, <code>plan_prod</code>, <code>apply_dev</code>, <code>apply_staging</code>, or <code>apply_prod</code> job, and download the artifacts.</p> </li> <li> <p>Review Plan Artifacts:</p> <ul> <li>Open <code>tfplan.txt</code> for a human-readable summary of proposed changes.</li> <li>Use <code>tfplan.json</code> for programmatic analysis with tools like <code>jq</code> (e.g., <code>jq '.resource_changes[] | {address: .address, change: .change.actions}' tfplan.json</code>).</li> </ul> </li> <li> <p>Review Apply Logs:</p> <ul> <li>Open <code>apply.log</code> to review the changes applied, including resources created, updated, or deleted.</li> <li>For long-term audit, retrieve logs from S3: <code>aws s3 cp s3://my-audit-logs/terraform-apply/&lt;env&gt;/&lt;timestamp&gt;.log .</code></li> </ul> </li> <li> <p>Review Output Artifacts:</p> <ul> <li>Open <code>outputs.json</code> to inspect dynamically generated values (e.g., S3 bucket ARNs, RDS endpoints).</li> <li>Parse with <code>jq</code> (e.g., <code>jq '.data_lake_arn.value' outputs.json</code>).</li> <li>For audit or integration, retrieve from S3: <code>aws s3 cp s3://my-audit-logs/terraform-outputs/&lt;env&gt;/&lt;timestamp&gt;.json .</code></li> </ul> </li> <li> <p>Team Review:</p> <ul> <li>Share artifact links in merge request comments or integrate with notification tools (e.g., Slack) to alert reviewers.</li> <li>Require approval from team members before triggering <code>apply</code> jobs.</li> </ul> </li> <li> <p>Apply Changes: After approving plan artifacts, trigger the corresponding <code>apply</code> job, which uses the <code>tfplan</code> artifact, generates <code>apply.log</code>, and captures <code>outputs.json</code>.</p> </li> </ol>"},{"location":"resources/Terraform_Data_Engineer_Cheatsheet/#example-workflow_1","title":"Example Workflow","text":"<ul> <li>A developer pushes a change to the <code>main</code> branch, triggering <code>plan_dev</code>.</li> <li>The team downloads <code>environments/dev/tfplan.txt</code> to review changes (e.g., new S3 bucket).</li> <li>They check <code>tfplan.json</code> with <code>jq</code> to verify compliance (e.g., correct instance types).</li> <li>After approval, a team member triggers <code>apply_dev</code>, generating <code>apply.log</code> and <code>outputs.json</code>, which are stored as artifacts and uploaded to S3 for audit.</li> <li>The team reviews <code>outputs.json</code> to confirm resource endpoints (e.g., <code>data_lake_arn</code>).</li> </ul>"},{"location":"resources/Terraform_Data_Engineer_Cheatsheet/#11-best-practices-general","title":"11. Best Practices (General)","text":"<ul> <li>Modularize: Break infrastructure into reusable modules (e.g., S3, RDS, Databricks, Glue).</li> <li>Version Control: Store Terraform code in Git, with separate branches for environments.</li> <li>State Security: Use remote backends with encryption and locking (e.g., S3 + DynamoDB).</li> <li>Least Privilege: Use IAM roles with minimal permissions for Terraform execution.</li> <li>Tagging: Enforce consistent tagging for cost allocation and resource tracking.</li> <li>DRY Principle: Use variables, modules, and <code>for_each</code> to avoid duplication.</li> <li>Testing: Use <code>terraform plan</code> to validate changes; consider tools like <code>terratest</code>.</li> <li>CI/CD Integration: Integrate with GitLab CI/CD for automated deployments.</li> <li> <p>Environment Isolation: Use separate folders or state files for <code>dev</code>, <code>staging</code>, <code>prod</code> instead of workspaces for better clarity and flexibility.</p> </li> <li> <p>RDS-Specific Best Practices:</p> <ul> <li>Use <code>multi_az</code> for production to ensure high availability.</li> <li>Store <code>db_password</code> in AWS Secrets Manager and reference it via <code>data</code> sources.</li> <li>Enable automated backups with appropriate retention periods.</li> <li>Use parameter groups to tune database performance per environment.</li> </ul> </li> <li> <p>Glue-Specific Best Practices:</p> <ul> <li>Store PySpark scripts in S3 and reference them in <code>script_location</code>.</li> <li>Use environment-specific worker types and counts to optimize cost and performance.</li> <li>Enable CloudWatch metrics and logs for job monitoring.</li> <li>Use IAM roles with specific permissions for S3, CloudWatch, and other services.</li> </ul> </li> </ul>"},{"location":"resources/Terraform_Data_Engineer_Cheatsheet/#12-best-practices-for-cicd-pipelines-with-iac","title":"12. Best Practices for CI/CD Pipelines with IaC","text":"<ul> <li>Separate Environments: Use distinct jobs for each environment (<code>dev</code>, <code>staging</code>, <code>prod</code>) to prevent cross-contamination. This project uses separate folders (<code>environments/&lt;env&gt;</code>) instead of Terraform workspaces for clear isolation and environment-specific configurations.</li> <li>Manual Approvals: Require manual triggers for <code>apply</code> jobs, especially for production, to avoid unintended changes.</li> <li>State Management: Use remote backends with locking (e.g., S3 with DynamoDB) to prevent concurrent state modifications.</li> <li>Secrets Management: Store sensitive data (e.g., AWS credentials, DB passwords) in GitLab CI/CD variables or a secrets manager like AWS Secrets Manager.</li> <li>Linting and Validation: Run <code>terraform validate</code> and tools like <code>tflint</code> in the pipeline to catch errors early.</li> <li>Plan Artifacts: Store <code>terraform plan</code> outputs to (<code>tfplan</code>, <code>tfplan.json</code>, <code>tfplan.txt</code>) for review before applying, with JSON for automation and text for human-readable review.</li> <li>Apply Log Retention: Store <code>terraform apply</code> logs (<code>apply.log</code>) as GitLab artifacts (e.g., for 1 year) or upload to S3 (e.g., my-audit-logs) for long-term audit retention, with encryption and access controls to meet compliance requirements.</li> <li> <p>Output Management: Capture <code>terraform output -json</code> as <code>outputs.json</code> artifacts for review, audit, or integration with other systems. Avoid sensitive data in outputs and store in secure storage (e.g., S3) for long-term retention.</p> </li> <li> <p>Data Source Management:</p> <ul> <li>Minimize data source queries to reduce API calls and improve pipeline performance.</li> <li>Use precise filters (e.g., tags, IDs) to avoid ambiguous results.</li> <li>Handle missing resources with <code>count</code> or <code>try</code> to prevent pipeline failures.</li> <li>Validate data source resolution in the <code>plan</code> stage to catch errors early.</li> <li>Document data source dependencies in <code>README.md</code> or comments in <code>.tf</code> files.</li> </ul> </li> <li> <p>Secure Artifacts: Restrict access to artifacts to authorized users to protect sensitive data in plan, apply, log, and output files.</p> </li> <li>Role-Based Access: Use least-privilege IAM roles for CI/CD runners, scoped to specific environments.</li> <li>Testing: Integrate testing tools like <code>terratest</code> or <code>checkov</code> for unit, integration, and compliance testing of Terraform modules and plans.</li> <li>Pipeline Triggers: Run pipelines on merge requests and main branch pushes to catch issues early.</li> <li> <p>Version Pinning: Pin Terraform and provider versions in the pipeline to avoid breaking changes.</p> </li> <li> <p>Rollback Strategy:</p> <ul> <li>State Backups: Regularly back up <code>terraform.tfstate</code> to a separate S3 bucket after <code>apply</code> to enable recovery from failures or unintended changes. Use versioning on the backup bucket for additional protection.</li> <li>Terraform Destroy: Document procedures for <code>terraform destroy</code> to remove all managed resources in an environment, including prerequisites (e.g., emptying S3 buckets) and post-cleanup steps (e.g., removing state files).</li> <li>Manual Rollback: Maintain documentation for manual rollback of specific resources (e.g., reverting RDS instance types, deleting Glue jobs) when <code>destroy</code> is too destructive, including steps to restore from backups or previous state files.</li> <li>Implementation: Automate state backups in the CI/CD pipeline post-<code>apply</code>. Include rollback instructions in <code>README.md</code> or a dedicated <code>ROLLBACK.md</code>.</li> </ul> </li> <li> <p>Monitoring and Logging: Enable verbose logging (<code>TF_LOG=DEBUG</code>) for debugging and monitor pipeline logs in GitLab.</p> </li> <li>Code Reviews: Require peer reviews for merge requests to ensure quality and catch potential issues.</li> <li>Documentation: Include pipeline setup, artifact review, log retention, rollback procedures, output management, and data source usage instructions in <code>README.md</code>.</li> </ul>"},{"location":"resources/Terraform_Data_Engineer_Cheatsheet/#rollback-strategy-details","title":"Rollback Strategy Details","text":""},{"location":"resources/Terraform_Data_Engineer_Cheatsheet/#state-backups","title":"State Backups","text":"<ul> <li>Why: The <code>terraform.tfstate</code> file tracks resource mappings, critical for recovery after failed <code>apply</code> or unintended changes.</li> <li> <p>How:</p> <ul> <li>Configure the pipeline to copy <code>terraform.tfstate</code> to a backup S3 bucket post-<code>apply</code> (see <code>backup_state_*</code> jobs).</li> <li>Enable versioning on the backup bucket (<code>my-terraform-state-backup</code>) to retain historical states.</li> <li>Example: <code>aws s3 cp s3://my-terraform-state/data-infra/dev/terraform.tfstate s3://my-terraform-state-backup/data-infra/dev/$(date +%Y-%m-%d_%H-%M-%S).tfstate</code></li> </ul> </li> <li> <p>Restore Process:</p> <ol> <li>Identify the backup state file (e.g., via S3 console or <code>aws s3 ls</code>).</li> <li>Copy to the active state location: <code>aws s3 cp s3://my-terraform-state-backup/data-infra/dev/&lt;timestamp&gt;.tfstate s3://my-terraform-state/data-infra/dev/terraform.tfstate</code>.</li> <li>Run <code>terraform plan</code> to verify alignment with infrastructure.</li> <li>Apply changes if needed: <code>terraform apply</code>.</li> </ol> </li> <li> <p>Best Practices:</p> <ul> <li>Encrypt backups with S3 server-side encryption.</li> <li>Restrict access via IAM policies.</li> <li>Schedule regular backup jobs or trigger post-<code>apply</code>.</li> <li>Test restoration periodically to ensure reliability.</li> </ul> </li> </ul>"},{"location":"resources/Terraform_Data_Engineer_Cheatsheet/#terraform-destroy","title":"Terraform Destroy","text":"<ul> <li>Why: <code>terraform destroy</code> removes all Terraform-managed resources in an environment, useful for complete rollback.</li> <li> <p>How:</p> <ul> <li>Run <code>terraform destroy</code> in the environment directory (e.g., <code>environments/dev</code>).</li> <li>Example: <code>cd environments/dev; terraform init; terraform destroy -var-file=terraform.tfvars</code>.</li> </ul> </li> <li> <p>Prerequisites:</p> <ul> <li>Empty S3 buckets (delete objects or disable versioning).</li> <li>Remove dependent resources not managed by Terraform (e.g., manually created RDS snapshots).</li> <li>Verify state file integrity before execution.</li> </ul> </li> <li> <p>Post-Cleanup:</p> <ul> <li>Remove state file from S3: <code>aws s3 rm s3://my-terraform-state/data-infra/dev/terraform.tfstate</code>.</li> <li>Update lock table (e.g., DynamoDB) if necessary.</li> </ul> </li> <li> <p>Documentation:</p> <ul> <li>Create a <code>ROLLBACK.md</code> with steps:   <pre><code># Terraform Destroy Rollback\n1. Navigate to the environment: `cd environments/&lt;env&gt;`.\n2. Initialize: `terraform init -backend-config=backend.tf`.\n3. Empty S3 buckets: `aws s3 rm s3://&lt;bucket&gt; --recursive`.\n4. Run destroy: `terraform destroy -var-file=terraform.tfvars`.\n5. Remove state: `aws s3 rm s3://my-terraform-state/data-infra/&lt;env&gt;/terraform.tfstate`.\n</code></pre></li> <li> <p>Risks:</p> </li> <li> <p>Destructive; unsuitable for partial rollbacks.</p> </li> <li>Requires careful validation to avoid data loss.</li> </ul> </li> </ul>"},{"location":"resources/Terraform_Data_Engineer_Cheatsheet/#manual-rollback","title":"Manual Rollback","text":"<ul> <li>Why: For targeted rollbacks when <code>destroy</code> is too aggressive (e.g., reverting an RDS instance type change).</li> <li> <p>How:</p> <ul> <li>Modify Terraform code to revert changes (e.g., set <code>instance_class</code> back to original value).</li> <li>Use a previous state backup to restore resource mappings.</li> <li> <p>Example (RDS rollback):</p> </li> <li> <p>Restore state: <code>aws s3 cp s3://my-terraform-state-backup/data-infra/dev/&lt;timestamp&gt;.tfstate s3://my-terraform-state/data-infra/dev/terraform.tfstate</code>.</p> </li> <li>Update <code>main.tf</code> or variables (e.g., <code>instance_class = \"db.t3.micro\"</code>).</li> <li>Run <code>terraform plan</code> and <code>terraform apply</code>.</li> </ul> </li> <li> <p>Specific Cases:</p> <ul> <li>S3: Delete objects or restore from versioning.</li> <li>RDS: Revert instance type, restore from snapshot, or adjust parameters.</li> <li>Glue: Delete jobs or revert configurations via Terraform.</li> <li>Snowflake: Drop schemas/tables or restore from time travel.</li> </ul> </li> <li> <p>Documentation:</p> </li> <li> <p>In <code>ROLLBACK.md</code>, detail resource-specific rollback steps:     </p><pre><code>## Manual Rollback\n# RDS Example\n\n1. Restore state: `aws s3 cp s3://my-terraform-state-backup/...`.\n2. Update `instance_class` in `modules/rds/main.tf`.\n3. Run `terraform plan` and `terraform apply`.\n\n## S3 Bucket\n\n1. Delete objects: `aws s3 rm s3://&lt;bucket&gt; --recursive`.\n2. Update Terraform to remove or bucket or adjust policies.\n</code></pre><p></p> </li> <li> <p>Best Practices:</p> <ul> <li>Test rollbacks in <code>dev</code> before applying to production.</li> <li>Maintain backups (e.g., RDS snapshots) before applying changes.</li> <li>Log all rollback actions for audit purposes.</li> </ul> </li> </ul>"},{"location":"resources/Terraform_Data_Engineer_Cheatsheet/#13-common-gotchas","title":"13. Common Gotchas","text":"<ul> <li>State Drift: Use <code>terraform refresh</code> to sync state with actual infrastructure.</li> <li>Provider Versions: Pin versions to avoid breaking changes.</li> <li>Resource Dependencies: Use <code>depends_on</code> or implicit dependencies (e.g., referencing outputs).</li> <li>Sensitive Data: Store secrets in AWS Secrets Manager or HashiCorp Vault, not <code>.tfvars</code>.</li> <li> <p>Rate Limits: Handle cloud provider API limits with <code>terraform plan</code> retries or delays.</p> </li> <li> <p>RDS-Specific Gotchas:</p> <ul> <li>Changing <code>allocated_storage</code> or <code>instance_class</code> may cause downtime.</li> <li>Ensure <code>subnet_ids</code> are in the same VPC as <code>security_group_ids</code>.</li> <li>Avoid <code>skip_final_snapshot</code> in production to prevent data loss.</li> </ul> </li> <li> <p>Glue-Specific Gotchas:</p> <ul> <li>Ensure the Glue IAM role has permissions for S3 bucket script and temp directories.</li> <li>Verify <code>glue_version</code> compatibility with PySpark script dependencies.</li> <li>Monitor job execution time to avoid unexpected costs in production.</li> </ul> </li> <li> <p>CI/CD-Specific Gotchas:</p> <ul> <li>Ensure CI/CD runners have correct permissions for each environment.</li> <li>Avoid caching sensitive state files or logs in CI/CD pipelines.</li> <li>Test pipeline changes in a non-production branch first.</li> </ul> </li> <li> <p>Output-Specific Gotchas:</p> <ul> <li>Avoid outputting sensitive data (e.g., database passwords) in <code>outputs.tf</code>.</li> <li>Outputs are only updated after <code>terraform apply</code> or <code>terraform refresh</code> if state changes.</li> <li>Missing outputs in <code>outputs.tf</code> can\u2019t be retrieved without redefining them.</li> </ul> </li> <li> <p>Data Source-Specific Gotchas:</p> <ul> <li>Missing resources cause errors unless handled (e.g., with <code>count</code> or <code>try</code>).</li> <li>Ambiguous data sources (e.g., no filters) may return multiple results, causing failures.</li> <li>Excessive API queries from data sources can hit provider rate limits, slowing pipelines.</li> <li>Ensure provider versions support required data source attributes.</li> </ul> </li> </ul>"},{"location":"resources/Terraform_Data_Engineer_Cheatsheet/#14-advanced-features","title":"14. Advanced Features","text":"<ul> <li> <p>Dynamic Blocks: Generate repetitive resource configurations.   </p><pre><code>resource \"aws_s3_bucket\" \"bucket\" {\n  bucket = \"${var.environment}-${var.bucket_name}\"\n  dynamic \"lifecycle_rule\" {\n    for_each = var.lifecycle_rules\n    content {\n      id      = lifecycle_rule.value.id\n      status  = \"Enabled\"\n      expiration {\n        days = lifecycle_rule.value.days\n      }\n    }\n  }\n}\n</code></pre><p></p> </li> <li> <p>Count and For_Each: Create multiple resources dynamically.   </p><pre><code>resource \"aws_s3_bucket\" \"buckets\" {\n  for_each = var.bucket_names\n  bucket   = \"${each.key}-${var.environment}\"\n}\n</code></pre><p></p> </li> <li> <p>Data Sources:</p> <ul> <li>Definition: Query existing resources or external data without managing them to retrieve attributes (e.g., VPC IDs, AMI IDs, database names).</li> <li> <p>Syntax:   </p><pre><code>data \"&lt;PROVIDER&gt;_&lt;TYPE&gt;\" \"&lt;NAME&gt;\" {\n  [CONFIG]\n}\n</code></pre>   Example: Fetch a VPC:   <pre><code>data \"aws_vpc\" \"default\" {\n  default = true\n}\n</code></pre><p></p> </li> <li> <p>Use Cases:</p> <ul> <li>Reference existing infrastructure (e.g., VPCs, IAM roles).</li> <li>Fetch dynamic values (e.g., latest AMI ID, availability zones).</li> <li>Integrate with other Terraform state files (e.g., <code>terraform_remote_state</code>).</li> <li>Query external APIs or scripts (e.g., <code>external</code> provider).</li> </ul> </li> <li> <p>Examples:</p> <ul> <li> <p>AWS: Fetch subnets for an RDS instance:   </p><pre><code>data \"aws_vpc\" \"default\" {\n  default = true\n}\n\ndata \"aws_subnets\" \"default_subnets\" {\n  filter {\n    name   = \"vpc-id\"\n    values = [data.aws_vpc.default.id]\n  }\n}\n\nresource \"aws_db_instance\" \"rds\" {\n  db_subnet_group_name = aws_db_subnet_group.rds.name\n  vpc_security_group_ids = [aws_security_group.rds.id]\n  subnet_ids = data.aws_subnets.default_subnets.ids\n}\n</code></pre><p></p> </li> <li> <p>Remote State: Access another project\u2019s outputs:   </p><pre><code># Fetch outputs from another Terraform configuration\ndata \"terraform_remote_state\" \"network\" {\n  backend = \"s3\"\n  config = {\n    bucket = \"my-terraform-state\"\n    key    = \"networking/terraform.tfstate\"\n    region = \"us-east-1\"\n  }\n}\n# Use the remote state output in a resource\nresource \"aws_s3_bucket\" \"bucket\" {\n  bucket = \"${var.environment}-data-lake\"\n  tags = {\n    VPC = data.terraform_remote_state.network.outputs.vpc_id\n  }\n}\n</code></pre><p></p> </li> <li>External Data: Fetch data from a script:   <pre><code>data \"external\" \"config\" {\n  program = [\"bash\", \"${path.module}/get_config.sh\"]\n}\n\nresource \"aws_s3_bucket\" \"bucket\" {\n  bucket = \"${var.environment}-${data.external.config.result.bucket_suffix}\"\n}\n</code></pre></li> </ul> </li> <li> <p>Best Practices:</p> <ul> <li>Minimize queries to reduce API calls and improve performance.</li> <li>Use specific filters (e.g., <code>tags</code>, <code>id</code>) to avoid ambiguity.</li> <li>Handle missing resources with <code>count</code> or <code>try</code>:   <pre><code>data \"aws_vpc\" \"existing\" {\n  count = var.vpc_id != \"\" ? 1 : 0\n  id    = var.vpc_id\n}\n</code></pre></li> <li>Avoid sensitive data in data source outputs.</li> <li>Pin provider versions to ensure data source compatibility.</li> <li>Document usage with comments in <code>.tf</code> files.</li> </ul> </li> <li> <p>Gotchas:</p> <ul> <li>Missing resources cause errors unless handled.</li> <li>Ambiguous results (e.g., no filters) cause failures.</li> <li>Rate limits may slow pipelines with many data sources.</li> <li>Ensure state file access for <code>terraform_remote_state</code>.</li> </ul> </li> <li> <p>examples:</p> <ul> <li>Snowflake: <code>data.snowflake_role</code> to fetch an existing role.</li> <li>Databricks: <code>data.databricks_spark_version</code> to get the latest Spark version.</li> <li>Glue Crawler: <code>data.aws_glue_catalog_database</code> to reference an existing database.</li> <li>RDS: <code>data.aws_vpc</code> and <code>data.aws_subnets</code> to fetch VPC and subnets.</li> <li>Glue Job: <code>data.terraform_remote_state</code> to access the S3 bucket from another configuration.</li> </ul> </li> </ul> </li> </ul>"},{"location":"resources/Terraform_Data_Engineer_Cheatsheet/#15-debugging","title":"15. Debugging","text":"<ul> <li>Verbose Logging: <code>export TF_LOG=DEBUG</code></li> <li>State Inspection: <code>terraform state show aws_s3_bucket.bucket</code></li> <li>Plan Analysis: <code>terraform plan -out=tfplan; terraform show -json tfplan</code></li> <li>Output Inspection: <code>terraform output -json</code> or <code>terraform output &lt;output_name&gt;</code></li> <li>Data Source Debugging: Check <code>TF_LOG=DEBUG</code> for data source query details; use <code>terraform plan</code> to verify fetched values.</li> <li>Error Handling: Check provider-specific error codes in logs.</li> </ul>"},{"location":"resources/Terraform_Data_Engineer_Cheatsheet/#16-tools-extensions","title":"16. Tools &amp; Extensions","text":"<ul> <li>Terraform CLI: Core tool for managing infrastructure.</li> <li>tfenv: Manage multiple Terraform versions.</li> <li>tflint: Linter for Terraform code.</li> <li>terraform-docs: Generate documentation from <code>.tf</code> files.</li> <li>VS Code Terraform Extension: Syntax highlighting and autocompletion.</li> </ul>"},{"location":"resources/Terraform_Data_Engineer_Cheatsheet/#notes","title":"Notes","text":"<ul> <li>Provider Requirements: Ensure providers (e.g., <code>aws</code>, <code>snowflake</code>, <code>databricks</code>) are configured in <code>terraform.tf</code> or <code>main.tf</code> for data sources to work. The cheatsheet\u2019s Section 5 already includes AWS and Snowflake providers.</li> <li>IAM Permissions: Data sources require read permissions for the queried resources (e.g., <code>ec2:DescribeVpcs</code> for <code>data.aws_vpc</code>). Ensure the CI/CD runner\u2019s IAM role includes these.</li> <li>Pipeline Integration: Data sources are resolved in the <code>plan</code> and <code>apply</code> jobs without changes to <code>.gitlab-ci.yml</code>. Monitor pipeline logs for data source errors.</li> <li>Testing: Run <code>terraform plan</code> locally in <code>environments/dev</code> to verify data sources fetch expected values. Check <code>tfplan.json</code> for details.</li> <li>Use <code>data</code> blocks to fetch existing resources (e.g., VPCs, Snowflake roles) or dynamic values (e.g., AMI IDs).</li> <li>Define in <code>main.tf</code> (e.g., <code>data.aws_vpc.default</code>) and reference in resources or modules.</li> <li>Document dependencies in <code>main.tf</code> comments.</li> <li>Validate in CI/CD <code>plan</code> jobs to catch errors.</li> <li>Minimize queries, use filters, handle missing resources, validate in <code>plan</code>, and document dependencies.</li> </ul>"},{"location":"resources/Terraform_Data_Engineer_Cheatsheet/#appendix","title":"Appendix","text":""},{"location":"resources/Terraform_Data_Engineer_Cheatsheet/#a-more-detailed-look-at-data-sources","title":"A More Detailed Look at Data Sources","text":"<ul> <li> <p>Definition: Data sources in Terraform allow you to fetch information about existing resources or external data that are not managed by your current Terraform configuration. They provide a way to query cloud providers, APIs, or other systems to retrieve attributes like IDs, ARNs, or configurations without creating or modifying resources.</p> </li> <li> <p>Purpose:</p> <ul> <li>Reference resources created outside Terraform (e.g., an existing VPC or IAM role).</li> <li>Access computed values (e.g., the latest AMI ID, availability zones).</li> <li>Integrate with other Terraform configurations via remote state.</li> <li>Avoid hardcoding values by dynamically retrieving them.</li> </ul> </li> <li> <p>How They Work:</p> <ul> <li>Data sources are defined using the <code>data</code> block in Terraform configuration files (e.g., <code>main.tf</code>).</li> <li>They query the provider\u2019s API (e.g., AWS, Snowflake, Databricks) during <code>terraform plan</code> or <code>terraform apply</code> to fetch data.</li> <li>The retrieved data is stored in the Terraform state file for the duration of the operation but does not manage the underlying resource.</li> <li>Data source attributes can be referenced in other resources, outputs, or modules, just like managed resources.</li> </ul> </li> </ul>"},{"location":"resources/Terraform_Data_Engineer_Cheatsheet/#syntax-and-structure","title":"Syntax and Structure","text":"<p>A data source block follows this format: </p><pre><code>data \"&lt;PROVIDER&gt;_&lt;TYPE&gt;\" \"&lt;NAME&gt;\" {\n  [CONFIGURATION]\n}\n</code></pre><p></p> <ul> <li>PROVIDER: The provider (e.g., <code>aws</code>, <code>snowflake</code>, <code>databricks</code>).</li> <li>TYPE: The resource type to query (e.g., <code>aws_vpc</code>, <code>snowflake_database</code>).</li> <li>NAME: A local name for referencing the data source in your configuration.</li> <li>CONFIGURATION: Filters or parameters to identify the resource (e.g., <code>id</code>, <code>name</code>, <code>tags</code>).</li> </ul> <p>Example: </p><pre><code>data \"aws_vpc\" \"default\" {\n  default = true\n}\n</code></pre><p></p> <ul> <li>This queries AWS for the default VPC and makes its attributes (e.g., <code>id</code>, <code>cidr_block</code>) available as <code>data.aws_vpc.default.&lt;attribute&gt;</code>.</li> </ul>"},{"location":"resources/Terraform_Data_Engineer_Cheatsheet/#key-characteristics","title":"Key Characteristics","text":"<ul> <li>Read-Only: Data sources only fetch data; they cannot create, update, or delete resources.</li> <li>Provider Dependency: Each data source is tied to a specific provider, which must be configured in your Terraform setup (e.g., <code>provider \"aws\"</code>).</li> <li>State Integration: Data source results are cached in the state file during execution but do not persist as managed resources.</li> <li>Dynamic Values: Data sources are ideal for retrieving values that change over time (e.g., latest AMI IDs, external resource IDs).</li> <li>Lifecycle: Data sources are refreshed during <code>terraform plan</code> or <code>terraform apply</code> unless explicitly skipped (e.g., using <code>-refresh=false</code>).</li> </ul>"},{"location":"resources/Terraform_Data_Engineer_Cheatsheet/#common-use-cases","title":"Common Use Cases","text":"<ol> <li>Referencing Existing Infrastructure:</li> <li>Fetch details of resources created manually or by other Terraform configurations (e.g., VPCs, subnets, IAM roles).</li> <li>Example: Use an existing security group for an RDS instance.</li> <li>Dynamic Resource Configuration:</li> <li>Retrieve dynamic values like the latest AMI ID or availability zones to configure resources.</li> <li>Example: Select the latest Ubuntu AMI for an EC2 instance.</li> <li>Cross-Module Integration:</li> <li>Access outputs from another Terraform configuration using the <code>terraform_remote_state</code> data source.</li> <li>Example: Reference an S3 bucket ARN from a different project\u2019s state.</li> <li>Environment-Specific Configurations:</li> <li>Query environment-specific resources (e.g., a Snowflake database in <code>prod</code>).</li> <li>Example: Fetch a Snowflake database name for a schema.</li> <li>External Data:</li> <li>Use the <code>http</code> or <code>external</code> provider to fetch data from APIs or scripts.</li> <li>Example: Retrieve a configuration value from an external API.</li> </ol>"},{"location":"resources/Terraform_Data_Engineer_Cheatsheet/#example-fetching-an-existing-vpc","title":"Example: Fetching an Existing VPC","text":"<pre><code># Fetch the default VPC\ndata \"aws_vpc\" \"default\" {\n  default = true\n}\n\n# Fetch subnets in the VPC\ndata \"aws_subnets\" \"default_subnets\" {\n  filter {\n    name   = \"vpc-id\"\n    values = [data.aws_vpc.default.id]\n  }\n}\n\n# Use the subnet IDs in an RDS instance\nmodule \"rds_postgres\" {\n  source         = \"../../modules/rds\"\n  db_name        = \"datawarehouse\"\n  environment    = var.environment\n  subnet_ids     = data.aws_subnets.default_subnets.ids\n  security_group_ids = [aws_security_group.rds_sg.id]\n}\n</code></pre> <ul> <li>Here, <code>data.aws_vpc.default</code> retrieves the default VPC\u2019s ID, and <code>data.aws_subnets.default_subnets</code> fetches its subnets, which are then used in the RDS module.</li> </ul>"},{"location":"resources/Terraform_Data_Engineer_Cheatsheet/#example-remote-state-data-source","title":"Example: Remote State Data Source","text":"<pre><code># Fetch outputs from another Terraform configuration\ndata \"terraform_remote_state\" \"network\" {\n  backend = \"s3\"\n  config = {\n    bucket = \"my-terraform-state\"\n    key    = \"networking/terraform.tfstate\"\n    region = \"us-east-1\"\n  }\n}\n\n# Use the remote state output in a resource\nresource \"aws_s3_bucket\" \"bucket\" {\n  bucket = \"${var.environment}-data-lake\"\n  tags = {\n    VPC = data.terraform_remote_state.network.outputs.vpc_id\n  }\n}\n</code></pre> - This retrieves the <code>vpc_id</code> output from a separate Terraform configuration\u2019s state file."},{"location":"resources/Terraform_Data_Engineer_Cheatsheet/#example-external-data-source","title":"Example: External Data Source","text":"<pre><code># Use the external provider to run a script\ndata \"external\" \"config\" {\n  program = [\"bash\", \"${path.module}/get_config.sh\"]\n}\n\n# Use the result in a resource\nresource \"aws_s3_bucket\" \"bucket\" {\n  bucket = \"${var.environment}-${data.external.config.result.bucket_suffix}\"\n}\n</code></pre> <ul> <li>The <code>get_config.sh</code> script returns JSON (e.g., <code>{\"bucket_suffix\": \"data-lake\"}</code>), which is used to name the bucket.</li> </ul>"},{"location":"resources/Terraform_Data_Engineer_Cheatsheet/#integration-with-cicd-setup","title":"Integration with CI/CD  Setup","text":"<ul> <li>Folder-Based Structure: The project uses separate directories (<code>environments/dev</code>, <code>environments/staging</code>, <code>environments/prod</code>) for environment isolation. Data sources can be defined in each environment\u2019s <code>main.tf</code> to fetch environment-specific resources (e.g., a <code>prod</code> VPC).</li> <li>CI/CD Pipeline: Data sources are queried during <code>terraform plan</code> and <code>terraform apply</code> jobs in the GitLab pipeline, ensuring dynamic values are resolved automatically.</li> <li>Modules: Data sources can be used in modules (e.g., <code>modules/rds</code>) to reference existing infrastructure, as shown in the RDS example above.</li> <li>State Management: Data source results are stored in the environment-specific state file (e.g., <code>data-infra/dev/terraform.tfstate</code>) during execution.</li> </ul>"},{"location":"resources/Terraform_Data_Engineer_Cheatsheet/#best-practices","title":"Best Practices","text":"<ol> <li>Minimize Queries: Avoid excessive data source queries to reduce API calls and improve performance.</li> <li>Use Filters: Specify precise filters (e.g., <code>tags</code>, <code>id</code>) to avoid ambiguous results.</li> <li>Handle Missing Resources:</li> <li>Use <code>count</code> or <code>try</code> to handle cases where a data source might not return results.</li> <li>Example:      <pre><code>data \"aws_vpc\" \"existing\" {\n  count = var.vpc_id != \"\" ? 1 : 0\n  id    = var.vpc_id\n}\n</code></pre></li> <li>Avoid Sensitive Data: Don\u2019t expose sensitive attributes (e.g., secrets) in data source outputs.</li> <li>Pin Provider Versions: Ensure the provider version supports the data source type and attributes.</li> <li>Document Usage: Include comments in <code>main.tf</code> explaining why a data source is used.</li> <li>Test in CI/CD: Verify data sources resolve correctly in the pipeline\u2019s <code>plan</code> stage.</li> </ol>"},{"location":"resources/Terraform_Data_Engineer_Cheatsheet/#common-gotchas","title":"Common Gotchas","text":"<ul> <li>Missing Resources: If a data source can\u2019t find a resource, Terraform will error unless handled (e.g., with <code>count</code> or <code>try</code>).</li> <li>Stale Data: Data sources rely on provider APIs; ensure the provider is up-to-date to avoid stale results.</li> <li>Rate Limits: Excessive data source queries may hit provider API limits, causing failures in <code>plan</code> or <code>apply</code>.</li> <li>State Dependency: Data sources are resolved during execution, so ensure the state file is accessible (e.g., in remote backends).</li> <li>Ambiguous Results: Without proper filters, data sources may return multiple results, causing errors.</li> </ul>"},{"location":"resources/Terraform_Data_Engineer_Cheatsheet/#a-deeper-look-into-environment-isolation-approaches","title":"A deeper look into Environment Isolation Approaches","text":""},{"location":"resources/Terraform_Data_Engineer_Cheatsheet/#folder-based-isolation","title":"Folder-Based Isolation","text":"<p>Folder-based isolation involves organizing Terraform configurations into separate directories for each environment (e.g., <code>environments/dev/</code>, <code>environments/staging/</code>, <code>environments/prod/</code>). Each directory contains its own <code>main.tf</code>, <code>variables.tf</code>, <code>outputs.tf</code>, <code>backend.tf</code>, and <code>terraform.tfvars</code>, with distinct state files and configurations tailored to the environment.</p>"},{"location":"resources/Terraform_Data_Engineer_Cheatsheet/#terraform-workspaces","title":"Terraform Workspaces","text":"<p>Terraform workspaces allow multiple environments to share the same configuration files within a single directory, using different state files for each workspace (e.g., <code>default</code>, <code>dev</code>, <code>staging</code>, <code>prod</code>). Workspaces are managed with commands like <code>terraform workspace new &lt;env&gt;</code> and <code>terraform workspace select &lt;env&gt;</code>, and variables are typically controlled via CLI flags or conditional logic.</p>"},{"location":"resources/Terraform_Data_Engineer_Cheatsheet/#advantages-of-folder-based-isolation","title":"Advantages of Folder-Based Isolation","text":"<p>The folder-based isolation approach, as demonstrated in the cheatsheet's <code>environments/</code> structure, offers several advantages over workspaces, particularly for data infrastructure projects involving complex resources like S3 buckets, RDS databases, Snowflake, Databricks, and Glue jobs.</p>"},{"location":"resources/Terraform_Data_Engineer_Cheatsheet/#1-clear-separation-of-configurations","title":"1. Clear Separation of Configurations","text":"<ul> <li>Advantage: Each environment has its own dedicated directory with independent configuration files, making it easier to customize resources, variables, and outputs without relying on conditional logic.</li> <li>Example: In the cheatsheet, <code>environments/dev/main.tf</code> might define a smaller RDS instance (<code>db.t3.micro</code>) compared to <code>environments/prod/main.tf</code> (<code>db.m5.large</code>). This is explicit and avoids complex <code>if</code> statements or workspace-specific variables.</li> <li>Workspace Challenge: With workspaces, all environments share the same <code>main.tf</code>, requiring conditional logic (e.g., <code>count = terraform.workspace == \"prod\" ? 1 : 0</code>) or variable overrides, which can lead to errors or reduced readability.</li> <li>Impact: Folder-based isolation improves maintainability and reduces the risk of misconfiguration, especially for data engineers managing diverse infrastructure across environments.</li> </ul>"},{"location":"resources/Terraform_Data_Engineer_Cheatsheet/#2-independent-state-files-and-backends","title":"2. Independent State Files and Backends","text":"<ul> <li>Advantage: Each environment has its own state file and backend configuration, stored in separate paths (e.g., <code>s3://my-terraform-state/data-infra/dev/terraform.tfstate</code> vs. <code>s3://my-terraform-state/data-infra/prod/terraform.tfstate</code>), ensuring complete isolation.</li> <li>Example: The cheatsheet's <code>environments/dev/backend.tf</code> specifies:   <pre><code>terraform {\n  backend \"s3\" {\n    bucket         = \"my-terraform-state\"\n    key            = \"data-infra/dev/terraform.tfstate\"\n    region         = \"us-east-1\"\n    dynamodb_table = \"terraform-locks\"\n  }\n}\n</code></pre>   This isolates <code>dev</code> state from <code>prod</code>, preventing accidental cross-environment changes.</li> <li>Workspace Challenge: Workspaces store state files in the same backend with a workspace prefix (e.g., <code>env:/dev/terraform.tfstate</code>), which can lead to accidental state overwrites if the wrong workspace is selected. Additionally, all workspaces share the same backend configuration, limiting flexibility.</li> <li>Impact: Folder-based isolation enhances security and auditability by ensuring state files are distinctly managed, critical for compliance in data infrastructure projects.</li> </ul>"},{"location":"resources/Terraform_Data_Engineer_Cheatsheet/#3-simplified-cicd-pipelines","title":"3. Simplified CI/CD Pipelines","text":"<ul> <li>Advantage: Folder-based isolation aligns naturally with CI/CD pipelines, as each environment's directory can be targeted independently, reducing complexity in pipeline scripts.</li> <li>Example: The cheatsheet's <code>.gitlab-ci.yml</code> defines separate jobs (<code>plan_dev</code>, <code>apply_dev</code>, <code>plan_staging</code>, etc.) that operate on specific directories (e.g., <code>cd environments/dev</code>). This avoids the need to switch workspaces in the pipeline, simplifying configuration and reducing errors.   <pre><code>plan_dev:\n  stage: plan\n  script:\n    - cd environments/dev\n    - terraform init -backend-config=backend.tf\n    - terraform plan -var-file=terraform.tfvars -out=tfplan\n</code></pre></li> <li>Workspace Challenge: With workspaces, pipelines must run <code>terraform workspace select &lt;env&gt;</code> before each job, increasing the risk of selecting the wrong workspace or encountering state conflicts. This adds complexity to pipeline scripts and requires careful error handling.</li> <li>Impact: Folder-based isolation streamlines GitLab CI/CD pipelines, improving reliability and auditability for automated deployments of data infrastructure.</li> </ul>"},{"location":"resources/Terraform_Data_Engineer_Cheatsheet/#4-enhanced-team-collaboration","title":"4. Enhanced Team Collaboration","text":"<ul> <li>Advantage: Separate directories make it easier for teams to work on different environments simultaneously without conflicts, as each environment's configuration and state are isolated.</li> <li>Example: In the cheatsheet, a developer can modify <code>environments/dev/main.tf</code> to test a new Glue job while another team member updates <code>environments/prod/main.tf</code> to adjust RDS settings, without risking state or configuration clashes.</li> <li>Workspace Challenge: Workspaces share a single directory, so concurrent changes to <code>main.tf</code> or state files can lead to conflicts, especially in large teams. Developers must carefully coordinate workspace selection to avoid overwriting each other's changes.</li> <li>Impact: Folder-based isolation supports parallel development, critical for data engineering teams managing complex, multi-environment setups.</li> </ul>"},{"location":"resources/Terraform_Data_Engineer_Cheatsheet/#5-better-auditability-and-traceability","title":"5. Better Auditability and Traceability","text":"<ul> <li>Advantage: Folder-based isolation provides clear, environment-specific configuration files and state files, making it easier to audit changes and track infrastructure history.</li> <li>Example: The cheatsheet's structure allows auditors to review <code>environments/prod/terraform.tfvars</code> and <code>apply.log</code> (stored as artifacts) to verify production settings and changes. State backups in <code>s3://my-terraform-state/backups/prod/</code> provide a clear history for rollback or compliance.</li> <li>Workspace Challenge: Workspaces mix configurations in a single <code>main.tf</code>, requiring auditors to parse conditional logic or variable overrides to understand environment-specific settings. State files are less distinctly separated, complicating audit trails.</li> <li>Impact: Folder-based isolation simplifies compliance with regulatory requirements (e.g., GDPR, HIPAA) by providing transparent, environment-specific records.</li> </ul>"},{"location":"resources/Terraform_Data_Engineer_Cheatsheet/#6-flexibility-for-environment-specific-customization","title":"6. Flexibility for Environment-Specific Customization","text":"<ul> <li>Advantage: Each environment can have unique configurations, providers, or even Terraform versions without affecting others, offering maximum flexibility.</li> <li>Example: In the cheatsheet, <code>environments/staging/</code> might use a different AWS region or provider version than <code>environments/prod/</code>, defined in their respective <code>main.tf</code> and <code>backend.tf</code>. This is straightforward with separate directories.</li> <li>Workspace Challenge: Workspaces share the same provider configuration and Terraform version, limiting customization unless complex workarounds (e.g., provider aliases) are used.</li> <li>Impact: Folder-based isolation supports diverse data infrastructure requirements, such as regional differences or provider-specific settings for Snowflake or Databricks.</li> </ul>"},{"location":"resources/Terraform_Data_Engineer_Cheatsheet/#7-reduced-risk-of-human-error","title":"7. Reduced Risk of Human Error","text":"<ul> <li>Advantage: Operating within a specific directory (e.g., <code>environments/dev/</code>) eliminates the need to select a workspace, reducing the chance of applying changes to the wrong environment.</li> <li>Example: Running <code>terraform apply</code> in <code>environments/dev/</code> affects only the <code>dev</code> environment, with no risk of accidentally targeting <code>prod</code>. The cheatsheet's pipeline reinforces this by scoping jobs to directories.</li> <li>Workspace Challenge: Forgetting to run <code>terraform workspace select prod</code> before <code>terraform apply</code> can result in catastrophic changes to the wrong environment, a common error in high-pressure scenarios.</li> <li>Impact: Folder-based isolation enhances safety, particularly for production data infrastructure where errors can lead to data loss or downtime.</li> </ul>"},{"location":"resources/Terraform_Data_Engineer_Cheatsheet/#when-to-use-workspaces","title":"When to Use Workspaces","text":"<p>While folder-based isolation is generally preferred, workspaces may be suitable for:</p> <ul> <li>Simple Projects: Small projects with minimal configuration differences between environments (e.g., a single S3 bucket with different names).</li> <li>Temporary Environments: Spinning up ephemeral environments (e.g., feature branches) where maintaining separate directories is overkill.</li> <li>Legacy Projects: Existing setups already using workspaces, where migration to folders is not cost-effective.</li> </ul> <p>However, for data infrastructure projects with complex resources, multiple team members, and strict compliance requirements, folder-based isolation (as in the cheatsheet) is superior.</p>"},{"location":"resources/Terraform_Data_Engineer_Cheatsheet/#recommendations","title":"Recommendations","text":"<ul> <li>Adopt Folder-Based Isolation: Use the cheatsheet's <code>environments/</code> structure to organize <code>dev</code>, <code>staging</code>, and <code>prod</code> configurations in separate directories.</li> <li>Document in <code>README.md</code>: Include setup instructions for navigating environment directories and running Terraform commands, as shown in the cheatsheet:   <pre><code>## Running Terraform\n- Navigate to the environment directory: `cd environments/dev`\n- Initialize: `terraform init`\n- Apply: `terraform apply -var-file=terraform.tfvars`\n</code></pre></li> <li>Leverage CI/CD: Configure pipelines (e.g., <code>.gitlab-ci.yml</code>) to target specific directories, as in the cheatsheet, to enforce environment isolation.</li> <li>Secure State Files: Use distinct backend paths and versioning (e.g., <code>s3://my-terraform-state/data-infra/&lt;env&gt;/terraform.tfstate</code>) to protect state files, as shown in the cheatsheet's <code>backend.tf</code>.</li> </ul>"},{"location":"resources/Terraform_Scenarios/","title":"Terraform Scenarios","text":""},{"location":"resources/Terraform_Scenarios/#terraform-scenarios","title":"Terraform Scenarios","text":"<p>AI Assisted (Grok 3)</p> <p>Common Scenarios and how to handle them.</p>"},{"location":"resources/Terraform_Scenarios/#project-structure-reference","title":"Project Structure Reference","text":"<pre><code>data-infra/\n\u251c\u2500\u2500 modules/                     # Reusable Terraform modules (e.g., s3_bucket, rds, glue_job)\n\u251c\u2500\u2500 environments/                # Environment-specific configurations\n\u2502   \u251c\u2500\u2500 dev/\n\u2502   \u2502   \u251c\u2500\u2500 main.tf\n\u2502   \u2502   \u251c\u2500\u2500 variables.tf\n\u2502   \u2502   \u251c\u2500\u2500 outputs.tf\n\u2502   \u2502   \u251c\u2500\u2500 backend.tf\n\u2502   \u2502   \u2514\u2500\u2500 terraform.tfvars\n\u2502   \u251c\u2500\u2500 staging/\n\u2502   \u2514\u2500\u2500 prod/\n\u251c\u2500\u2500 scripts/\n\u251c\u2500\u2500 .gitignore\n\u251c\u2500\u2500 README.md\n\u2514\u2500\u2500 .gitlab-ci.yml               # CI/CD pipeline with validate, plan, and apply stages\n</code></pre>"},{"location":"resources/Terraform_Scenarios/#1-what-happens-if-your-state-file-is-accidentally-deleted","title":"1. What happens if your state file is accidentally deleted?","text":"<p>Answer: If the Terraform state file (<code>terraform.tfstate</code>) is deleted, Terraform loses track of managed infrastructure. The next <code>terraform apply</code> assumes resources don\u2019t exist and attempts to recreate them, potentially causing duplicates or failures. Recovery involves restoring a backup or manually importing resources with <code>terraform import</code>. Always enable versioning on remote state storage (e.g., S3).</p> <p>Example:</p> <ul> <li>Project Context: The project uses an S3 backend for state storage (<code>s3://my-terraform-state/data-infra/dev/terraform.tfstate</code>) with versioning enabled.</li> <li>Scenario: The <code>dev</code> state file is deleted.</li> <li>Recovery:</li> </ul> <p></p><pre><code># Restore from S3 versioned backup\naws s3api get-object --bucket my-terraform-state --key data-infra/dev/terraform.tfstate --version-id &lt;version-id&gt; terraform.tfstate\ncd environments/dev\nterraform init\nterraform apply\n</code></pre>   Alternatively, import an existing S3 bucket:   <pre><code>terraform import module.data_lake.aws_s3_bucket.bucket dev-data-lake\n</code></pre> - Best Practice: Enable S3 bucket versioning and backup state files before <code>apply</code> in the pipeline (see <code>apply_dev</code> job in <code>.gitlab-ci.yml</code>)<p></p>"},{"location":"resources/Terraform_Scenarios/#2-how-do-you-handle-large-scale-refactoring-without-downtime","title":"2. How do you handle large-scale refactoring without downtime?","text":"<p>Answer: For most resources, use <code>terraform state mv</code> to rename them in the state file without destruction. For S3 buckets, which have immutable names in AWS, create a new bucket, copy data, and update the state. Split refactoring into smaller, non-destructive pull requests (PRs), use targeted applies (<code>terraform apply -target</code>), and verify plans to prevent resource destruction. Test in <code>dev</code> before <code>staging</code> or <code>prod</code>.</p> <p>Example:</p> <ul> <li>Project Context: Refactor the <code>s3_bucket</code> module to rename the bucket from <code>dev-data-lake</code> to <code>dev-data-lake-v2</code> in <code>environments/dev/main.tf</code>. Since S3 bucket names are immutable, a new bucket is created, and data is copied.</li> <li> <p>Step-by-Step Process:</p> </li> <li> <p>Add New Bucket Module:</p> <ul> <li>Update <code>environments/dev/main.tf</code> to define a temporary module (<code>data_lake_v2</code>) alongside <code>data_lake</code>.    <pre><code># environments/dev/main.tf\nmodule \"data_lake\" {\n  source      = \"../../modules/s3_bucket\"\n  bucket_name = \"data-lake\"  # Original: dev-data-lake\n  environment = var.environment\n}\n\nmodule \"data_lake_v2\" {\n  source      = \"../../modules/s3_bucket\"\n  bucket_name = \"data-lake-v2\"  # New: dev-data-lake-v2\n  environment = var.environment\n}\n</code></pre></li> <li>Run:    <pre><code>cd environments/dev\nterraform init\nterraform apply -var-file=terraform.tfvars\n</code></pre></li> <li>Outcome: Creates <code>dev-data-lake-v2</code>. <code>dev-data-lake</code> remains active, ensuring no downtime.</li> </ul> </li> <li> <p>Copy Data:</p> <ul> <li>Sync data from <code>dev-data-lake</code> to <code>dev-data-lake-v2</code>.    <pre><code>aws s3 sync s3://dev-data-lake s3://dev-data-lake-v2\n</code></pre></li> <li>Outcome: <code>dev-data-lake-v2</code> contains all data. Services using <code>dev-data-lake</code> are unaffected.</li> </ul> </li> <li> <p>Update State:</p> <ul> <li>Move the state entry for <code>dev-data-lake-v2</code> to replace <code>dev-data-lake</code>.    <pre><code>terraform state mv module.data_lake.aws_s3_bucket.bucket module.data_lake_v2.aws_s3_bucket.bucket\n</code></pre></li> <li>Outcome: State now maps <code>module.data_lake_v2.aws_s3_bucket.bucket</code> to <code>dev-data-lake-v2</code>.</li> </ul> </li> <li> <p>Remove Old Bucket from State:</p> <ul> <li>Remove <code>dev-data-lake</code> from state.    <pre><code>terraform state rm module.data_lake.aws_s3_bucket.bucket\n</code></pre></li> <li>Outcome: Terraform no longer manages <code>dev-data-lake</code>, which remains in AWS.</li> </ul> </li> <li> <p>Update Configuration:</p> <ul> <li>Revise <code>main.tf</code> to use only the new bucket.    <pre><code># environments/dev/main.tf\nmodule \"data_lake\" {\n  source      = \"../../modules/s3_bucket\"\n  bucket_name = \"data-lake-v2\"\n  environment = var.environment\n}\n</code></pre></li> <li>Run:    <pre><code>terraform plan -var-file=terraform.tfvars  # Verify no destroy\nterraform apply -var-file=terraform.tfvars\n</code></pre></li> <li>Outcome: Terraform manages <code>dev-data-lake-v2</code> under <code>module.data_lake</code>.</li> </ul> </li> <li> <p>Update Dependencies:</p> <ul> <li>Modify dependent resources (e.g., Glue jobs) to reference <code>dev-data-lake-v2</code>.    <pre><code># modules/glue_job/main.tf\ndata \"aws_s3_bucket\" \"bucket\" {\n  bucket = \"${var.environment}-data-lake-v2\"\n}\n</code></pre></li> <li>Apply changes in a separate PR.</li> <li>Outcome: Services transition to <code>dev-data-lake-v2</code> without disruption.</li> </ul> </li> <li> <p>Delete Old Bucket (Optional):</p> <ul> <li>If safe, delete <code>dev-data-lake</code>.    <pre><code>aws s3 rb s3://dev-data-lake --force\n</code></pre></li> <li>Ensure <code>force_destroy = false</code> in <code>modules/s3_bucket/main.tf</code> to prevent accidental deletion:    <pre><code>resource \"aws_s3_bucket\" \"bucket\" {\n  bucket        = \"${var.environment}-${var.bucket_name}\"\n  force_destroy = false\n}\n</code></pre></li> <li>Outcome: Old bucket is removed after confirmation.</li> </ul> </li> <li> <p>Best Practice: Test in <code>dev</code> using the <code>plan_dev</code> job, automate data sync in <code>apply_dev</code>, document in PRs, and ensure <code>force_destroy = false</code>. Update <code>README.md</code>:</p> </li> </ul> <pre><code>## S3 Bucket Renaming\n- Add new module in `main.tf`.\n- Sync data: `aws s3 sync s3://dev-data-lake s3://dev-data-lake-v2`.\n- Update state: `terraform state mv`.\n- Remove old module and state entry.\n- Delete old bucket if safe.\n</code></pre>"},{"location":"resources/Terraform_Scenarios/#3-what-happens-if-a-resource-fails-halfway-through-a-terraform-apply","title":"3. What happens if a resource fails halfway through a terraform apply?","text":"<p>Answer: If a resource fails during <code>terraform apply</code>, Terraform creates a partial deployment. Successful resources are applied, but failed ones are marked as tainted in the state file. Use targeted applies (<code>terraform apply -target</code>) or <code>-refresh-only</code> to recover systematically, addressing failures one by one.</p> <p>Example:</p> <ul> <li>Project Context: The <code>apply_dev</code> job fails when creating an RDS instance due to an invalid parameter.</li> <li>Scenario: The <code>module.rds_postgres.aws_db_instance.rds_instance</code> fails, but the <code>module.data_lake.aws_s3_bucket.bucket</code> is created.</li> <li>Recovery:</li> </ul> <p></p><pre><code>cd environments/dev\nterraform init\nterraform plan -var-file=terraform.tfvars  # Check tainted resources\nterraform apply -target=module.rds_postgres.aws_db_instance.rds_instance  # Retry specific resource\n</code></pre>   Review <code>apply.log</code> artifact from the <code>apply_dev</code> job in GitLab to diagnose the error.<p></p> <ul> <li>Best Practice: Use the pipeline\u2019s <code>apply.log</code> (stored in S3: <code>s3://my-audit-logs/terraform-apply/dev/</code>) for debugging and target specific resources to minimize disruption.</li> </ul>"},{"location":"resources/Terraform_Scenarios/#4-how-do-you-manage-secrets-in-terraform","title":"4. How do you manage secrets in Terraform?","text":"<p>Answer: Store secrets in external systems like AWS Secrets Manager or HashiCorp Vault, use encrypted remote state, mark outputs as sensitive, and integrate with CI/CD securely. Avoid hardcoding secrets in <code>.tfvars</code> or code, and consider managing highly sensitive values outside Terraform.</p> <p>Example:</p> <ul> <li>Project Context: The RDS module in <code>environments/dev/main.tf</code> requires a <code>db_password</code>.</li> <li>Implementation:</li> </ul> <p></p><pre><code># environments/dev/main.tf\ndata \"aws_secretsmanager_secret_version\" \"db_password\" {\n  secret_id = \"dev/rds/password\"\n}\n\nmodule \"rds_postgres\" {\n  source             = \"../../modules/rds\"\n  db_name            = \"datawarehouse\"\n  environment        = var.environment\n  db_username        = var.db_username\n  db_password        = data.aws_secretsmanager_secret_version.db_password.secret_string\n  subnet_ids         = data.aws_subnet_ids.default.ids\n  security_group_ids = [aws_security_group.rds_sg.id]\n}\n\n# outputs.tf\noutput \"rds_endpoint\" {\n  value     = module.rds_postgres.aws_db_instance.rds_instance.endpoint\n  sensitive = true\n}\n</code></pre>   Store <code>AWS_ACCESS_KEY_ID</code> and <code>AWS_SECRET_ACCESS_KEY</code> in GitLab CI/CD variables for pipeline authentication.<p></p> <ul> <li>Best Practice: Use the <code>backend.tf</code> S3 backend with encryption and restrict access via IAM policies. Avoid storing secrets in <code>terraform.tfvars</code>.</li> </ul>"},{"location":"resources/Terraform_Scenarios/#5-what-happens-if-terraform-plan-shows-no-changes-but-infrastructure-was-modified-outside-terraform","title":"5. What happens if terraform plan shows no changes but infrastructure was modified outside Terraform?","text":"<p>Answer: Terraform is unaware of external changes (state drift) until <code>terraform refresh</code> updates the state file. Implement regular drift detection in CI/CD pipelines to catch unauthorized modifications and reconcile them with <code>terraform apply</code> or <code>terraform import</code>.</p> <p>Example:</p> <ul> <li>Project Context: An S3 bucket (<code>dev-data-lake</code>) is manually modified in the AWS Console to change its ACL.</li> <li>Detection:</li> </ul> <p></p><pre><code># .gitlab-ci.yml\ndrift_detection:\n  stage: validate\n  extends: .terraform_base\n  script:\n    - cd environments/dev\n    - terraform init -backend-config=backend.tf\n    - terraform refresh -var-file=terraform.tfvars\n    - terraform plan -var-file=terraform.tfvars -out=tfplan\n  artifacts:\n    paths:\n      - environments/dev/tfplan.txt\n    expire_in: 1 week\n</code></pre> <pre><code>cd environments/dev\nterraform init\nterraform refresh -var-file=terraform.tfvars\nterraform plan -var-file=terraform.tfvars  # Shows drift\nterraform apply -var-file=terraform.tfvars  # Reconcile changes\n</code></pre><p></p> <ul> <li>Best Practice: Schedule a <code>drift_detection</code> job weekly in <code>.gitlab-ci.yml</code> and review <code>tfplan.txt</code> artifacts to identify drift.</li> </ul>"},{"location":"resources/Terraform_Scenarios/#6-what-happens-if-you-delete-a-resource-definition-from-your-configuration","title":"6. What happens if you delete a resource definition from your configuration?","text":"<p>Answer: Removing a resource from Terraform configuration causes <code>terraform apply</code> to destroy the corresponding infrastructure. Use <code>terraform state rm</code> to remove the resource from state without destroying it, or add <code>lifecycle { prevent_destroy = true }</code> for critical resources.</p> <p>Example:</p> <ul> <li>Project Context: The <code>module.data_lake.aws_s3_bucket.bucket</code> is removed from <code>environments/prod/main.tf</code>.</li> <li>Prevention:</li> </ul> <p></p><pre><code># modules/s3_bucket/main.tf\nresource \"aws_s3_bucket\" \"bucket\" {\n  bucket = \"${var.environment}-${var.bucket_name}\"\n  lifecycle {\n    prevent_destroy = true\n  }\n}\n</code></pre>   If removed accidentally:   <pre><code>cd environments/prod\nterraform state rm module.data_lake.aws_s3_bucket.bucket\nterraform plan -var-file=terraform.tfvars  # Verify no destroy\n</code></pre><p></p> <ul> <li>Best Practice: Apply <code>prevent_destroy</code> to critical resources like production S3 buckets or RDS instances in the <code>modules/</code> directory.</li> </ul>"},{"location":"resources/Terraform_Scenarios/#7-what-happens-if-terraform-provider-apis-change-between-versions","title":"7. What happens if Terraform provider APIs change between versions?","text":"<p>Answer: Provider API changes can break compatibility, causing errors in resource creation or updates. Read release notes, pin provider versions, test upgrades in <code>dev</code>, and use targeted applies for gradual migration.</p> <p>Example:</p> <ul> <li>Project Context: Upgrading the AWS provider from <code>~&gt; 4.0</code> to <code>~&gt; 5.0</code> in <code>environments/dev/main.tf</code>.</li> <li>Implementation:</li> </ul> <p></p><pre><code># environments/dev/main.tf\nterraform {\n  required_providers {\n    aws = {\n      source  = \"hashicorp/aws\"\n      version = \"~&gt; 5.0\"  # Upgraded from 4.0\n    }\n  }\n}\n</code></pre> <pre><code>cd environments/dev\nterraform init -upgrade\nterraform plan -var-file=terraform.tfvars  # Check for breaking changes\nterraform apply -var-file=terraform.tfvars\n</code></pre><p></p> <ul> <li>Best Practice: Test upgrades in the <code>plan_dev</code> job, review release notes (e.g., AWS provider changelog), and update one environment at a time.</li> </ul>"},{"location":"resources/Terraform_Scenarios/#8-how-do-you-implement-zero-downtime-infrastructure-updates","title":"8. How do you implement zero-downtime infrastructure updates?","text":"<p>Answer: Use <code>create_before_destroy</code> lifecycle blocks, blue-green deployments, health checks, and state manipulation. For databases, leverage replicas or managed services with failover capabilities to avoid downtime.</p> <p>Example:</p> <ul> <li>Project Context: Update the RDS instance class in <code>environments/prod/main.tf</code> without downtime.</li> <li>Implementation:</li> </ul> <p></p><pre><code># modules/rds/main.tf\nresource \"aws_db_instance\" \"rds_instance\" {\n  identifier           = \"${var.environment}-${var.db_name}\"\n  instance_class       = var.prod_instance_class\n  allocated_storage    = var.allocated_storage\n  multi_az             = true  # Enable for failover\n  apply_immediately    = false # Apply during maintenance window\n  lifecycle {\n    create_before_destroy = true\n  }\n}\n</code></pre> <pre><code>cd environments/prod\nterraform init\nterraform plan -var-file=terraform.tfvars -out=tfplan\nterraform apply tfplan\n</code></pre><p></p> <ul> <li>Best Practice: Enable <code>multi_az</code> for production RDS (as in the cheatsheet), use <code>apply_immediately = false</code>, and test in <code>staging</code> via the <code>apply_staging</code> job.</li> </ul>"},{"location":"resources/Terraform_Scenarios/#9-what-happens-if-you-have-circular-dependencies-in-your-terraform-modules","title":"9. What happens if you have circular dependencies in your Terraform modules?","text":"<p>Answer: Circular dependencies cause Terraform to fail with \"dependency cycle\" errors. Refactor modules using data sources, outputs, or restructured resources to establish a clear dependency hierarchy.</p> <p>Example:</p> <ul> <li>Project Context: The <code>s3_bucket</code> module depends on a <code>glue_job</code> module, which references the S3 bucket\u2019s ARN.</li> <li>Resolution:</li> </ul> <pre><code># modules/s3_bucket/main.tf\nresource \"aws_s3_bucket\" \"bucket\" {\n  bucket = \"${var.environment}-${var.bucket_name}\"\n}\n\noutput \"bucket_arn\" {\n  value = aws_s3_bucket.bucket.arn\n}\n\n# modules/glue_job/main.tf\ndata \"aws_s3_bucket\" \"bucket\" {\n  bucket = \"${var.environment}-${var.bucket_name}\"\n}\n\nresource \"aws_glue_job\" \"pyspark_job\" {\n  name     = \"${var.environment}-${var.job_name}\"\n  role_arn = var.glue_role_arn\n  command {\n    script_location = \"s3://${data.aws_s3_bucket.bucket.bucket}/${var.script_path}\"\n  }\n}\n</code></pre> <ul> <li>Best Practice: Use data sources to fetch existing resources, avoiding direct dependencies. Validate with the <code>validate</code> job in <code>.gitlab-ci.yml</code>.</li> </ul>"},{"location":"resources/Terraform_Scenarios/#10-what-happens-if-you-rename-a-resource-in-your-terraform-code","title":"10. What happens if you rename a resource in your Terraform code?","text":"<p>Answer: Renaming a resource in Terraform code is interpreted as destroying and recreating the resource. Use <code>terraform state mv</code> to update the state file, preserving the existing infrastructure and avoiding rebuilds or downtime. For S3 buckets, create a new bucket and copy data, as bucket names are immutable.</p> <p>Example:</p> <ul> <li>Project Context: Rename <code>module.data_lake.aws_s3_bucket.bucket</code> to <code>module.data_lake.aws_s3_bucket.data_lake</code> in <code>environments/dev/main.tf</code>.</li> <li>Steps:</li> </ul> <p></p><pre><code># Original: environments/dev/main.tf\nmodule \"data_lake\" {\n  source      = \"../../modules/s3_bucket\"\n  bucket_name = \"data-lake\"\n  environment = var.environment\n}\n</code></pre> <pre><code># Updated: modules/s3_bucket/main.tf\nresource \"aws_s3_bucket\" \"data_lake\" {  # Renamed from bucket\n  bucket = \"${var.environment}-${var.bucket_name}\"\n}\n</code></pre> <pre><code>cd environments/dev\nterraform init\nterraform state mv module.data_lake.aws_s3_bucket.bucket module.data_lake.aws_s3_bucket.data_lake\nterraform plan -var-file=terraform.tfvars  # Verify no destroy\nterraform apply -var-file=terraform.tfvars\n</code></pre><p></p> <ul> <li>Best Practice: Run <code>terraform state mv</code> in the <code>plan_dev</code> job to preview changes, and document renaming in PRs for team review.</li> </ul>"},{"location":"resources/Terraform_Scenarios/#additional-notes","title":"Additional Notes","text":"<ul> <li>Pipeline Integration: Use the <code>.gitlab-ci.yml</code> from the main cheatsheet to automate validation, planning, and applying changes, ensuring safe handling of state, secrets, and drift detection.</li> <li>Documentation: Update <code>README.md</code> to include recovery steps for each scenario (e.g., state restoration, drift detection setup).</li> </ul> <pre><code>## Handling Terraform Scenarios\n- State Deletion: Restore from `s3://my-terraform-state/backups/&lt;env&gt;/&lt;timestamp&gt;.tfstate`.\n- S3 Bucket Renaming: See Scenario 2 for detailed steps.\n- Drift Detection: Run the `drift_detection` job in GitLab CI/CD.\n- Zero-Downtime Updates: Enable `create_before_destroy` and `multi_az` for RDS in `modules/rds/main.tf`.\n</code></pre> <ul> <li>Testing: Test all changes in <code>dev</code> or <code>staging</code> environments using the <code>plan_dev</code> and <code>apply_dev</code> jobs before applying to <code>prod</code>.</li> </ul>"},{"location":"resources/asyncio_deep_dive/","title":"Asyncio Deep Dive: Mastering Asynchronous Python","text":""},{"location":"resources/asyncio_deep_dive/#asyncio-deep-dive-mastering-asynchronous-python","title":"Asyncio Deep Dive: Mastering Asynchronous Python","text":"<p>AI Assisted (Claude Sonnet 4)</p>"},{"location":"resources/asyncio_deep_dive/#the-fundamental-problem-why-asyncio-exists","title":"The Fundamental Problem: Why Asyncio Exists","text":"<p>Traditional Python code executes synchronously - one operation at a time, waiting for each to complete before starting the next. This creates a critical inefficiency problem:</p> <pre><code>import time\nimport requests\n\ndef sequential_downloads():\n    \"\"\"Traditional synchronous approach\"\"\"\n    print(\"\ud83d\udc0c Starting sequential downloads...\")\n    start = time.time()\n\n    # Each request blocks until complete\n    response1 = requests.get(\"https://httpbin.org/delay/2\")  # Wait 2 seconds\n    print(f\"\u2705 First download complete\")\n\n    response2 = requests.get(\"https://httpbin.org/delay/2\")  # Wait another 2 seconds  \n    print(f\"\u2705 Second download complete\")\n\n    response3 = requests.get(\"https://httpbin.org/delay/2\")  # Wait another 2 seconds\n    print(f\"\u2705 Third download complete\")\n\n    total_time = time.time() - start\n    print(f\"\u23f1\ufe0f  Total time: {total_time:.2f} seconds\")  # ~6 seconds\n\n    return [response1, response2, response3]\n\n# sequential_downloads()  # Uncomment to see the slow approach\n</code></pre> <p>The inefficiency: While waiting for the first request, your CPU sits idle even though it could be starting the second and third requests simultaneously.</p> <p>Asyncio's solution: Enable cooperative multitasking where your program can juggle multiple operations, switching between them while waiting for I/O:</p> <pre><code>import asyncio\nimport aiohttp\nimport time\n\nasync def concurrent_downloads():\n    \"\"\"Asyncio's concurrent approach\"\"\"\n    print(\"\ud83d\ude80 Starting concurrent downloads...\")\n    start = time.time()\n\n    async with aiohttp.ClientSession() as session:\n        # Start all three requests simultaneously\n        tasks = [\n            session.get(\"https://httpbin.org/delay/2\"),\n            session.get(\"https://httpbin.org/delay/2\"), \n            session.get(\"https://httpbin.org/delay/2\")\n        ]\n\n        # Wait for all to complete\n        responses = await asyncio.gather(*tasks)\n\n        # Process responses\n        for i, response in enumerate(responses, 1):\n            print(f\"\u2705 Download {i} complete\")\n            response.close()\n\n    total_time = time.time() - start\n    print(f\"\u23f1\ufe0f  Total time: {total_time:.2f} seconds\")  # ~2 seconds!\n\n    return responses\n\nasyncio.run(concurrent_downloads())\n</code></pre> <p>Key insight: Asyncio doesn't make individual operations faster - it eliminates wasted waiting time by running operations concurrently.</p>"},{"location":"resources/asyncio_deep_dive/#the-event-loop-the-heart-of-asyncio","title":"The Event Loop: The Heart of Asyncio","text":"<p>The event loop is asyncio's central coordinator - a single-threaded mechanism that manages and executes asynchronous tasks. Think of it as a highly efficient restaurant manager:</p> <pre><code>import asyncio\nimport random\nimport time\n\nasync def restaurant_simulation():\n    \"\"\"Demonstrates how the event loop coordinates multiple operations\"\"\"\n\n    async def serve_customer(customer_id, service_time):\n        print(f\"\ud83c\udf7d\ufe0f  Customer {customer_id}: Order taken\")\n\n        # This represents I/O waiting (kitchen preparing food)\n        # The event loop can serve other customers during this time\n        await asyncio.sleep(service_time)\n\n        print(f\"\u2705 Customer {customer_id}: Order ready! ({service_time}s)\")\n        return f\"Meal for customer {customer_id}\"\n\n    print(\"\ud83c\udfea Restaurant opens - Multiple customers arrive\")\n\n    # Customers with different service times\n    customers = [\n        serve_customer(1, 3.0),  # Slow order\n        serve_customer(2, 1.0),  # Fast order\n        serve_customer(3, 2.0),  # Medium order\n        serve_customer(4, 1.5),  # Medium-fast order\n    ]\n\n    # Event loop serves all customers concurrently\n    start_time = time.time()\n    meals = await asyncio.gather(*customers)\n    total_time = time.time() - start_time\n\n    print(f\"\ud83c\udfea All customers served in {total_time:.1f}s\")\n    print(\"Notice: Faster orders completed first, regardless of arrival order!\")\n\n    return meals\n\nasyncio.run(restaurant_simulation())\n</code></pre> <p>Event Loop Mechanics:</p> <p>The event loop operates on a simple but powerful principle:</p> <ol> <li>Run ready tasks until they hit an <code>await</code> (suspension point)</li> <li>Track waiting tasks and when they'll be ready to resume</li> <li>Resume tasks as their wait conditions are met</li> <li>Repeat until all tasks complete</li> </ol> <p>Here's a simplified view of what happens internally:</p> <pre><code># Conceptual view - don't run this code\nclass ConceptualEventLoop:\n    def __init__(self):\n        self.ready_tasks = []       # Tasks ready to run now\n        self.waiting_tasks = []     # Tasks waiting for I/O or timers\n        self.time_callbacks = []    # Scheduled callbacks\n\n    def run_until_complete(self, main_task):\n        \"\"\"Main event loop execution\"\"\"\n        self.ready_tasks.append(main_task)\n\n        while self.ready_tasks or self.waiting_tasks:\n            # Execute all ready tasks\n            while self.ready_tasks:\n                task = self.ready_tasks.pop(0)\n                try:\n                    # Run task until it hits 'await'\n                    task.step()\n                except TaskComplete:\n                    continue  # Task finished\n                except TaskSuspended as suspend_info:\n                    # Task is waiting for something\n                    self.waiting_tasks.append((task, suspend_info))\n\n            # Check which waiting tasks are now ready\n            self.check_waiting_tasks()\n\n            # Handle timer callbacks\n            self.handle_timer_callbacks()\n\n    def check_waiting_tasks(self):\n        \"\"\"Move completed waits back to ready queue\"\"\"\n        still_waiting = []\n        for task, wait_info in self.waiting_tasks:\n            if wait_info.is_ready():\n                self.ready_tasks.append(task)\n            else:\n                still_waiting.append((task, wait_info))\n        self.waiting_tasks = still_waiting\n</code></pre>"},{"location":"resources/asyncio_deep_dive/#coroutines-the-building-blocks","title":"Coroutines: The Building Blocks","text":"<p>Coroutines are special functions that can be suspended and resumed. They're the fundamental units of work in asyncio.</p>"},{"location":"resources/asyncio_deep_dive/#creating-and-understanding-coroutines","title":"Creating and Understanding Coroutines","text":"<pre><code>import asyncio\n\n# Regular function - runs to completion immediately\ndef regular_function(name):\n    print(f\"Regular function {name} starting\")\n    return f\"Result from {name}\"\n\n# Coroutine function - can be suspended and resumed\nasync def coroutine_function(name):\n    print(f\"Coroutine {name} starting\")\n\n    # This is a suspension point - execution can pause here\n    await asyncio.sleep(1)\n\n    print(f\"Coroutine {name} resuming after 1 second\")\n    return f\"Async result from {name}\"\n\nasync def demonstrate_coroutine_lifecycle():\n    print(\"=== Regular Function ===\")\n    result = regular_function(\"RegularFunc\")\n    print(f\"Result: {result}\")\n\n    print(\"\\n=== Coroutine Function ===\")\n\n    # Calling a coroutine function doesn't run it!\n    coro = coroutine_function(\"CoroFunc\")\n    print(f\"Coroutine object created: {type(coro)}\")\n\n    # You must await it to actually run it\n    result = await coro\n    print(f\"Result: {result}\")\n\n    print(\"\\n=== Multiple Coroutines Concurrently ===\")\n    # Multiple coroutines can run together\n    results = await asyncio.gather(\n        coroutine_function(\"Coro1\"),\n        coroutine_function(\"Coro2\"), \n        coroutine_function(\"Coro3\")\n    )\n    print(f\"All results: {results}\")\n\nasyncio.run(demonstrate_coroutine_lifecycle())\n</code></pre>"},{"location":"resources/asyncio_deep_dive/#coroutine-states-and-execution-flow","title":"Coroutine States and Execution Flow","text":"<pre><code>import asyncio\nimport inspect\n\nasync def trace_coroutine_execution():\n    \"\"\"Demonstrates the different states a coroutine goes through\"\"\"\n\n    async def traced_operation(operation_id):\n        print(f\"\ud83d\ude80 Operation {operation_id}: Starting\")\n\n        print(f\"\u23f8\ufe0f  Operation {operation_id}: About to suspend (await)\")\n        await asyncio.sleep(1)  # Suspension point\n\n        print(f\"\u25b6\ufe0f  Operation {operation_id}: Resumed after await\")\n\n        print(f\"\u23f8\ufe0f  Operation {operation_id}: Another suspension point\")\n        await asyncio.sleep(0.5)\n\n        print(f\"\u2705 Operation {operation_id}: Completed\")\n        return f\"Result-{operation_id}\"\n\n    print(\"Creating coroutines (not running yet)...\")\n    coro1 = traced_operation(\"A\")\n    coro2 = traced_operation(\"B\")\n\n    print(f\"Coroutine states: {inspect.getcoroutinestate(coro1)}\")\n\n    print(\"\\nStarting concurrent execution...\")\n    results = await asyncio.gather(coro1, coro2)\n\n    print(f\"\\nFinal results: {results}\")\n\nasyncio.run(trace_coroutine_execution())\n</code></pre>"},{"location":"resources/asyncio_deep_dive/#understanding-await-the-suspension-mechanism","title":"Understanding await: The Suspension Mechanism","text":"<p>The <code>await</code> keyword is where the magic happens. It's not just syntax - it's the mechanism that allows cooperative multitasking.</p>"},{"location":"resources/asyncio_deep_dive/#what-await-actually-does","title":"What await Actually Does","text":"<pre><code>import asyncio\nimport time\n\nasync def demonstrate_await_behavior():\n    \"\"\"Shows exactly what happens at await points\"\"\"\n\n    print(\"\ud83c\udfac Starting demonstration\")\n\n    print(\"\ud83d\udccd Point 1: Before first await\")\n    start_time = time.time()\n\n    # This suspends the coroutine and yields control to the event loop\n    await asyncio.sleep(1)  \n\n    print(f\"\ud83d\udccd Point 2: After first await ({time.time() - start_time:.1f}s elapsed)\")\n\n    # Another suspension point\n    await asyncio.sleep(0.5)\n\n    print(f\"\ud83d\udccd Point 3: After second await ({time.time() - start_time:.1f}s elapsed)\")\n\n    return \"Demo complete\"\n\nasync def demonstrate_concurrent_awaits():\n    \"\"\"Shows how multiple awaits can run concurrently\"\"\"\n\n    async def worker(worker_id, work_duration):\n        print(f\"\ud83d\udc77 Worker {worker_id}: Starting work ({work_duration}s)\")\n        await asyncio.sleep(work_duration)\n        print(f\"\u2705 Worker {worker_id}: Work complete\")\n        return f\"Work result from {worker_id}\"\n\n    print(\"\ud83c\udfed Starting workers concurrently...\")\n    start_time = time.time()\n\n    # All workers start at the same time\n    results = await asyncio.gather(\n        worker(\"Alpha\", 2.0),\n        worker(\"Beta\", 1.0),\n        worker(\"Gamma\", 1.5)\n    )\n\n    total_time = time.time() - start_time\n    print(f\"\ud83c\udfc1 All work completed in {total_time:.1f}s\")\n    print(f\"\ud83d\udcca Results: {results}\")\n\nasync def main():\n    await demonstrate_await_behavior()\n    print(\"\\n\" + \"=\"*50 + \"\\n\")\n    await demonstrate_concurrent_awaits()\n\nasyncio.run(main())\n</code></pre>"},{"location":"resources/asyncio_deep_dive/#await-rules-and-awaitables","title":"await Rules and Awaitables","text":"<p>Not everything can be awaited. Only awaitable objects work with <code>await</code>:</p> <pre><code>import asyncio\n\nclass CustomAwaitable:\n    \"\"\"Example of creating a custom awaitable object\"\"\"\n\n    def __init__(self, value, delay):\n        self.value = value\n        self.delay = delay\n\n    def __await__(self):\n        \"\"\"This makes the object awaitable\"\"\"\n        # Yield control to event loop for the specified delay\n        yield from asyncio.sleep(self.delay).__await__()\n        # Return the final value\n        return f\"Custom result: {self.value}\"\n\nasync def demonstrate_awaitables():\n    print(\"=== Built-in Awaitables ===\")\n\n    # Coroutines are awaitable\n    async def simple_coro():\n        await asyncio.sleep(0.1)\n        return \"Coroutine result\"\n\n    result1 = await simple_coro()\n    print(f\"Coroutine result: {result1}\")\n\n    # Tasks are awaitable\n    task = asyncio.create_task(simple_coro())\n    result2 = await task\n    print(f\"Task result: {result2}\")\n\n    # Futures are awaitable\n    future = asyncio.Future()\n    future.set_result(\"Future result\")\n    result3 = await future\n    print(f\"Future result: {result3}\")\n\n    print(\"\\n=== Custom Awaitable ===\")\n    custom = CustomAwaitable(\"Hello World\", 0.5)\n    result4 = await custom\n    print(f\"Custom awaitable result: {result4}\")\n\n    print(\"\\n=== What's NOT Awaitable ===\")\n    try:\n        # This will fail - regular functions aren't awaitable\n        # await time.sleep(1)  # \u274c TypeError\n        pass\n    except TypeError as e:\n        print(f\"Error: {e}\")\n\nasyncio.run(demonstrate_awaitables())\n</code></pre>"},{"location":"resources/asyncio_deep_dive/#tasks-concurrent-execution-units","title":"Tasks: Concurrent Execution Units","text":"<p>Tasks are the primary mechanism for running coroutines concurrently. They wrap coroutines and manage their execution within the event loop.</p>"},{"location":"resources/asyncio_deep_dive/#understanding-tasks-vs-coroutines","title":"Understanding Tasks vs Coroutines","text":"<pre><code>import asyncio\nimport time\n\nasync def demonstrate_tasks_vs_coroutines():\n    \"\"\"Shows the critical difference between creating coroutines and tasks\"\"\"\n\n    async def background_work(work_id, duration):\n        print(f\"\ud83d\udd27 Work {work_id}: Starting ({duration}s)\")\n        for i in range(int(duration)):\n            await asyncio.sleep(1)\n            print(f\"\u2699\ufe0f  Work {work_id}: Progress {i+1}/{int(duration)}\")\n        print(f\"\u2705 Work {work_id}: Complete\")\n        return f\"Result-{work_id}\"\n\n    print(\"=== Coroutines: Don't Start Until Awaited ===\")\n    start_time = time.time()\n\n    # Creating coroutines doesn't start them\n    coro1 = background_work(\"Coro-A\", 2)\n    coro2 = background_work(\"Coro-B\", 2)\n\n    print(f\"\u23f1\ufe0f  Coroutines created at {time.time() - start_time:.1f}s\")\n    await asyncio.sleep(1)  # Wait 1 second\n    print(f\"\u23f1\ufe0f  1 second later - no work has started yet\")\n\n    # Now run them sequentially\n    result1 = await coro1  # Runs for 2 seconds\n    result2 = await coro2  # Runs for another 2 seconds\n\n    coro_time = time.time() - start_time\n    print(f\"\u23f1\ufe0f  Coroutines completed in {coro_time:.1f}s (sequential)\")\n\n    print(\"\\n=== Tasks: Start Immediately ===\")\n    start_time = time.time()\n\n    # Creating tasks starts them immediately!\n    task1 = asyncio.create_task(background_work(\"Task-A\", 2))\n    task2 = asyncio.create_task(background_work(\"Task-B\", 2))\n\n    print(f\"\u23f1\ufe0f  Tasks created at {time.time() - start_time:.1f}s\")\n    await asyncio.sleep(1)  # Wait 1 second\n    print(f\"\u23f1\ufe0f  1 second later - work is already running!\")\n\n    # Wait for both to complete (they run concurrently)\n    results = await asyncio.gather(task1, task2)\n\n    task_time = time.time() - start_time\n    print(f\"\u23f1\ufe0f  Tasks completed in {task_time:.1f}s (concurrent)\")\n\n    print(f\"\\n\ud83d\udcca Time difference: {coro_time - task_time:.1f}s saved with tasks!\")\n\nasyncio.run(demonstrate_tasks_vs_coroutines())\n</code></pre>"},{"location":"resources/asyncio_deep_dive/#task-management-and-control","title":"Task Management and Control","text":"<pre><code>import asyncio\nimport random\n\nasync def demonstrate_task_management():\n    \"\"\"Shows how to manage, monitor, and control tasks\"\"\"\n\n    async def worker_task(worker_id):\n        \"\"\"A worker that does some work and might fail\"\"\"\n        try:\n            work_time = random.uniform(1, 4)\n            print(f\"\ud83d\udc77 Worker {worker_id}: Starting {work_time:.1f}s of work\")\n\n            # Simulate work with progress updates\n            for i in range(int(work_time)):\n                await asyncio.sleep(1)\n                print(f\"\u2699\ufe0f  Worker {worker_id}: {i+1}/{int(work_time)} seconds complete\")\n\n            # Simulate random failures\n            if random.random() &lt; 0.3:  # 30% chance of failure\n                raise Exception(f\"Worker {worker_id} encountered an error!\")\n\n            print(f\"\u2705 Worker {worker_id}: Work completed successfully\")\n            return f\"Success-{worker_id}\"\n\n        except asyncio.CancelledError:\n            print(f\"\ud83d\uded1 Worker {worker_id}: Cancelled during work\")\n            raise  # Re-raise to complete cancellation\n\n    print(\"\ud83c\udfed Starting worker management demo\")\n\n    # Create multiple tasks\n    tasks = [\n        asyncio.create_task(worker_task(f\"W{i}\"))\n        for i in range(1, 6)\n    ]\n\n    # Monitor task progress\n    async def monitor_tasks():\n        while True:\n            await asyncio.sleep(0.5)\n\n            pending = [t for t in tasks if not t.done()]\n            completed = [t for t in tasks if t.done() and not t.cancelled()]\n            cancelled = [t for t in tasks if t.cancelled()]\n            failed = [t for t in tasks if t.done() and t.exception()]\n\n            print(f\"\ud83d\udcca Status: {len(pending)} pending, {len(completed)} completed, \"\n                  f\"{len(cancelled)} cancelled, {len(failed)} failed\")\n\n            if not pending:\n                break\n\n    monitor_task = asyncio.create_task(monitor_tasks())\n\n    try:\n        # Let tasks run for a bit\n        await asyncio.sleep(2)\n\n        # Cancel any remaining tasks that are taking too long\n        for task in tasks:\n            if not task.done():\n                print(f\"\u23f0 Cancelling slow task...\")\n                task.cancel()\n\n        # Wait for all tasks to finish (including cancellation)\n        results = await asyncio.gather(*tasks, return_exceptions=True)\n\n        print(\"\\n\ud83d\udccb Final Results:\")\n        for i, result in enumerate(results):\n            if isinstance(result, asyncio.CancelledError):\n                print(f\"  Task {i+1}: Cancelled\")\n            elif isinstance(result, Exception):\n                print(f\"  Task {i+1}: Failed - {result}\")\n            else:\n                print(f\"  Task {i+1}: {result}\")\n\n    finally:\n        monitor_task.cancel()\n        try:\n            await monitor_task\n        except asyncio.CancelledError:\n            pass\n\nasyncio.run(demonstrate_task_management())\n</code></pre>"},{"location":"resources/asyncio_deep_dive/#concurrent-execution-patterns","title":"Concurrent Execution Patterns","text":"<p>Asyncio provides several patterns for running multiple operations. Each has different use cases and behaviors.</p>"},{"location":"resources/asyncio_deep_dive/#asynciogather-all-or-nothing-coordination","title":"asyncio.gather(): All or Nothing Coordination","text":"<pre><code>import asyncio\nimport aiohttp\nimport time\n\nasync def fetch_url(session, url, name):\n    \"\"\"Fetch a URL and return info about the request\"\"\"\n    start_time = time.time()\n    print(f\"\ud83c\udf10 {name}: Starting request to {url}\")\n\n    try:\n        async with session.get(url) as response:\n            content = await response.text()\n            duration = time.time() - start_time\n\n        print(f\"\u2705 {name}: Completed in {duration:.2f}s ({len(content)} chars)\")\n        return {\n            \"name\": name,\n            \"url\": url,\n            \"duration\": duration,\n            \"size\": len(content),\n            \"status\": response.status\n        }\n    except Exception as e:\n        duration = time.time() - start_time\n        print(f\"\u274c {name}: Failed after {duration:.2f}s - {e}\")\n        raise\n\nasync def demonstrate_gather():\n    \"\"\"Shows gather() behavior with success and failure scenarios\"\"\"\n\n    print(\"=== gather() with all successful requests ===\")\n    async with aiohttp.ClientSession() as session:\n        try:\n            results = await asyncio.gather(\n                fetch_url(session, \"https://httpbin.org/delay/1\", \"Fast\"),\n                fetch_url(session, \"https://httpbin.org/delay/2\", \"Medium\"),\n                fetch_url(session, \"https://httpbin.org/delay/1.5\", \"Quick\")\n            )\n\n            print(\"\ud83d\udcca All requests successful:\")\n            for result in results:\n                print(f\"  {result['name']}: {result['duration']:.2f}s\")\n\n        except Exception as e:\n            print(f\"\u274c gather() failed: {e}\")\n\n    print(\"\\n=== gather() with one failure (default behavior) ===\")\n    async with aiohttp.ClientSession() as session:\n        try:\n            results = await asyncio.gather(\n                fetch_url(session, \"https://httpbin.org/delay/1\", \"Success1\"),\n                fetch_url(session, \"https://httpbin.org/status/500\", \"Failure\"),  # Will fail\n                fetch_url(session, \"https://httpbin.org/delay/1\", \"Success2\")\n            )\n        except Exception as e:\n            print(f\"\u274c gather() failed fast due to: {e}\")\n\n    print(\"\\n=== gather() with return_exceptions=True ===\")\n    async with aiohttp.ClientSession() as session:\n        results = await asyncio.gather(\n            fetch_url(session, \"https://httpbin.org/delay/1\", \"Success1\"),\n            fetch_url(session, \"https://httpbin.org/status/500\", \"Failure\"),\n            fetch_url(session, \"https://httpbin.org/delay/1\", \"Success2\"),\n            return_exceptions=True  # Don't fail, return exceptions\n        )\n\n        print(\"\ud83d\udcca Mixed results (exceptions returned):\")\n        for i, result in enumerate(results):\n            if isinstance(result, Exception):\n                print(f\"  Request {i+1}: Failed - {result}\")\n            else:\n                print(f\"  Request {i+1}: Success - {result['name']}\")\n\nasyncio.run(demonstrate_gather())\n</code></pre>"},{"location":"resources/asyncio_deep_dive/#asyncioas_completed-process-results-as-they-arrive","title":"asyncio.as_completed(): Process Results as They Arrive","text":"<pre><code>import asyncio\nimport random\nimport time\n\nasync def variable_duration_task(task_id):\n    \"\"\"Task with random duration to demonstrate as_completed behavior\"\"\"\n    duration = random.uniform(0.5, 3.0)\n    print(f\"\ud83d\ude80 Task {task_id}: Starting ({duration:.1f}s estimated)\")\n\n    await asyncio.sleep(duration)\n\n    print(f\"\u2705 Task {task_id}: Completed after {duration:.1f}s\")\n    return {\n        \"task_id\": task_id,\n        \"duration\": duration,\n        \"completed_at\": time.time()\n    }\n\nasync def demonstrate_as_completed():\n    \"\"\"Shows how as_completed processes results in completion order\"\"\"\n\n    print(\"\ud83c\udfaf Creating tasks with random durations\")\n    start_time = time.time()\n\n    # Create tasks with different expected completion times\n    tasks = [\n        variable_duration_task(f\"T{i}\")\n        for i in range(1, 6)\n    ]\n\n    print(\"\u23f3 Processing results as they complete (not in creation order):\")\n\n    completion_order = []\n\n    # Process results as they become available\n    for completed_task in asyncio.as_completed(tasks):\n        result = await completed_task\n        elapsed = time.time() - start_time\n\n        completion_order.append(result['task_id'])\n        print(f\"\ud83d\udce5 Received result from {result['task_id']} \"\n              f\"after {elapsed:.1f}s total elapsed\")\n\n    total_time = time.time() - start_time\n    print(f\"\\n\ud83d\udcca All tasks completed in {total_time:.1f}s\")\n    print(f\"\ud83d\udd04 Completion order: {' \u2192 '.join(completion_order)}\")\n    print(\"\ud83d\udca1 Notice: Results arrived in completion order, not creation order!\")\n\nasync def demonstrate_as_completed_with_timeout():\n    \"\"\"Shows as_completed with timeout handling\"\"\"\n\n    async def slow_task(task_id, duration):\n        await asyncio.sleep(duration)\n        return f\"Task {task_id} result\"\n\n    print(\"\\n\" + \"=\"*50)\n    print(\"\u23f0 as_completed with timeout demonstration\")\n\n    tasks = [\n        slow_task(\"A\", 1),\n        slow_task(\"B\", 3),  # Will timeout\n        slow_task(\"C\", 2),\n        slow_task(\"D\", 4),  # Will timeout\n    ]\n\n    completed_count = 0\n    timeout_seconds = 2.5\n\n    try:\n        for completed_task in asyncio.as_completed(tasks, timeout=timeout_seconds):\n            try:\n                result = await completed_task\n                completed_count += 1\n                print(f\"\u2705 Completed {completed_count}: {result}\")\n            except asyncio.TimeoutError:\n                print(f\"\u23f0 Task timed out after {timeout_seconds}s\")\n                break\n    except asyncio.TimeoutError:\n        print(f\"\u23f0 Overall timeout reached after {timeout_seconds}s\")\n        print(f\"\ud83d\udcca Completed {completed_count} tasks before timeout\")\n\nasync def main():\n    await demonstrate_as_completed()\n    await demonstrate_as_completed_with_timeout()\n\nasyncio.run(main())\n</code></pre>"},{"location":"resources/asyncio_deep_dive/#asynciowait-fine-grained-control","title":"asyncio.wait(): Fine-Grained Control","text":"<pre><code>import asyncio\nimport time\n\nasync def controlled_task(task_id, duration, should_fail=False):\n    \"\"\"Task for demonstrating wait() control features\"\"\"\n    print(f\"\ud83d\ude80 Task {task_id}: Starting\")\n\n    try:\n        await asyncio.sleep(duration)\n\n        if should_fail:\n            raise ValueError(f\"Task {task_id} intentionally failed\")\n\n        print(f\"\u2705 Task {task_id}: Completed successfully\")\n        return f\"Result from task {task_id}\"\n\n    except asyncio.CancelledError:\n        print(f\"\ud83d\uded1 Task {task_id}: Cancelled\")\n        raise\n\nasync def demonstrate_wait_conditions():\n    \"\"\"Shows different wait conditions and their behaviors\"\"\"\n\n    print(\"=== FIRST_COMPLETED: Return when any task finishes ===\")\n    tasks = [\n        asyncio.create_task(controlled_task(\"A\", 1)),\n        asyncio.create_task(controlled_task(\"B\", 2)),\n        asyncio.create_task(controlled_task(\"C\", 3)),\n    ]\n\n    start_time = time.time()\n    done, pending = await asyncio.wait(\n        tasks, \n        return_when=asyncio.FIRST_COMPLETED\n    )\n    elapsed = time.time() - start_time\n\n    print(f\"\u23f1\ufe0f  First completion after {elapsed:.1f}s\")\n    print(f\"\ud83d\udcca {len(done)} completed, {len(pending)} still running\")\n\n    # Get result from completed task\n    for task in done:\n        result = await task\n        print(f\"\ud83c\udfaf First result: {result}\")\n\n    # Cancel remaining tasks\n    for task in pending:\n        task.cancel()\n\n    # Wait for cancellation to complete\n    await asyncio.wait(pending)\n\n    print(\"\\n=== FIRST_EXCEPTION: Return when any task fails ===\")\n    tasks = [\n        asyncio.create_task(controlled_task(\"X\", 2, should_fail=False)),\n        asyncio.create_task(controlled_task(\"Y\", 1, should_fail=True)),   # Will fail\n        asyncio.create_task(controlled_task(\"Z\", 3, should_fail=False)),\n    ]\n\n    start_time = time.time()\n    done, pending = await asyncio.wait(\n        tasks,\n        return_when=asyncio.FIRST_EXCEPTION\n    )\n    elapsed = time.time() - start_time\n\n    print(f\"\u23f1\ufe0f  First exception after {elapsed:.1f}s\")\n\n    # Check for exceptions\n    for task in done:\n        try:\n            result = await task\n            print(f\"\u2705 Task completed: {result}\")\n        except Exception as e:\n            print(f\"\u274c Task failed: {e}\")\n\n    # Cancel and wait for remaining tasks\n    for task in pending:\n        task.cancel()\n    await asyncio.wait(pending)\n\nasync def demonstrate_wait_with_timeout():\n    \"\"\"Shows wait() with timeout functionality\"\"\"\n\n    print(\"\\n=== wait() with timeout ===\")\n    tasks = [\n        asyncio.create_task(controlled_task(\"Fast\", 1)),\n        asyncio.create_task(controlled_task(\"Medium\", 3)),\n        asyncio.create_task(controlled_task(\"Slow\", 5)),\n    ]\n\n    start_time = time.time()\n\n    # Wait up to 2 seconds\n    done, pending = await asyncio.wait(\n        tasks,\n        timeout=2.0,\n        return_when=asyncio.ALL_COMPLETED\n    )\n\n    elapsed = time.time() - start_time\n    print(f\"\u23f1\ufe0f  Timeout reached after {elapsed:.1f}s\")\n    print(f\"\ud83d\udcca {len(done)} completed, {len(pending)} timed out\")\n\n    # Process completed tasks\n    for task in done:\n        result = await task\n        print(f\"\u2705 Completed before timeout: {result}\")\n\n    # Handle timed-out tasks\n    for task in pending:\n        print(f\"\u23f0 Task timed out, cancelling...\")\n        task.cancel()\n\n    # Wait for cancellation\n    await asyncio.wait(pending)\n\nasync def main():\n    await demonstrate_wait_conditions()\n    await demonstrate_wait_with_timeout()\n\nasyncio.run(main())\n</code></pre>"},{"location":"resources/asyncio_deep_dive/#synchronization-coordinating-concurrent-operations","title":"Synchronization: Coordinating Concurrent Operations","text":"<p>When multiple coroutines work with shared resources, you need synchronization primitives to prevent race conditions and coordinate access.</p>"},{"location":"resources/asyncio_deep_dive/#asynciolock-mutual-exclusion","title":"asyncio.Lock: Mutual Exclusion","text":"<pre><code>import asyncio\nimport random\n\nclass SharedCounter:\n    \"\"\"Demonstrates the need for locks in concurrent operations\"\"\"\n\n    def __init__(self):\n        self.value = 0\n        self.lock = asyncio.Lock()\n        self.operation_log = []\n\n    async def unsafe_increment(self, worker_id):\n        \"\"\"Unsafe increment that can cause race conditions\"\"\"\n        # Read current value\n        old_value = self.value\n\n        # Simulate some processing time (this is where race conditions occur)\n        await asyncio.sleep(0.01)\n\n        # Write back the incremented value\n        self.value = old_value + 1\n\n        self.operation_log.append(f\"Worker {worker_id}: {old_value} \u2192 {self.value}\")\n\n    async def safe_increment(self, worker_id):\n        \"\"\"Safe increment using a lock\"\"\"\n        async with self.lock:  # Only one worker can execute this block at a time\n            old_value = self.value\n\n            # Even with delay, no race condition occurs\n            await asyncio.sleep(0.01)\n\n            self.value = old_value + 1\n            self.operation_log.append(f\"Worker {worker_id}: {old_value} \u2192 {self.value}\")\n\nasync def demonstrate_race_conditions():\n    \"\"\"Shows the difference between safe and unsafe concurrent operations\"\"\"\n\n    print(\"=== Race Condition Demonstration ===\")\n\n    # Test unsafe operations\n    unsafe_counter = SharedCounter()\n\n    print(\"\ud83d\udea8 Running UNSAFE concurrent increments...\")\n    tasks = [\n        unsafe_counter.unsafe_increment(f\"U{i}\")\n        for i in range(10)\n    ]\n\n    await asyncio.gather(*tasks)\n\n    print(f\"\u274c Expected final value: 10, Actual: {unsafe_counter.value}\")\n    print(\"\ud83d\udccb Last few operations:\")\n    for log_entry in unsafe_counter.operation_log[-5:]:\n        print(f\"  {log_entry}\")\n\n    # Test safe operations\n    safe_counter = SharedCounter()\n\n    print(\"\\n\u2705 Running SAFE concurrent increments...\")\n    tasks = [\n        safe_counter.safe_increment(f\"S{i}\")\n        for i in range(10)\n    ]\n\n    await asyncio.gather(*tasks)\n\n    print(f\"\u2705 Expected final value: 10, Actual: {safe_counter.value}\")\n    print(\"\ud83d\udccb Last few operations:\")\n    for log_entry in safe_counter.operation_log[-5:]:\n        print(f\"  {log_entry}\")\n\nasync def demonstrate_lock_usage_patterns():\n    \"\"\"Shows different patterns for using locks effectively\"\"\"\n\n    lock = asyncio.Lock()\n    shared_resource = {\"data\": [], \"processing_count\": 0}\n\n    async def producer(producer_id):\n        \"\"\"Produces data items\"\"\"\n        for i in range(3):\n            item = f\"P{producer_id}-Item{i}\"\n\n            async with lock:\n                shared_resource[\"data\"].append(item)\n                print(f\"\ud83d\udce6 Producer {producer_id} added: {item}\")\n\n            await asyncio.sleep(0.1)  # Simulate work between productions\n\n    async def processor():\n        \"\"\"Processes data items\"\"\"\n        processed_items = []\n\n        while len(processed_items) &lt; 9:  # Expect 3 producers \u00d7 3 items each\n            async with lock:\n                if shared_resource[\"data\"]:\n                    item = shared_resource[\"data\"].pop(0)\n                    shared_resource[\"processing_count\"] += 1\n                    current_count = shared_resource[\"processing_count\"]\n                else:\n                    item = None\n\n            if item:\n                # Process outside the lock to allow other operations\n                await asyncio.sleep(0.05)  # Simulate processing time\n                processed_items.append(item)\n                print(f\"\ud83d\udd27 Processed: {item} (#{current_count})\")\n            else:\n                # No items available, wait a bit\n                await asyncio.sleep(0.01)\n\n        return processed_items\n\n    print(\"\\n=== Producer-Consumer with Lock ===\")\n\n    # Start producers and processor concurrently\n    producer_tasks = [producer(i) for i in range(1, 4)]\n    processor_task = processor()\n\n    results = await asyncio.gather(*producer_tasks, processor_task)\n    processed_items = results[-1]  # Last result is from processor\n\n    print(f\"\ud83d\udcca Processing complete: {len(processed_items)} items processed\")\n\nasync def main():\n    await demonstrate_race_conditions()\n    await demonstrate_lock_usage_patterns()\n\nasyncio.run(main())\n</code></pre>"},{"location":"resources/asyncio_deep_dive/#asynciosemaphore-limiting-concurrent-access","title":"asyncio.Semaphore: Limiting Concurrent Access","text":"<pre><code>import asyncio\nimport aiohttp\nimport time\n\nclass RateLimitedDownloader:\n    \"\"\"Demonstrates semaphore usage for controlling concurrent operations\"\"\"\n\n    def __init__(self, max_concurrent_downloads=3):\n        self.semaphore = asyncio.Semaphore(max_concurrent_downloads)\n        self.download_stats = {\n            \"started\": 0,\n            \"completed\": 0,\n            \"failed\": 0,\n            \"concurrent_peak\": 0,\n            \"currently_running\": 0\n        }\n        self.stats_lock = asyncio.Lock()\n\n    async def download_file(self, session, url, file_id):\n        \"\"\"Download a file with concurrency limiting\"\"\"\n\n        # Wait for permission to start download\n        async with self.semaphore:\n            # Update stats\n            async with self.stats_lock:\n                self.download_stats[\"started\"] += 1\n                self.download_stats[\"currently_running\"] += 1\n                if self.download_stats[\"currently_running\"] &gt; self.download_stats[\"concurrent_peak\"]:\n                    self.download_stats[\"concurrent_peak\"] = self.download_stats[\"currently_running\"]\n\n            print(f\"\ud83c\udf10 Download {file_id}: Starting (concurrent: {self.download_stats['currently_running']})\")\n\n            try:\n                # Simulate download\n                start_time = time.time()\n                async with session.get(url) as response:\n                    data = await response.read()\n                    download_time = time.time() - start_time\n\n                # Simulate file processing\n                await asyncio.sleep(0.1)\n\n                async with self.stats_lock:\n                    self.download_stats[\"completed\"] += 1\n                    self.download_stats[\"currently_running\"] -= 1\n\n                print(f\"\u2705 Download {file_id}: Complete ({len(data)} bytes in {download_time:.2f}s)\")\n                return {\"file_id\": file_id, \"size\": len(data), \"duration\": download_time}\n\n            except Exception as e:\n                async with self.stats_lock:\n                    self.download_stats[\"failed\"] += 1\n                    self.download_stats[\"currently_running\"] -= 1\n\n                print(f\"\u274c Download {file_id}: Failed - {e}\")\n                raise\n\nasync def demonstrate_semaphore_benefits():\n    \"\"\"Shows how semaphores prevent overwhelming services\"\"\"\n\n    print(\"=== Semaphore: Controlling Concurrent Downloads ===\")\n\n    # Create downloader with limit of 3 concurrent downloads\n    downloader = RateLimitedDownloader(max_concurrent_downloads=3)\n\n    # Prepare many download URLs\n    urls = [f\"https://httpbin.org/bytes/1024\" for _ in range(10)]\n\n    async with aiohttp.ClientSession() as session:\n        # Start all downloads (but semaphore limits concurrency)\n        download_tasks = [\n            downloader.download_file(session, url, f\"File-{i+1}\")\n            for i, url in enumerate(urls)\n        ]\n\n        start_time = time.time()\n        results = await asyncio.gather(*download_tasks, return_exceptions=True)\n        total_time = time.time() - start_time\n\n    # Analyze results\n    successful = [r for r in results if isinstance(r, dict)]\n    failed = [r for r in results if isinstance(r, Exception)]\n\n    print(f\"\\n\ud83d\udcca Download Statistics:\")\n    print(f\"  Total downloads: {len(urls)}\")\n    print(f\"  Successful: {len(successful)}\")\n    print(f\"  Failed: {len(failed)}\")\n    print(f\"  Total time: {total_time:.2f}s\")\n    print(f\"  Peak concurrent: {downloader.download_stats['concurrent_peak']}\")\n    print(f\"  Average time per download: {total_time/len(successful):.2f}s\")\n\nasync def demonstrate_semaphore_vs_unlimited():\n    \"\"\"Compares semaphore-limited vs unlimited concurrent access\"\"\"\n\n    async def unlimited_downloads():\n        \"\"\"Downloads without any concurrency limiting\"\"\"\n        async def simple_download(session, url, file_id):\n            start_time = time.time()\n            async with session.get(url) as response:\n                data = await response.read()\n            duration = time.time() - start_time\n            print(f\"\ud83d\ude80 Unlimited {file_id}: {duration:.2f}s\")\n            return duration\n\n        async with aiohttp.ClientSession() as session:\n            tasks = [\n                simple_download(session, f\"https://httpbin.org/delay/1\", f\"U{i}\")\n                for i in range(8)\n            ]\n            return await asyncio.gather(*tasks)\n\n    async def limited_downloads():\n        \"\"\"Downloads with semaphore limiting to 3 concurrent\"\"\"\n        semaphore = asyncio.Semaphore(3)\n\n        async def limited_download(session, url, file_id):\n            async with semaphore:\n                start_time = time.time()\n                async with session.get(url) as response:\n                    data = await response.read()\n                duration = time.time() - start_time\n                print(f\"\ud83c\udf9b\ufe0f  Limited {file_id}: {duration:.2f}s\")\n                return duration\n\n        async with aiohttp.ClientSession() as session:\n            tasks = [\n                limited_download(session, f\"https://httpbin.org/delay/1\", f\"L{i}\")\n                for i in range(8)\n            ]\n            return await asyncio.gather(*tasks)\n\n    print(\"\\n\" + \"=\"*50)\n    print(\"\u26a1 Unlimited Concurrent Downloads (may overwhelm server):\")\n    start_time = time.time()\n    unlimited_results = await unlimited_downloads()\n    unlimited_time = time.time() - start_time\n\n    print(f\"\ud83c\udf9b\ufe0f  Limited Concurrent Downloads (3 at a time):\")\n    start_time = time.time()\n    limited_results = await limited_downloads()\n    limited_time = time.time() - start_time\n\n    print(f\"\\n\ud83d\udcc8 Comparison:\")\n    print(f\"  Unlimited: {unlimited_time:.2f}s total, avg {sum(unlimited_results)/len(unlimited_results):.2f}s per download\")\n    print(f\"  Limited:   {limited_time:.2f}s total, avg {sum(limited_results)/len(limited_results):.2f}s per download\")\n\nasync def main():\n    await demonstrate_semaphore_benefits()\n    await demonstrate_semaphore_vs_unlimited()\n\nasyncio.run(main())\n</code></pre>"},{"location":"resources/asyncio_deep_dive/#asyncioevent-signaling-and-coordination","title":"asyncio.Event: Signaling and Coordination","text":"<pre><code>import asyncio\nimport random\nimport time\n\nasync def demonstrate_event_coordination():\n    \"\"\"Shows how events coordinate multiple coroutines\"\"\"\n\n    # Events for coordination\n    start_event = asyncio.Event()\n    data_ready_event = asyncio.Event()\n    processing_complete_event = asyncio.Event()\n\n    # Shared data\n    shared_data = {\"items\": [], \"processed_items\": []}\n\n    async def data_producer():\n        \"\"\"Produces data and signals when ready\"\"\"\n        print(\"\ud83d\udce6 Producer: Waiting for start signal...\")\n        await start_event.wait()\n\n        print(\"\ud83d\udce6 Producer: Starting data generation...\")\n        for i in range(5):\n            item = f\"Data-{i}\"\n            shared_data[\"items\"].append(item)\n            print(f\"\ud83d\udce6 Producer: Generated {item}\")\n            await asyncio.sleep(0.2)  # Simulate generation time\n\n        print(\"\ud83d\udce6 Producer: All data generated, signaling data_ready\")\n        data_ready_event.set()\n\n    async def data_processor(processor_id):\n        \"\"\"Processes data after it's ready\"\"\"\n        print(f\"\ud83d\udd27 Processor {processor_id}: Waiting for data...\")\n        await data_ready_event.wait()\n\n        print(f\"\ud83d\udd27 Processor {processor_id}: Data is ready, starting processing...\")\n\n        while shared_data[\"items\"]:\n            if shared_data[\"items\"]:  # Double-check in case another processor took it\n                item = shared_data[\"items\"].pop(0)\n\n                # Simulate processing\n                processing_time = random.uniform(0.1, 0.3)\n                await asyncio.sleep(processing_time)\n\n                processed_item = f\"Processed-{item}-by-P{processor_id}\"\n                shared_data[\"processed_items\"].append(processed_item)\n                print(f\"\ud83d\udd27 Processor {processor_id}: Completed {processed_item}\")\n\n        print(f\"\ud83d\udd27 Processor {processor_id}: No more items to process\")\n\n        # Check if all processing is done\n        if not shared_data[\"items\"] and len(shared_data[\"processed_items\"]) == 5:\n            print(f\"\ud83d\udd27 Processor {processor_id}: All processing complete, signaling completion\")\n            processing_complete_event.set()\n\n    async def coordinator():\n        \"\"\"Coordinates the entire process\"\"\"\n        print(\"\ud83d\udc68\u200d\ud83d\udcbc Coordinator: Setting up workers...\")\n\n        # Start producer and processors\n        producer_task = asyncio.create_task(data_producer())\n        processor_tasks = [\n            asyncio.create_task(data_processor(i))\n            for i in range(1, 4)  # 3 processors\n        ]\n\n        await asyncio.sleep(0.5)  # Let everyone get ready\n\n        print(\"\ud83d\udc68\u200d\ud83d\udcbc Coordinator: Sending start signal!\")\n        start_event.set()\n\n        # Wait for processing to complete\n        await processing_complete_event.wait()\n\n        print(\"\ud83d\udc68\u200d\ud83d\udcbc Coordinator: All work completed!\")\n\n        # Wait for all tasks to finish\n        await producer_task\n        await asyncio.gather(*processor_tasks)\n\n        return shared_data[\"processed_items\"]\n\n    print(\"=== Event-Coordinated Processing Pipeline ===\")\n    results = await coordinator()\n    print(f\"\ud83d\udcca Final results: {len(results)} items processed\")\n    for item in results:\n        print(f\"  \u2705 {item}\")\n\nasync def demonstrate_event_patterns():\n    \"\"\"Shows common event usage patterns\"\"\"\n\n    async def waiter(waiter_id, event):\n        \"\"\"Waits for an event with timeout\"\"\"\n        print(f\"\u23f3 Waiter {waiter_id}: Waiting for event...\")\n\n        try:\n            await asyncio.wait_for(event.wait(), timeout=2.0)\n            print(f\"\ud83c\udf89 Waiter {waiter_id}: Event received!\")\n        except asyncio.TimeoutError:\n            print(f\"\u23f0 Waiter {waiter_id}: Timeout waiting for event\")\n\n    async def delayed_setter(event, delay, event_name):\n        \"\"\"Sets an event after a delay\"\"\"\n        print(f\"\u23f0 Setter: Will signal '{event_name}' in {delay}s\")\n        await asyncio.sleep(delay)\n        print(f\"\ud83d\udce2 Setter: Signaling '{event_name}'!\")\n        event.set()\n\n    print(\"\\n=== Event Timeout Patterns ===\")\n\n    # Test 1: Event set before timeout\n    quick_event = asyncio.Event()\n    await asyncio.gather(\n        waiter(\"A\", quick_event),\n        delayed_setter(quick_event, 1.0, \"quick_event\")\n    )\n\n    # Test 2: Event not set before timeout\n    slow_event = asyncio.Event()\n    await asyncio.gather(\n        waiter(\"B\", slow_event),\n        delayed_setter(slow_event, 3.0, \"slow_event\")  # Will timeout\n    )\n\nasync def main():\n    await demonstrate_event_coordination()\n    await demonstrate_event_patterns()\n\nasyncio.run(main())\n</code></pre>"},{"location":"resources/asyncio_deep_dive/#error-handling-and-resilience","title":"Error Handling and Resilience","text":"<p>Proper error handling in asyncio requires understanding how exceptions propagate through concurrent operations.</p>"},{"location":"resources/asyncio_deep_dive/#exception-propagation-in-concurrent-code","title":"Exception Propagation in Concurrent Code","text":"<pre><code>import asyncio\nimport random\n\nasync def demonstrate_exception_propagation():\n    \"\"\"Shows how exceptions behave in different asyncio patterns\"\"\"\n\n    async def reliable_task(task_id):\n        await asyncio.sleep(0.5)\n        return f\"Success from {task_id}\"\n\n    async def unreliable_task(task_id, failure_rate=0.5):\n        await asyncio.sleep(0.5)\n        if random.random() &lt; failure_rate:\n            raise ValueError(f\"Task {task_id} failed randomly!\")\n        return f\"Success from {task_id}\"\n\n    print(\"=== gather() Exception Behavior ===\")\n\n    print(\"\ud83e\uddea Test 1: gather() with default behavior (fail fast)\")\n    try:\n        results = await asyncio.gather(\n            reliable_task(\"R1\"),\n            unreliable_task(\"U1\", failure_rate=1.0),  # Will definitely fail\n            reliable_task(\"R2\")\n        )\n        print(f\"\u2705 All completed: {results}\")\n    except Exception as e:\n        print(f\"\u274c gather() failed: {e}\")\n        print(\"\ud83d\udca1 Note: When one task fails, gather() raises immediately\")\n\n    print(\"\\n\ud83e\uddea Test 2: gather() with return_exceptions=True\")\n    results = await asyncio.gather(\n        reliable_task(\"R3\"),\n        unreliable_task(\"U2\", failure_rate=1.0),  # Will definitely fail\n        reliable_task(\"R4\"),\n        return_exceptions=True  # Return exceptions instead of raising\n    )\n\n    print(\"\ud83d\udcca Mixed results:\")\n    for i, result in enumerate(results):\n        if isinstance(result, Exception):\n            print(f\"  Task {i+1}: \u274c {result}\")\n        else:\n            print(f\"  Task {i+1}: \u2705 {result}\")\n\nasync def demonstrate_task_exception_handling():\n    \"\"\"Shows proper exception handling with individual tasks\"\"\"\n\n    async def monitored_task(task_id, should_fail=False):\n        try:\n            await asyncio.sleep(0.3)\n            if should_fail:\n                raise RuntimeError(f\"Task {task_id} encountered an error\")\n            return f\"Result from {task_id}\"\n        except asyncio.CancelledError:\n            print(f\"\ud83d\uded1 Task {task_id}: Cancelled\")\n            raise  # Always re-raise CancelledError\n\n    print(\"\\n=== Individual Task Exception Handling ===\")\n\n    # Create tasks\n    tasks = [\n        asyncio.create_task(monitored_task(\"T1\", should_fail=False)),\n        asyncio.create_task(monitored_task(\"T2\", should_fail=True)),\n        asyncio.create_task(monitored_task(\"T3\", should_fail=False)),\n        asyncio.create_task(monitored_task(\"T4\", should_fail=True)),\n    ]\n\n    # Handle each task individually\n    results = []\n    for i, task in enumerate(tasks):\n        try:\n            result = await task\n            results.append(result)\n            print(f\"\u2705 Task {i+1}: {result}\")\n        except Exception as e:\n            results.append(f\"Error: {e}\")\n            print(f\"\u274c Task {i+1}: {e}\")\n\n    print(f\"\ud83d\udcca Final results: {len([r for r in results if not r.startswith('Error')])} successful\")\n\nasync def demonstrate_timeout_handling():\n    \"\"\"Shows comprehensive timeout handling strategies\"\"\"\n\n    async def slow_operation(operation_id, duration):\n        print(f\"\ud83d\udc0c Operation {operation_id}: Starting ({duration}s)\")\n        await asyncio.sleep(duration)\n        print(f\"\u2705 Operation {operation_id}: Completed\")\n        return f\"Result from {operation_id}\"\n\n    print(\"\\n=== Timeout Handling Strategies ===\")\n\n    # Strategy 1: Individual operation timeout\n    print(\"\ud83e\uddea Individual operation timeout:\")\n    try:\n        result = await asyncio.wait_for(\n            slow_operation(\"Slow1\", 3),\n            timeout=2.0\n        )\n        print(f\"\u2705 Result: {result}\")\n    except asyncio.TimeoutError:\n        print(\"\u23f0 Operation timed out\")\n\n    # Strategy 2: Partial completion with timeout\n    print(\"\\n\ud83e\uddea Partial completion with timeout:\")\n    operations = [\n        slow_operation(\"Op1\", 1),    # Will complete\n        slow_operation(\"Op2\", 2),    # Will complete  \n        slow_operation(\"Op3\", 4),    # Will timeout\n        slow_operation(\"Op4\", 5),    # Will timeout\n    ]\n\n    try:\n        completed, pending = await asyncio.wait(\n            operations,\n            timeout=2.5,\n            return_when=asyncio.ALL_COMPLETED\n        )\n\n        print(f\"\ud83d\udcca Completed: {len(completed)}, Pending: {len(pending)}\")\n\n        # Process completed tasks\n        for task in completed:\n            try:\n                result = await task\n                print(f\"\u2705 Completed: {result}\")\n            except Exception as e:\n                print(f\"\u274c Failed: {e}\")\n\n        # Cancel pending tasks\n        for task in pending:\n            task.cancel()\n\n        # Wait for cancellation to complete\n        if pending:\n            await asyncio.wait(pending)\n\n    except Exception as e:\n        print(f\"\u274c Unexpected error: {e}\")\n\nasync def demonstrate_resilient_patterns():\n    \"\"\"Shows patterns for building resilient async applications\"\"\"\n\n    async def unreliable_service_call(call_id):\n        \"\"\"Simulates an unreliable external service\"\"\"\n        await asyncio.sleep(0.1)\n        if random.random() &lt; 0.7:  # 70% failure rate\n            raise ConnectionError(f\"Service call {call_id} failed\")\n        return f\"Service result {call_id}\"\n\n    async def retry_with_backoff(operation, max_retries=3, base_delay=0.1):\n        \"\"\"Retry an operation with exponential backoff\"\"\"\n        for attempt in range(max_retries + 1):\n            try:\n                return await operation()\n            except Exception as e:\n                if attempt == max_retries:\n                    raise  # Final attempt failed\n\n                delay = base_delay * (2 ** attempt)  # Exponential backoff\n                print(f\"\u26a0\ufe0f  Attempt {attempt + 1} failed: {e}. Retrying in {delay:.1f}s...\")\n                await asyncio.sleep(delay)\n\n    async def circuit_breaker_call(service_func, failure_threshold=3):\n        \"\"\"Simple circuit breaker pattern\"\"\"\n        if not hasattr(circuit_breaker_call, 'failures'):\n            circuit_breaker_call.failures = 0\n            circuit_breaker_call.last_failure_time = 0\n\n        # Check if circuit is open\n        if circuit_breaker_call.failures &gt;= failure_threshold:\n            time_since_failure = asyncio.get_event_loop().time() - circuit_breaker_call.last_failure_time\n            if time_since_failure &lt; 5.0:  # 5 second timeout\n                raise Exception(\"Circuit breaker is OPEN - service unavailable\")\n            else:\n                # Reset circuit breaker\n                circuit_breaker_call.failures = 0\n\n        try:\n            result = await service_func()\n            circuit_breaker_call.failures = 0  # Reset on success\n            return result\n        except Exception as e:\n            circuit_breaker_call.failures += 1\n            circuit_breaker_call.last_failure_time = asyncio.get_event_loop().time()\n            raise\n\n    print(\"\\n=== Resilient Service Calls ===\")\n\n    # Test retry pattern\n    print(\"\ud83d\udd04 Testing retry with backoff:\")\n    try:\n        result = await retry_with_backoff(\n            lambda: unreliable_service_call(\"RetryTest\"),\n            max_retries=3\n        )\n        print(f\"\u2705 Retry succeeded: {result}\")\n    except Exception as e:\n        print(f\"\u274c All retries failed: {e}\")\n\n    # Test circuit breaker pattern\n    print(\"\\n\ud83d\udd0c Testing circuit breaker:\")\n    for i in range(8):\n        try:\n            result = await circuit_breaker_call(\n                lambda: unreliable_service_call(f\"CB-{i}\")\n            )\n            print(f\"\u2705 Call {i}: {result}\")\n        except Exception as e:\n            print(f\"\u274c Call {i}: {e}\")\n\nasync def main():\n    await demonstrate_exception_propagation()\n    await demonstrate_task_exception_handling()\n    await demonstrate_timeout_handling()\n    await demonstrate_resilient_patterns()\n\nasyncio.run(main())\n</code></pre>"},{"location":"resources/asyncio_deep_dive/#performance-optimization-and-best-practices","title":"Performance Optimization and Best Practices","text":"<p>Understanding asyncio performance characteristics helps you build efficient applications.</p>"},{"location":"resources/asyncio_deep_dive/#connection-pooling-and-resource-management","title":"Connection Pooling and Resource Management","text":"<pre><code>import asyncio\nimport aiohttp\nimport time\n\nclass HighPerformanceHTTPClient:\n    \"\"\"Demonstrates proper resource management and connection pooling\"\"\"\n\n    def __init__(self, max_connections=100, requests_per_host=30):\n        # Configure connection pooling\n        self.connector = aiohttp.TCPConnector(\n            limit=max_connections,           # Total connection pool size\n            limit_per_host=requests_per_host, # Connections per host\n            keepalive_timeout=30,            # Keep connections alive\n            enable_cleanup_closed=True,      # Clean up closed connections\n            use_dns_cache=True,              # Cache DNS lookups\n            ttl_dns_cache=300                # DNS cache TTL\n        )\n\n        # Configure timeouts\n        self.timeout = aiohttp.ClientTimeout(\n            total=30,      # Total request timeout\n            connect=5,     # Connection timeout\n            sock_read=10   # Socket read timeout\n        )\n\n        self.session = None\n        self.request_count = 0\n        self.connection_stats = {\"created\": 0, \"reused\": 0}\n\n    async def __aenter__(self):\n        self.session = aiohttp.ClientSession(\n            connector=self.connector,\n            timeout=self.timeout\n        )\n        return self\n\n    async def __aexit__(self, exc_type, exc_val, exc_tb):\n        if self.session:\n            await self.session.close()\n        await self.connector.close()\n\n    async def get(self, url, **kwargs):\n        \"\"\"Make GET request with connection reuse\"\"\"\n        self.request_count += 1\n\n        async with self.session.get(url, **kwargs) as response:\n            # Track connection reuse\n            connection_key = response.connection.key\n            if hasattr(self, '_seen_connections'):\n                if connection_key in self._seen_connections:\n                    self.connection_stats[\"reused\"] += 1\n                else:\n                    self.connection_stats[\"created\"] += 1\n                    self._seen_connections.add(connection_key)\n            else:\n                self._seen_connections = {connection_key}\n                self.connection_stats[\"created\"] += 1\n\n            return await response.json()\n\n    def get_stats(self):\n        \"\"\"Get performance statistics\"\"\"\n        return {\n            \"total_requests\": self.request_count,\n            \"connections_created\": self.connection_stats[\"created\"],\n            \"connections_reused\": self.connection_stats[\"reused\"],\n            \"reuse_rate\": self.connection_stats[\"reused\"] / max(1, self.request_count) * 100\n        }\n\nasync def demonstrate_connection_pooling():\n    \"\"\"Shows the benefits of connection pooling\"\"\"\n\n    print(\"=== Connection Pooling Performance ===\")\n\n    async def test_with_pooling():\n        \"\"\"Test with proper connection pooling\"\"\"\n        async with HighPerformanceHTTPClient(max_connections=10) as client:\n            start_time = time.time()\n\n            # Make many requests to the same host\n            tasks = [\n                client.get(f\"https://httpbin.org/get?id={i}\")\n                for i in range(20)\n            ]\n\n            results = await asyncio.gather(*tasks, return_exceptions=True)\n\n            duration = time.time() - start_time\n            stats = client.get_stats()\n\n            successful = len([r for r in results if not isinstance(r, Exception)])\n\n            return {\n                \"duration\": duration,\n                \"successful_requests\": successful,\n                \"stats\": stats\n            }\n\n    async def test_without_pooling():\n        \"\"\"Test without connection pooling (creates new session each time)\"\"\"\n        async def single_request(url):\n            async with aiohttp.ClientSession() as session:\n                async with session.get(url) as response:\n                    return await response.json()\n\n        start_time = time.time()\n\n        tasks = [\n            single_request(f\"https://httpbin.org/get?id={i}\")\n            for i in range(20)\n        ]\n\n        results = await asyncio.gather(*tasks, return_exceptions=True)\n        duration = time.time() - start_time\n        successful = len([r for r in results if not isinstance(r, Exception)])\n\n        return {\n            \"duration\": duration,\n            \"successful_requests\": successful,\n            \"stats\": {\"note\": \"No pooling - each request creates new connection\"}\n        }\n\n    # Test both approaches\n    print(\"\ud83d\udd04 Testing WITH connection pooling:\")\n    pooled_results = await test_with_pooling()\n\n    print(\"\ud83d\udd04 Testing WITHOUT connection pooling:\")\n    no_pool_results = await test_without_pooling()\n\n    # Compare results\n    print(f\"\\n\ud83d\udcca Performance Comparison:\")\n    print(f\"  With pooling:    {pooled_results['duration']:.2f}s for {pooled_results['successful_requests']} requests\")\n    print(f\"  Without pooling: {no_pool_results['duration']:.2f}s for {no_pool_results['successful_requests']} requests\")\n    print(f\"  Speedup: {no_pool_results['duration'] / pooled_results['duration']:.1f}x\")\n\n    print(f\"\\n\ud83d\udd0c Connection Statistics:\")\n    stats = pooled_results['stats']\n    print(f\"  Connections created: {stats['connections_created']}\")\n    print(f\"  Connections reused: {stats['connections_reused']}\")\n    print(f\"  Reuse rate: {stats['reuse_rate']:.1f}%\")\n\nasync def demonstrate_batching_optimization():\n    \"\"\"Shows the performance benefits of batching operations\"\"\"\n\n    async def process_item(item_id):\n        \"\"\"Simulate processing a single item\"\"\"\n        await asyncio.sleep(0.1)  # Simulate processing time\n        return f\"Processed-{item_id}\"\n\n    async def process_batch(item_ids):\n        \"\"\"Simulate processing a batch of items (more efficient)\"\"\"\n        await asyncio.sleep(0.05 * len(item_ids))  # Batch processing is more efficient\n        return [f\"BatchProcessed-{item_id}\" for item_id in item_ids]\n\n    print(\"\\n=== Batching vs Individual Processing ===\")\n\n    items = list(range(1, 21))  # 20 items to process\n\n    # Individual processing\n    print(\"\ud83d\udd04 Individual processing:\")\n    start_time = time.time()\n    individual_results = await asyncio.gather(*[\n        process_item(item_id) for item_id in items\n    ])\n    individual_time = time.time() - start_time\n\n    # Batch processing\n    print(\"\ud83d\udd04 Batch processing:\")\n    batch_size = 5\n    batches = [items[i:i+batch_size] for i in range(0, len(items), batch_size)]\n\n    start_time = time.time()\n    batch_results = await asyncio.gather(*[\n        process_batch(batch) for batch in batches\n    ])\n    # Flatten results\n    batch_results_flat = [item for batch in batch_results for item in batch]\n    batch_time = time.time() - start_time\n\n    print(f\"\\n\ud83d\udcca Batching Performance:\")\n    print(f\"  Individual: {individual_time:.2f}s for {len(individual_results)} items\")\n    print(f\"  Batched:    {batch_time:.2f}s for {len(batch_results_flat)} items\")\n    print(f\"  Speedup: {individual_time / batch_time:.1f}x\")\n\nasync def demonstrate_memory_efficient_streaming():\n    \"\"\"Shows memory-efficient processing of large datasets\"\"\"\n\n    async def generate_large_dataset():\n        \"\"\"Simulate a large data source\"\"\"\n        for i in range(10000):  # Large dataset\n            yield {\"id\": i, \"data\": f\"item_{i}\", \"value\": i * 2}\n            # Small delay to simulate real data source\n            if i % 100 == 0:\n                await asyncio.sleep(0.001)\n\n    async def process_streaming_data():\n        \"\"\"Process data as it arrives (memory efficient)\"\"\"\n        processed_count = 0\n        total_value = 0\n\n        async for item in generate_large_dataset():\n            # Process each item as it arrives\n            total_value += item[\"value\"]\n            processed_count += 1\n\n            if processed_count % 1000 == 0:\n                print(f\"\ud83d\udcca Processed {processed_count} items (streaming)\")\n\n        return {\"count\": processed_count, \"total_value\": total_value}\n\n    async def process_batch_loaded_data():\n        \"\"\"Load all data then process (memory intensive)\"\"\"\n        # Load all data into memory\n        all_data = []\n        async for item in generate_large_dataset():\n            all_data.append(item)\n\n        print(f\"\ud83d\udcca Loaded {len(all_data)} items into memory\")\n\n        # Process all data\n        total_value = sum(item[\"value\"] for item in all_data)\n\n        return {\"count\": len(all_data), \"total_value\": total_value}\n\n    print(\"\\n=== Memory-Efficient Streaming ===\")\n\n    import psutil\n    import os\n\n    process = psutil.Process(os.getpid())\n\n    # Test streaming approach\n    print(\"\ud83c\udf0a Streaming processing:\")\n    memory_before = process.memory_info().rss / 1024 / 1024  # MB\n    start_time = time.time()\n\n    streaming_result = await process_streaming_data()\n\n    streaming_time = time.time() - start_time\n    memory_after = process.memory_info().rss / 1024 / 1024  # MB\n    memory_used_streaming = memory_after - memory_before\n\n    # Test batch loading approach\n    print(\"\ud83d\udce6 Batch loading processing:\")\n    memory_before = process.memory_info().rss / 1024 / 1024  # MB\n    start_time = time.time()\n\n    batch_result = await process_batch_loaded_data()\n\n    batch_time = time.time() - start_time\n    memory_after = process.memory_info().rss / 1024 / 1024  # MB\n    memory_used_batch = memory_after - memory_before\n\n    print(f\"\\n\ud83d\udcca Memory and Performance Comparison:\")\n    print(f\"  Streaming: {streaming_time:.2f}s, ~{memory_used_streaming:.1f}MB extra memory\")\n    print(f\"  Batch:     {batch_time:.2f}s, ~{memory_used_batch:.1f}MB extra memory\")\n    print(f\"  Memory savings with streaming: {memory_used_batch - memory_used_streaming:.1f}MB\")\n\nasync def main():\n    await demonstrate_connection_pooling()\n    await demonstrate_batching_optimization()\n    await demonstrate_memory_efficient_streaming()\n\nasyncio.run(main())\n</code></pre>"},{"location":"resources/asyncio_deep_dive/#common-pitfalls-and-solutions","title":"Common Pitfalls and Solutions","text":"<p>Understanding common mistakes helps you write better asyncio code.</p>"},{"location":"resources/asyncio_deep_dive/#blocking-the-event-loop","title":"Blocking the Event Loop","text":"<pre><code>import asyncio\nimport time\nimport requests\nimport aiohttp\nimport concurrent.futures\n\nasync def demonstrate_blocking_problems():\n    \"\"\"Shows how blocking calls break asyncio's concurrency\"\"\"\n\n    print(\"=== The Blocking Problem ===\")\n\n    async def bad_concurrent_requests():\n        \"\"\"\u274c DON'T DO THIS - blocking calls in async code\"\"\"\n        print(\"\ud83d\udc0c Making 'concurrent' requests with blocking library...\")\n        start_time = time.time()\n\n        # These look concurrent but actually run sequentially!\n        tasks = []\n        for i in range(3):\n            # requests.get() is a blocking call that stops the entire event loop\n            task = asyncio.create_task(\n                asyncio.to_thread(  # We'll fix this in the good example\n                    requests.get, f\"https://httpbin.org/delay/1\"\n                )\n            )\n            tasks.append(task)\n\n        results = await asyncio.gather(*tasks)\n        duration = time.time() - start_time\n        print(f\"\ud83d\udc0c 'Concurrent' requests took: {duration:.2f}s\")\n        return duration\n\n    async def truly_blocking_example():\n        \"\"\"\u274c What NOT to do - direct blocking calls\"\"\"\n        print(\"\ud83d\udeab Direct blocking calls (DON'T DO THIS):\")\n        start_time = time.time()\n\n        # This completely blocks the event loop!\n        # We'll simulate this with a CPU-intensive task\n        def cpu_intensive_work():\n            total = 0\n            for i in range(1000000):\n                total += i ** 2\n            return total\n\n        # This blocks everything\n        result = cpu_intensive_work()\n        duration = time.time() - start_time\n        print(f\"\ud83d\udeab Blocking operation took: {duration:.2f}s\")\n        return duration\n\n    async def good_concurrent_requests():\n        \"\"\"\u2705 DO THIS - proper async requests\"\"\"\n        print(\"\ud83d\ude80 Making truly concurrent requests...\")\n        start_time = time.time()\n\n        async with aiohttp.ClientSession() as session:\n            tasks = [\n                session.get(f\"https://httpbin.org/delay/1\")\n                for _ in range(3)\n            ]\n\n            responses = await asyncio.gather(*tasks)\n\n            # Close responses\n            for response in responses:\n                response.close()\n\n        duration = time.time() - start_time\n        print(f\"\ud83d\ude80 Truly concurrent requests took: {duration:.2f}s\")\n        return duration\n\n    async def good_cpu_intensive_work():\n        \"\"\"\u2705 DO THIS - CPU work in executor\"\"\"\n        print(\"\u26a1 CPU work in executor:\")\n        start_time = time.time()\n\n        def cpu_intensive_work():\n            total = 0\n            for i in range(1000000):\n                total += i ** 2\n            return total\n\n        # Run CPU work in thread pool\n        loop = asyncio.get_running_loop()\n        with concurrent.futures.ThreadPoolExecutor() as executor:\n            result = await loop.run_in_executor(executor, cpu_intensive_work)\n\n        duration = time.time() - start_time\n        print(f\"\u26a1 Non-blocking CPU work took: {duration:.2f}s\")\n        return duration\n\n    # Compare approaches\n    good_requests_time = await good_concurrent_requests()\n    bad_requests_time = await bad_concurrent_requests()\n\n    print(f\"\\n\ud83d\udcca Request Performance:\")\n    print(f\"  Speedup with proper async: {bad_requests_time / good_requests_time:.1f}x\")\n\n    blocking_time = await truly_blocking_example()\n    nonblocking_time = await good_cpu_intensive_work()\n\n    print(f\"\\n\ud83d\udcca CPU Work Performance:\")\n    print(f\"  Both approaches took similar time, but async version doesn't block the event loop\")\n\nasync def demonstrate_resource_leaks():\n    \"\"\"Shows resource leak problems and solutions\"\"\"\n\n    print(\"\\n=== Resource Management ===\")\n\n    async def resource_leak_example():\n        \"\"\"\u274c Resource leak example\"\"\"\n        print(\"\ud83d\udea8 Resource leak example (DON'T DO THIS):\")\n\n        sessions = []\n        for i in range(5):\n            # Creating sessions but not closing them!\n            session = aiohttp.ClientSession()\n            sessions.append(session)\n\n            try:\n                async with session.get(\"https://httpbin.org/get\") as response:\n                    data = await response.json()\n                    print(f\"  Request {i+1}: Status {response.status}\")\n            except Exception as e:\n                print(f\"  Request {i+1}: Error {e}\")\n\n        # Sessions are never closed - RESOURCE LEAK!\n        print(\"\ud83d\udea8 Sessions created but never closed - memory and connection leak!\")\n\n        # Clean up for demo (you shouldn't rely on this)\n        for session in sessions:\n            await session.close()\n\n    async def proper_resource_management():\n        \"\"\"\u2705 Proper resource management\"\"\"\n        print(\"\u2705 Proper resource management:\")\n\n        # Method 1: Context manager (preferred)\n        async with aiohttp.ClientSession() as session:\n            tasks = [\n                session.get(\"https://httpbin.org/get\")\n                for _ in range(5)\n            ]\n\n            responses = await asyncio.gather(*tasks)\n\n            for i, response in enumerate(responses):\n                print(f\"  Request {i+1}: Status {response.status}\")\n                response.close()\n\n        # Session automatically closed by context manager\n        print(\"\u2705 Session automatically closed by context manager\")\n\n    async def manual_cleanup_example():\n        \"\"\"\u2705 Manual cleanup when context manager not available\"\"\"\n        print(\"\u2705 Manual cleanup example:\")\n\n        session = aiohttp.ClientSession()\n        try:\n            async with session.get(\"https://httpbin.org/get\") as response:\n                data = await response.json()\n                print(f\"  Manual cleanup: Status {response.status}\")\n        except Exception as e:\n            print(f\"  Error: {e}\")\n        finally:\n            await session.close()  # Always close in finally block\n            print(\"\u2705 Session manually closed in finally block\")\n\n    await resource_leak_example()\n    await proper_resource_management()\n    await manual_cleanup_example()\n\nasync def demonstrate_deadlock_avoidance():\n    \"\"\"Shows how to avoid deadlocks in async code\"\"\"\n\n    print(\"\\n=== Deadlock Avoidance ===\")\n\n    async def deadlock_example():\n        \"\"\"\u26a0\ufe0f Potential deadlock scenario\"\"\"\n        lock1 = asyncio.Lock()\n        lock2 = asyncio.Lock()\n\n        async def task_a():\n            async with lock1:\n                print(\"\ud83d\udd12 Task A: Acquired lock1\")\n                await asyncio.sleep(0.1)\n\n                # Try to acquire lock2 while holding lock1\n                print(\"\ud83d\udd12 Task A: Trying to acquire lock2...\")\n                async with lock2:\n                    print(\"\ud83d\udd12 Task A: Acquired lock2\")\n\n        async def task_b():\n            async with lock2:\n                print(\"\ud83d\udd12 Task B: Acquired lock2\")\n                await asyncio.sleep(0.1)\n\n                # Try to acquire lock1 while holding lock2\n                print(\"\ud83d\udd12 Task B: Trying to acquire lock1...\")\n                async with lock1:\n                    print(\"\ud83d\udd12 Task B: Acquired lock1\")\n\n        print(\"\u26a0\ufe0f Potential deadlock scenario (tasks acquire locks in different order):\")\n\n        try:\n            # This might deadlock!\n            await asyncio.wait_for(\n                asyncio.gather(task_a(), task_b()),\n                timeout=1.0\n            )\n            print(\"\u2705 No deadlock occurred\")\n        except asyncio.TimeoutError:\n            print(\"\u274c Deadlock detected! Tasks timed out\")\n\n    async def deadlock_solution():\n        \"\"\"\u2705 Deadlock-free solution\"\"\"\n        lock1 = asyncio.Lock()\n        lock2 = asyncio.Lock()\n\n        async def safe_task_a():\n            # Always acquire locks in the same order\n            async with lock1:\n                print(\"\u2705 Safe Task A: Acquired lock1\")\n                async with lock2:\n                    print(\"\u2705 Safe Task A: Acquired lock2\")\n                    await asyncio.sleep(0.1)\n\n        async def safe_task_b():\n            # Same order as task A\n            async with lock1:\n                print(\"\u2705 Safe Task B: Acquired lock1\")\n                async with lock2:\n                    print(\"\u2705 Safe Task B: Acquired lock2\")\n                    await asyncio.sleep(0.1)\n\n        print(\"\u2705 Deadlock-free solution (consistent lock ordering):\")\n        await asyncio.gather(safe_task_a(), safe_task_b())\n        print(\"\u2705 All tasks completed successfully\")\n\n    await deadlock_example()\n    await deadlock_solution()\n\nasync def demonstrate_forgotten_await():\n    \"\"\"Shows the consequences of forgetting await\"\"\"\n\n    print(\"\\n=== Forgotten await Problems ===\")\n\n    async def important_async_function():\n        \"\"\"An async function that should be awaited\"\"\"\n        await asyncio.sleep(0.1)\n        return \"Important result\"\n\n    async def forgot_await_example():\n        \"\"\"\u274c Forgetting to await\"\"\"\n        print(\"\u274c Forgetting to await:\")\n\n        # This doesn't actually run the function!\n        result = important_async_function()  # Returns coroutine object\n        print(f\"  Result type: {type(result)}\")\n        print(f\"  Result value: {result}\")\n        print(\"  \u26a0\ufe0f The function never actually executed!\")\n\n        # Clean up to prevent warning\n        result.close()\n\n    async def correct_await_example():\n        \"\"\"\u2705 Proper await usage\"\"\"\n        print(\"\u2705 Proper await usage:\")\n\n        result = await important_async_function()  # Actually runs the function\n        print(f\"  Result: {result}\")\n        print(\"  \u2705 Function executed and returned result\")\n\n    async def loop_await_mistake():\n        \"\"\"\u274c Common mistake in loops\"\"\"\n        print(\"\u274c Forgetting await in loops:\")\n\n        coroutines = []\n        for i in range(3):\n            # This creates coroutine objects but doesn't run them\n            coro = important_async_function()\n            coroutines.append(coro)\n\n        print(f\"  Created {len(coroutines)} coroutine objects\")\n        print(\"  \u26a0\ufe0f None of them have executed yet!\")\n\n        # Clean up\n        for coro in coroutines:\n            coro.close()\n\n    async def correct_loop_example():\n        \"\"\"\u2705 Correct loop with await\"\"\"\n        print(\"\u2705 Correct loop with concurrent execution:\")\n\n        # Create tasks that start immediately\n        tasks = [\n            asyncio.create_task(important_async_function())\n            for _ in range(3)\n        ]\n\n        results = await asyncio.gather(*tasks)\n        print(f\"  Results: {results}\")\n        print(\"  \u2705 All functions executed concurrently\")\n\n    await forgot_await_example()\n    await correct_await_example()\n    await loop_await_mistake()\n    await correct_loop_example()\n\nasync def main():\n    await demonstrate_blocking_problems()\n    await demonstrate_resource_leaks()\n    await demonstrate_deadlock_avoidance()\n    await demonstrate_forgotten_await()\n\nasyncio.run(main())\n</code></pre>"},{"location":"resources/asyncio_deep_dive/#real-world-application-patterns","title":"Real-World Application Patterns","text":"<p>Here are comprehensive patterns for building production asyncio applications:</p>"},{"location":"resources/asyncio_deep_dive/#web-scraping-with-rate-limiting","title":"Web Scraping with Rate Limiting","text":"<pre><code>import asyncio\nimport aiohttp\nimport time\nfrom urllib.parse import urljoin, urlparse\nimport random\n\nclass IntelligentWebScraper:\n    \"\"\"Production-ready web scraper with rate limiting and error handling\"\"\"\n\n    def __init__(self, base_delay=1.0, max_concurrent=5, max_retries=3):\n        self.base_delay = base_delay\n        self.max_concurrent = max_concurrent\n        self.max_retries = max_retries\n        self.session = None\n        self.semaphore = asyncio.Semaphore(max_concurrent)\n\n        # Statistics\n        self.stats = {\n            \"requests_made\": 0,\n            \"successful_requests\": 0,\n            \"failed_requests\": 0,\n            \"retries\": 0,\n            \"rate_limited\": 0\n        }\n\n        # Rate limiting per domain\n        self.last_request_time = {}\n        self.domain_delays = {}\n\n    async def __aenter__(self):\n        connector = aiohttp.TCPConnector(\n            limit=50,\n            limit_per_host=10,\n            keepalive_timeout=30\n        )\n\n        timeout = aiohttp.ClientTimeout(total=30)\n\n        self.session = aiohttp.ClientSession(\n            connector=connector,\n            timeout=timeout,\n            headers={\n                'User-Agent': 'Mozilla/5.0 (compatible; AsyncScraper/1.0)'\n            }\n        )\n        return self\n\n    async def __aexit__(self, exc_type, exc_val, exc_tb):\n        if self.session:\n            await self.session.close()\n\n    async def smart_delay(self, url):\n        \"\"\"Implement intelligent rate limiting per domain\"\"\"\n        domain = urlparse(url).netloc\n\n        # Get domain-specific delay\n        domain_delay = self.domain_delays.get(domain, self.base_delay)\n\n        # Check if we need to wait\n        if domain in self.last_request_time:\n            elapsed = time.time() - self.last_request_time[domain]\n            if elapsed &lt; domain_delay:\n                wait_time = domain_delay - elapsed\n                print(f\"\u23f0 Rate limiting: waiting {wait_time:.1f}s for {domain}\")\n                await asyncio.sleep(wait_time)\n                self.stats[\"rate_limited\"] += 1\n\n        self.last_request_time[domain] = time.time()\n\n    async def fetch_with_retries(self, url):\n        \"\"\"Fetch URL with intelligent retry logic\"\"\"\n        domain = urlparse(url).netloc\n\n        for attempt in range(self.max_retries + 1):\n            try:\n                async with self.semaphore:  # Limit concurrent requests\n                    await self.smart_delay(url)  # Rate limiting\n\n                    self.stats[\"requests_made\"] += 1\n\n                    async with self.session.get(url) as response:\n                        if response.status == 200:\n                            content = await response.text()\n                            self.stats[\"successful_requests\"] += 1\n                            return {\n                                \"url\": url,\n                                \"content\": content,\n                                \"status\": response.status,\n                                \"headers\": dict(response.headers)\n                            }\n\n                        elif response.status == 429:  # Rate limited\n                            # Increase delay for this domain\n                            self.domain_delays[domain] = self.domain_delays.get(domain, self.base_delay) * 2\n                            raise aiohttp.ClientResponseError(\n                                request_info=response.request_info,\n                                history=response.history,\n                                status=response.status,\n                                message=\"Rate limited\"\n                            )\n\n                        else:\n                            raise aiohttp.ClientResponseError(\n                                request_info=response.request_info,\n                                history=response.history,\n                                status=response.status\n                            )\n\n            except Exception as e:\n                self.stats[\"failed_requests\"] += 1\n\n                if attempt == self.max_retries:\n                    print(f\"\u274c Failed to fetch {url} after {self.max_retries + 1} attempts: {e}\")\n                    return None\n\n                # Exponential backoff\n                retry_delay = (2 ** attempt) + random.uniform(0, 1)\n                print(f\"\u26a0\ufe0f Attempt {attempt + 1} failed for {url}: {e}. Retrying in {retry_delay:.1f}s...\")\n                self.stats[\"retries\"] += 1\n                await asyncio.sleep(retry_delay)\n\n        return None\n\nasync def demonstrate_intelligent_scraping():\n    \"\"\"Demonstrates intelligent web scraping\"\"\"\n\n    print(\"=== Intelligent Web Scraping ===\")\n\n    # URLs to scrape\n    urls = [\n        \"https://httpbin.org/delay/1\",\n        \"https://httpbin.org/status/200\",\n        \"https://httpbin.org/json\",\n        \"https://httpbin.org/headers\",\n        \"https://httpbin.org/user-agent\",\n        \"https://httpbin.org/delay/2\",\n        \"https://httpbin.org/status/429\",  # Rate limited response\n        \"https://httpbin.org/html\",\n    ]\n\n    async with IntelligentWebScraper(base_delay=0.5, max_concurrent=3) as scraper:\n        start_time = time.time()\n\n        # Scrape all URLs concurrently with rate limiting\n        tasks = [scraper.fetch_with_retries(url) for url in urls]\n        results = await asyncio.gather(*tasks)\n\n        duration = time.time() - start_time\n\n        # Analyze results\n        successful = [r for r in results if r is not None]\n        failed = [r for r in results if r is None]\n\n        print(f\"\\n\ud83d\udcca Scraping Results:\")\n        print(f\"  Duration: {duration:.2f}s\")\n        print(f\"  Successful: {len(successful)}\")\n        print(f\"  Failed: {len(failed)}\")\n        print(f\"  Total requests made: {scraper.stats['requests_made']}\")\n        print(f\"  Rate limited events: {scraper.stats['rate_limited']}\")\n        print(f\"  Retries performed: {scraper.stats['retries']}\")\n\n        # Show successful results\n        for result in successful[:3]:  # Show first 3\n            print(f\"  \u2705 {result['url']}: {result['status']} ({len(result['content'])} chars)\")\n\nasyncio.run(demonstrate_intelligent_scraping())\n</code></pre> <p>This regenerated asyncio cheatsheet provides a comprehensive, deep-dive exploration of asyncio concepts with working examples and practical patterns. It maintains the focus on explaining how things work rather than just listing tips, making it an excellent learning and reference resource.</p>"},{"location":"resources/pyinstaller_deep_dive/","title":"PyInstaller Deep Dive: Complete Guide with uv","text":""},{"location":"resources/pyinstaller_deep_dive/#pyinstaller-deep-dive-complete-guide-with-uv","title":"PyInstaller Deep Dive: Complete Guide with uv","text":"<p>AI Assisted (Claude Sonnet 4)</p>"},{"location":"resources/pyinstaller_deep_dive/#what-is-pyinstaller","title":"What is PyInstaller?","text":"<p>PyInstaller bundles Python applications into standalone executables that can run on systems without Python installed. It analyzes your code, finds all dependencies, and packages everything into a single file or directory.</p> <p>How it works:</p> <ol> <li>Analysis: Scans your code for imports and dependencies</li> <li>Collection: Gathers all required modules, libraries, and data files</li> <li>Bundling: Creates a bootloader that extracts and runs your application</li> <li>Distribution: Produces executable(s) that work on target systems</li> </ol>"},{"location":"resources/pyinstaller_deep_dive/#installation-and-setup-with-uv","title":"Installation and Setup with uv","text":""},{"location":"resources/pyinstaller_deep_dive/#basic-setup","title":"Basic Setup","text":"<pre><code># Create project with uv\nuv init my-pyinstaller-project\ncd my-pyinstaller-project\n\n# Add PyInstaller as development dependency\nuv add --dev pyinstaller\n\n# Add your application dependencies\nuv add requests click rich pillow\n\n# Verify installation\nuv run pyinstaller --version\n</code></pre>"},{"location":"resources/pyinstaller_deep_dive/#project-structure","title":"Project Structure","text":"<pre><code>my-pyinstaller-project/\n\u251c\u2500\u2500 pyproject.toml          # Project configuration\n\u251c\u2500\u2500 uv.lock                 # Locked dependencies\n\u251c\u2500\u2500 src/\n\u2502   \u2514\u2500\u2500 myapp/\n\u2502       \u251c\u2500\u2500 __init__.py\n\u2502       \u251c\u2500\u2500 main.py         # Entry point\n\u2502       \u251c\u2500\u2500 utils.py\n\u2502       \u2514\u2500\u2500 data/           # Data files\n\u2502           \u251c\u2500\u2500 config.json\n\u2502           \u2514\u2500\u2500 logo.png\n\u251c\u2500\u2500 assets/                 # Build assets\n\u2502   \u251c\u2500\u2500 icon.ico           # Windows icon\n\u2502   \u251c\u2500\u2500 icon.icns          # macOS icon\n\u2502   \u2514\u2500\u2500 version.txt\n\u251c\u2500\u2500 build/                  # Build artifacts (auto-created)\n\u251c\u2500\u2500 dist/                   # Final executables (auto-created)\n\u2514\u2500\u2500 myapp.spec             # PyInstaller spec file\n</code></pre>"},{"location":"resources/pyinstaller_deep_dive/#basic-pyinstaller-usage","title":"Basic PyInstaller Usage","text":""},{"location":"resources/pyinstaller_deep_dive/#simple-one-file-executable","title":"Simple One-File Executable","text":"<pre><code># Basic one-file executable\nuv run pyinstaller --onefile src/myapp/main.py\n\n# One-file with custom name\nuv run pyinstaller --onefile --name myapp src/myapp/main.py\n\n# GUI application (no console window)\nuv run pyinstaller --onefile --windowed src/myapp/main.py\n\n# Console application (with console window)\nuv run pyinstaller --onefile --console src/myapp/main.py\n</code></pre>"},{"location":"resources/pyinstaller_deep_dive/#directory-distribution","title":"Directory Distribution","text":"<pre><code># Create directory distribution (faster startup)\nuv run pyinstaller src/myapp/main.py\n\n# Directory with custom name\nuv run pyinstaller --name myapp src/myapp/main.py\n</code></pre>"},{"location":"resources/pyinstaller_deep_dive/#adding-icons-and-metadata","title":"Adding Icons and Metadata","text":"<pre><code># Windows executable with icon\nuv run pyinstaller --onefile --icon=assets/icon.ico src/myapp/main.py\n\n# macOS executable with icon  \nuv run pyinstaller --onefile --icon=assets/icon.icns src/myapp/main.py\n\n# Add version information (Windows)\nuv run pyinstaller --onefile --version-file=assets/version.txt src/myapp/main.py\n</code></pre>"},{"location":"resources/pyinstaller_deep_dive/#pyinstaller-spec-files","title":"PyInstaller Spec Files","text":""},{"location":"resources/pyinstaller_deep_dive/#what-is-a-spec-file","title":"What is a Spec File?","text":"<p>A spec file is a Python script that tells PyInstaller exactly how to build your application. It provides fine-grained control over the build process.</p>"},{"location":"resources/pyinstaller_deep_dive/#generating-a-spec-file","title":"Generating a Spec File","text":"<pre><code># Generate spec file without building\nuv run pyi-makespec --onefile src/myapp/main.py\n\n# Generate with options\nuv run pyi-makespec --onefile --windowed --icon=assets/icon.ico src/myapp/main.py\n\n# This creates myapp.spec file\n</code></pre>"},{"location":"resources/pyinstaller_deep_dive/#basic-spec-file-structure","title":"Basic Spec File Structure","text":"<p>myapp.spec: </p><pre><code># -*- mode: python ; coding: utf-8 -*-\n\nblock_cipher = None\n\na = Analysis(\n    ['src/myapp/main.py'],           # Entry point scripts\n    pathex=[],                       # Additional search paths\n    binaries=[],                     # Binary files to include\n    datas=[],                        # Data files to include\n    hiddenimports=[],                # Modules not auto-detected\n    hookspath=[],                    # Custom hook directories\n    hooksconfig={},                  # Hook configuration\n    runtime_hooks=[],                # Runtime hook scripts\n    excludes=[],                     # Modules to exclude\n    win_no_prefer_redirects=False,\n    win_private_assemblies=False,\n    cipher=block_cipher,\n    noarchive=False,\n)\n\npyz = PYZ(a.pure, a.zipped_data, cipher=block_cipher)\n\nexe = EXE(\n    pyz,\n    a.scripts,\n    a.binaries,\n    a.zipfiles,\n    a.datas,\n    [],\n    name='myapp',                    # Executable name\n    debug=False,                     # Debug mode\n    bootloader_ignore_signals=False,\n    strip=False,                     # Strip debug symbols\n    upx=True,                        # UPX compression\n    upx_exclude=[],                  # Files to exclude from UPX\n    runtime_tmpdir=None,             # Temporary directory\n    console=True,                    # Console vs GUI mode\n    disable_windowed_traceback=False,\n    target_arch=None,\n    codesign_identity=None,          # macOS code signing\n    entitlements_file=None,          # macOS entitlements\n    icon='assets/icon.ico'           # Application icon\n)\n</code></pre><p></p>"},{"location":"resources/pyinstaller_deep_dive/#advanced-spec-file-configuration","title":"Advanced Spec File Configuration","text":"<p>Complete example with all options: </p><pre><code># -*- mode: python ; coding: utf-8 -*-\nimport sys\nimport os\nfrom PyInstaller.utils.hooks import collect_data_files, collect_submodules\n\n# Configuration\nblock_cipher = None\napp_name = 'MyApp'\napp_version = '1.0.0'\n\n# Collect data files from packages\ndatas = []\ndatas += collect_data_files('mypackage')  # Collect all data files\ndatas += [('src/myapp/data', 'data')]     # Manual data inclusion\n\n# Collect hidden imports\nhiddenimports = []\nhiddenimports += collect_submodules('requests')  # All requests submodules\nhiddenimports += ['pkg_resources.py2_warn']      # Common hidden import\n\n# Binary files (DLLs, shared libraries)\nbinaries = []\nif sys.platform.startswith('win'):\n    # Windows-specific binaries\n    binaries += [('C:/path/to/library.dll', '.')]\nelif sys.platform.startswith('linux'):\n    # Linux-specific binaries\n    binaries += [('/usr/lib/x86_64-linux-gnu/libssl.so.1.1', '.')]\nelif sys.platform.startswith('darwin'):\n    # macOS-specific binaries\n    binaries += [('/usr/local/lib/libcrypto.dylib', '.')]\n\na = Analysis(\n    ['src/myapp/main.py'],\n    pathex=[os.path.abspath('.')],\n    binaries=binaries,\n    datas=datas,\n    hiddenimports=hiddenimports,\n    hookspath=['hooks/'],            # Custom hooks directory\n    hooksconfig={\n        \"gi\": {\n            \"icons\": [\"Adwaita\"],\n            \"themes\": [\"Adwaita\"],\n            \"languages\": [\"en_US\", \"de_DE\"],\n        },\n    },\n    runtime_hooks=['hooks/runtime_hook.py'],\n    excludes=[\n        'tkinter',      # Exclude GUI frameworks if not used\n        'matplotlib',   # Large packages to exclude\n        'scipy',\n        'pandas',\n        'numpy.testing',\n    ],\n    win_no_prefer_redirects=False,\n    win_private_assemblies=False,\n    cipher=block_cipher,\n    noarchive=False,\n)\n\n# Remove unwanted files from collection\ndef remove_from_list(input_list, item_to_remove):\n    return [(name, path, type_) for name, path, type_ in input_list \n            if not name.startswith(item_to_remove)]\n\n# Clean up collected files\na.datas = remove_from_list(a.datas, 'share/doc')\na.datas = remove_from_list(a.datas, 'share/man')\n\npyz = PYZ(a.pure, a.zipped_data, cipher=block_cipher)\n\nexe = EXE(\n    pyz,\n    a.scripts,\n    a.binaries,\n    a.zipfiles,\n    a.datas,\n    [],\n    name=app_name,\n    debug=False,                     # Set to True for debugging\n    bootloader_ignore_signals=False,\n    strip=True,                      # Strip debug symbols (Linux/macOS)\n    upx=True,                        # Enable UPX compression\n    upx_exclude=[                    # Files to exclude from UPX\n        'vcruntime140.dll',\n        'python3x.dll',\n    ],\n    runtime_tmpdir=None,\n    console=False,                   # GUI application\n    disable_windowed_traceback=False,\n    target_arch=None,\n    codesign_identity=None,\n    entitlements_file=None,\n    icon='assets/icon.ico',\n    version='assets/version_info.py'  # Version information file\n)\n\n# macOS App Bundle (optional)\nif sys.platform.startswith('darwin'):\n    app = BUNDLE(\n        exe,\n        name=f'{app_name}.app',\n        icon='assets/icon.icns',\n        bundle_identifier='com.mycompany.myapp',\n        info_plist={\n            'NSHighResolutionCapable': 'True',\n            'CFBundleShortVersionString': app_version,\n            'CFBundleVersion': app_version,\n        },\n    )\n</code></pre><p></p>"},{"location":"resources/pyinstaller_deep_dive/#building-with-spec-file","title":"Building with Spec File","text":"<pre><code># Build using spec file\nuv run pyinstaller myapp.spec\n\n# Build with additional options\nuv run pyinstaller --clean myapp.spec\nuv run pyinstaller --noconfirm myapp.spec\n</code></pre>"},{"location":"resources/pyinstaller_deep_dive/#data-files-and-resources","title":"Data Files and Resources","text":""},{"location":"resources/pyinstaller_deep_dive/#including-data-files","title":"Including Data Files","text":"<p>PyInstaller needs explicit instructions for non-Python files:</p> <pre><code># In spec file - various ways to include data\ndatas = [\n    # (source, destination_in_bundle)\n    ('src/myapp/data/config.json', 'data'),\n    ('src/myapp/data/*.png', 'images'),          # Wildcards\n    ('assets/fonts/', 'fonts/'),                 # Entire directory\n]\n\n# Programmatically collect data files\nfrom PyInstaller.utils.hooks import collect_data_files\ndatas += collect_data_files('mypackage')\ndatas += collect_data_files('mypackage', subdir='templates')\n</code></pre>"},{"location":"resources/pyinstaller_deep_dive/#accessing-data-files-at-runtime","title":"Accessing Data Files at Runtime","text":"<p>In your Python code: </p><pre><code>import sys\nimport os\nfrom pathlib import Path\n\ndef get_resource_path(relative_path):\n    \"\"\"Get absolute path to resource, works for dev and for PyInstaller\"\"\"\n    try:\n        # PyInstaller creates a temp folder and stores path in _MEIPASS\n        base_path = sys._MEIPASS\n    except AttributeError:\n        # Development mode\n        base_path = os.path.abspath(\".\")\n\n    return os.path.join(base_path, relative_path)\n\n# Usage examples\nconfig_path = get_resource_path('data/config.json')\nimage_path = get_resource_path('images/logo.png')\n\n# Alternative using pathlib\ndef get_resource_path_pathlib(relative_path):\n    \"\"\"Get resource path using pathlib\"\"\"\n    if hasattr(sys, '_MEIPASS'):\n        # PyInstaller bundle\n        return Path(sys._MEIPASS) / relative_path\n    else:\n        # Development\n        return Path(__file__).parent / relative_path\n\n# Load configuration\nconfig_file = get_resource_path_pathlib('data/config.json')\nwith open(config_file, 'r') as f:\n    config = json.load(f)\n</code></pre><p></p>"},{"location":"resources/pyinstaller_deep_dive/#common-data-file-patterns","title":"Common Data File Patterns","text":"<pre><code># Spec file examples for common scenarios\n\n# Include all files from a directory\ndatas = [('src/myapp/templates', 'templates')]\n\n# Include specific file types\nimport glob\ndatas = [(f, 'images') for f in glob.glob('src/myapp/images/*.png')]\n\n# Include package data files\nfrom PyInstaller.utils.hooks import collect_data_files\ndatas += collect_data_files('babel', subdir='localedata')\ndatas += collect_data_files('certifi')  # SSL certificates\n\n# Include fonts (common for GUI apps)\nif sys.platform.startswith('win'):\n    datas += [('C:/Windows/Fonts/arial.ttf', 'fonts')]\nelif sys.platform.startswith('linux'):\n    datas += [('/usr/share/fonts/truetype/liberation/LiberationSans-Regular.ttf', 'fonts')]\n</code></pre>"},{"location":"resources/pyinstaller_deep_dive/#hidden-imports-and-dependencies","title":"Hidden Imports and Dependencies","text":""},{"location":"resources/pyinstaller_deep_dive/#understanding-hidden-imports","title":"Understanding Hidden Imports","text":"<p>PyInstaller's static analysis can miss dynamically imported modules:</p> <pre><code># These imports might be missed by PyInstaller\nmodule_name = 'requests.adapters'\n__import__(module_name)\n\n# Dynamic imports\nimportlib.import_module('cryptography.hazmat.backends.openssl')\n\n# Plugin systems\npkg_resources.iter_entry_points('myapp.plugins')\n</code></pre>"},{"location":"resources/pyinstaller_deep_dive/#specifying-hidden-imports","title":"Specifying Hidden Imports","text":"<p>In spec file: </p><pre><code>hiddenimports = [\n    # Common hidden imports\n    'pkg_resources.py2_warn',\n    'pkg_resources.markers',\n\n    # Cryptography\n    'cryptography.hazmat.backends.openssl',\n    'cryptography.hazmat.bindings._rust',\n\n    # Requests\n    'requests.packages.urllib3',\n    'requests.packages.urllib3.util.retry',\n\n    # SQLAlchemy\n    'sqlalchemy.dialects.sqlite',\n    'sqlalchemy.pool',\n\n    # Pillow/PIL\n    'PIL._tkinter_finder',\n\n    # PyQt/PySide\n    'sip',\n\n    # Scientific packages\n    'scipy.sparse.csgraph._validation',\n    'scipy.special._ufuncs_cxx',\n]\n\n# Automatically collect all submodules\nfrom PyInstaller.utils.hooks import collect_submodules\nhiddenimports += collect_submodules('requests')\nhiddenimports += collect_submodules('urllib3')\n</code></pre><p></p> <p>Command line: </p><pre><code>uv run pyinstaller --hidden-import=pkg_resources.py2_warn \\\n                   --hidden-import=cryptography.hazmat.backends.openssl \\\n                   src/myapp/main.py\n</code></pre><p></p>"},{"location":"resources/pyinstaller_deep_dive/#finding-missing-dependencies","title":"Finding Missing Dependencies","text":"<pre><code># Run with imports debug to see what's being imported\nuv run pyinstaller --debug=imports src/myapp/main.py\n\n# Check the build log for missing modules\n# Look for \"WARNING: Hidden import\" messages\n\n# Test the executable and check for ImportError\n./dist/myapp/myapp  # Run and observe errors\n</code></pre>"},{"location":"resources/pyinstaller_deep_dive/#shared-libraries-and-binary-dependencies","title":"Shared Libraries and Binary Dependencies","text":""},{"location":"resources/pyinstaller_deep_dive/#understanding-shared-library-issues","title":"Understanding Shared Library Issues","text":"<p>Common problems: - Missing system libraries (<code>.dll</code>, <code>.so</code>, <code>.dylib</code>) - Version mismatches between build and runtime systems - Architecture mismatches (32-bit vs 64-bit) - Path issues in bundled libraries</p>"},{"location":"resources/pyinstaller_deep_dive/#including-shared-libraries","title":"Including Shared Libraries","text":"<p>In spec file: </p><pre><code>import sys\nimport os\n\nbinaries = []\n\nif sys.platform.startswith('win'):\n    # Windows DLLs\n    binaries += [\n        ('C:/path/to/custom.dll', '.'),\n        ('venv/Lib/site-packages/package/lib/*.dll', 'lib'),\n    ]\n\nelif sys.platform.startswith('linux'):\n    # Linux shared objects\n    binaries += [\n        ('/usr/lib/x86_64-linux-gnu/libssl.so.1.1', '.'),\n        ('/usr/lib/x86_64-linux-gnu/libcrypto.so.1.1', '.'),\n        ('/usr/local/lib/libcustom.so', 'lib'),\n    ]\n\nelif sys.platform.startswith('darwin'):\n    # macOS dylibs\n    binaries += [\n        ('/usr/local/lib/libssl.1.1.dylib', '.'),\n        ('/usr/local/lib/libcrypto.1.1.dylib', '.'),\n        ('/opt/homebrew/lib/libcustom.dylib', 'lib'),\n    ]\n\n# Auto-collect binaries from packages\nfrom PyInstaller.utils.hooks import collect_dynamic_libs\nbinaries += collect_dynamic_libs('numpy')\nbinaries += collect_dynamic_libs('scipy')\n</code></pre><p></p>"},{"location":"resources/pyinstaller_deep_dive/#debugging-library-issues","title":"Debugging Library Issues","text":"<p>Finding missing libraries: </p><pre><code># Linux: Check dependencies\nldd dist/myapp/myapp\n\n# macOS: Check dependencies  \notool -L dist/myapp/myapp\n\n# Windows: Use Dependency Walker or similar tools\n# Or use PowerShell:\nGet-Command dist/myapp/myapp.exe | Select-Object -ExpandProperty FileVersionInfo\n</code></pre><p></p> <p>Runtime library debugging: </p><pre><code># Add to your main script for debugging\nimport sys\nprint(f\"Executable path: {sys.executable}\")\nprint(f\"Python path: {sys.path}\")\n\nif hasattr(sys, '_MEIPASS'):\n    print(f\"PyInstaller temp path: {sys._MEIPASS}\")\n    import os\n    print(\"Files in temp directory:\")\n    for root, dirs, files in os.walk(sys._MEIPASS):\n        level = root.replace(sys._MEIPASS, '').count(os.sep)\n        indent = ' ' * 2 * level\n        print(f\"{indent}{os.path.basename(root)}/\")\n        subindent = ' ' * 2 * (level + 1)\n        for file in files:\n            print(f\"{subindent}{file}\")\n</code></pre><p></p>"},{"location":"resources/pyinstaller_deep_dive/#common-library-solutions","title":"Common Library Solutions","text":"<p>OpenSSL/Cryptography: </p><pre><code># Often needed for HTTPS requests\nhiddenimports = [\n    'cryptography.hazmat.backends.openssl',\n    'cryptography.hazmat.bindings._rust',\n]\n\n# Include OpenSSL libraries explicitly\nif sys.platform.startswith('win'):\n    binaries += [\n        ('venv/Lib/site-packages/cryptography/hazmat/bindings/*.dll', '.'),\n    ]\n</code></pre><p></p> <p>NumPy/SciPy: </p><pre><code># Include BLAS/LAPACK libraries\nfrom PyInstaller.utils.hooks import collect_dynamic_libs\nbinaries += collect_dynamic_libs('numpy')\nbinaries += collect_dynamic_libs('scipy')\n\n# Exclude large unused parts\nexcludes = [\n    'numpy.tests',\n    'scipy.tests',\n    'matplotlib.tests',\n]\n</code></pre><p></p> <p>Qt/GUI Libraries: </p><pre><code># PyQt5/6 or PySide2/6\nhiddenimports += [\n    'sip',\n    'PyQt5.sip',\n    'PyQt5.QtCore',\n    'PyQt5.QtGui',\n    'PyQt5.QtWidgets',\n]\n\n# Include Qt plugins\ndatas += [\n    ('venv/Lib/site-packages/PyQt5/Qt/plugins', 'PyQt5/Qt/plugins'),\n]\n</code></pre><p></p>"},{"location":"resources/pyinstaller_deep_dive/#platform-specific-considerations","title":"Platform-Specific Considerations","text":""},{"location":"resources/pyinstaller_deep_dive/#windows-specific-issues","title":"Windows-Specific Issues","text":"<p>Console vs GUI Applications: </p><pre><code># Console application (shows command prompt)\nuv run pyinstaller --console src/myapp/main.py\n\n# GUI application (no command prompt)\nuv run pyinstaller --windowed src/myapp/main.py\n</code></pre><p></p> <p>Windows version information: </p><pre><code># Create version_info.py\nVSVersionInfo(\n  ffi=FixedFileInfo(\n    filevers=(1, 0, 0, 0),\n    prodvers=(1, 0, 0, 0),\n    mask=0x3f,\n    flags=0x0,\n    OS=0x4,\n    fileType=0x1,\n    subtype=0x0,\n    date=(0, 0)\n  ),\n  kids=[\n    StringFileInfo([\n      StringTable('040904B0', [\n        StringStruct('CompanyName', 'My Company'),\n        StringStruct('FileDescription', 'My Application'),\n        StringStruct('FileVersion', '1.0.0.0'),\n        StringStruct('InternalName', 'myapp'),\n        StringStruct('LegalCopyright', 'Copyright \u00a9 2024'),\n        StringStruct('OriginalFilename', 'myapp.exe'),\n        StringStruct('ProductName', 'My Application'),\n        StringStruct('ProductVersion', '1.0.0.0')\n      ])\n    ]),\n    VarFileInfo([VarStruct('Translation', [1033, 1200])])\n  ]\n)\n\n# Use in spec file\nexe = EXE(\n    # ... other parameters ...\n    version='version_info.py'\n)\n</code></pre><p></p> <p>Antivirus false positives: </p><pre><code># Code signing can reduce false positives\nsigntool sign /f certificate.pfx /p password /t http://timestamp.server dist/myapp.exe\n\n# Or exclude common problematic features\nuv run pyinstaller --exclude-module tkinter \\\n                   --exclude-module matplotlib \\\n                   src/myapp/main.py\n</code></pre><p></p>"},{"location":"resources/pyinstaller_deep_dive/#macos-specific-issues","title":"macOS-Specific Issues","text":"<p>App Bundle creation: </p><pre><code># In spec file for macOS app bundle\napp = BUNDLE(\n    exe,\n    name='MyApp.app',\n    icon='assets/icon.icns',\n    bundle_identifier='com.mycompany.myapp',\n    info_plist={\n        'CFBundleShortVersionString': '1.0.0',\n        'CFBundleVersion': '1.0.0',\n        'NSHighResolutionCapable': 'True',\n        'NSAppTransportSecurity': {\n            'NSAllowsArbitraryLoads': True\n        },\n        'LSMinimumSystemVersion': '10.13.0',\n    },\n)\n</code></pre><p></p> <p>Code signing and notarization: </p><pre><code># Sign the executable\ncodesign --force --sign \"Developer ID Application: Your Name\" dist/MyApp.app\n\n# Create zip for notarization\nditto -c -k --keepParent dist/MyApp.app MyApp.zip\n\n# Submit for notarization\nxcrun notarytool submit MyApp.zip \\\n    --keychain-profile \"AC_PASSWORD\" \\\n    --wait\n\n# Staple the ticket\nxcrun stapler staple dist/MyApp.app\n</code></pre><p></p>"},{"location":"resources/pyinstaller_deep_dive/#linux-specific-issues","title":"Linux-Specific Issues","text":"<p>Shared library compatibility: </p><pre><code># Check glibc version compatibility\nldd --version\n\n# Build on older system for compatibility\n# Or use containers/chroot environments\n</code></pre><p></p> <p>AppImage creation: </p><pre><code># Install AppImage tools\nuv add --dev appimage-builder\n\n# Create AppImage after PyInstaller build\nappimage-builder --recipe appimage.yml\n</code></pre><p></p>"},{"location":"resources/pyinstaller_deep_dive/#optimization-and-troubleshooting","title":"Optimization and Troubleshooting","text":""},{"location":"resources/pyinstaller_deep_dive/#reducing-bundle-size","title":"Reducing Bundle Size","text":"<p>Exclude unnecessary modules: </p><pre><code>excludes = [\n    # GUI frameworks if not used\n    'tkinter',\n    'PyQt5', 'PyQt6',\n    'PySide2', 'PySide6',\n\n    # Scientific libraries if not used\n    'matplotlib',\n    'scipy',\n    'pandas',\n    'numpy.tests',\n\n    # Development tools\n    'pytest',\n    'setuptools',\n    'pip',\n\n    # Documentation\n    'docutils',\n    'sphinx',\n\n    # Unused stdlib modules\n    'xml.etree',\n    'urllib.robotparser',\n    'calendar',\n    'datetime', # if not used\n]\n</code></pre><p></p> <p>Use UPX compression: </p><pre><code># In spec file\nexe = EXE(\n    # ... other parameters ...\n    upx=True,\n    upx_exclude=[\n        'vcruntime140.dll',  # Don't compress critical DLLs\n        'python3x.dll',\n    ]\n)\n</code></pre><p></p> <p>Optimize imports: </p><pre><code># Use specific imports instead of star imports\nfrom requests import get, post  # Instead of: from requests import *\nimport json  # Instead of: import json, xml, yaml, ...\n</code></pre><p></p>"},{"location":"resources/pyinstaller_deep_dive/#performance-optimization","title":"Performance Optimization","text":"<p>Faster startup: </p><pre><code># Use directory distribution instead of onefile\n# (onefile extracts to temp dir each startup)\n\n# Reduce number of modules\n# Use lazy imports where possible\ndef heavy_function():\n    import numpy as np  # Import only when needed\n    # ... function code\n</code></pre><p></p> <p>Memory optimization: </p><pre><code># Exclude test modules\nexcludes = [\n    '*.tests',\n    '*.test',\n    'tests',\n    'test',\n]\n\n# Use noarchive for faster imports\na = Analysis(\n    # ... other parameters ...\n    noarchive=False  # Set to True for faster imports, larger size\n)\n</code></pre><p></p>"},{"location":"resources/pyinstaller_deep_dive/#common-troubleshooting","title":"Common Troubleshooting","text":"<p>Debug build issues: </p><pre><code># Debug mode for detailed output\nuv run pyinstaller --debug=all src/myapp/main.py\n\n# Log imports to find missing modules\nuv run pyinstaller --debug=imports src/myapp/main.py\n\n# Clean build\nuv run pyinstaller --clean --noconfirm myapp.spec\n</code></pre><p></p> <p>Runtime debugging: </p><pre><code># Add debug output to your main script\nimport sys\nimport os\n\ndef debug_environment():\n    print(f\"Python executable: {sys.executable}\")\n    print(f\"Python version: {sys.version}\")\n    print(f\"Platform: {sys.platform}\")\n\n    if hasattr(sys, '_MEIPASS'):\n        print(f\"Running from PyInstaller bundle\")\n        print(f\"Temp directory: {sys._MEIPASS}\")\n        print(f\"Executable: {sys.executable}\")\n    else:\n        print(\"Running in development mode\")\n\n    print(f\"Current working directory: {os.getcwd()}\")\n    print(f\"Python path: {sys.path}\")\n\n# Call at startup for debugging\nif __name__ == \"__main__\":\n    debug_environment()\n    main()\n</code></pre><p></p> <p>Module not found errors: </p><pre><code># Add missing module as hidden import\nuv run pyinstaller --hidden-import=missing_module src/myapp/main.py\n\n# Or in spec file\nhiddenimports = ['missing_module']\n</code></pre><p></p>"},{"location":"resources/pyinstaller_deep_dive/#advanced-build-automation-with-uv","title":"Advanced Build Automation with uv","text":""},{"location":"resources/pyinstaller_deep_dive/#build-script-integration","title":"Build Script Integration","text":"<p>build.py: </p><pre><code>#!/usr/bin/env python3\n\"\"\"\nAdvanced PyInstaller build script using uv\n\"\"\"\nimport subprocess\nimport sys\nimport platform\nimport shutil\nfrom pathlib import Path\nimport argparse\n\ndef run_command(cmd, description=\"\"):\n    \"\"\"Run command and handle errors\"\"\"\n    print(f\"\ud83d\udd28 {description or cmd}\")\n    try:\n        result = subprocess.run(cmd, shell=True, check=True, \n                              capture_output=True, text=True)\n        print(f\"\u2705 {description or cmd} completed\")\n        return result\n    except subprocess.CalledProcessError as e:\n        print(f\"\u274c {description or cmd} failed: {e}\")\n        print(f\"Error output: {e.stderr}\")\n        sys.exit(1)\n\ndef clean_build():\n    \"\"\"Clean build directories\"\"\"\n    dirs_to_clean = ['build', 'dist', '__pycache__']\n    for dir_name in dirs_to_clean:\n        if Path(dir_name).exists():\n            shutil.rmtree(dir_name)\n            print(f\"\ud83e\uddf9 Cleaned {dir_name}\")\n\ndef build_executable(mode='onefile', gui=False, optimize=False, \n                    debug=False, spec_file=None):\n    \"\"\"Build executable with PyInstaller\"\"\"\n\n    # Sync dependencies first\n    run_command(\"uv sync\", \"Syncing dependencies\")\n\n    if spec_file:\n        # Use spec file\n        cmd = f\"uv run pyinstaller\"\n        if debug:\n            cmd += \" --debug=all\"\n        cmd += f\" {spec_file}\"\n    else:\n        # Command line build\n        cmd = f\"uv run pyinstaller\"\n\n        # Mode\n        if mode == 'onefile':\n            cmd += \" --onefile\"\n        elif mode == 'onedir':\n            cmd += \" --onedir\"\n\n        # GUI vs Console\n        if gui:\n            cmd += \" --windowed\"\n        else:\n            cmd += \" --console\"\n\n        # Optimization\n        if optimize:\n            cmd += \" --strip\"\n            cmd += \" --exclude-module tkinter\"\n            cmd += \" --exclude-module matplotlib\"\n\n        # Debug\n        if debug:\n            cmd += \" --debug=all\"\n\n        # Icon\n        system = platform.system().lower()\n        icon_ext = 'ico' if system == 'windows' else 'icns'\n        icon_path = f\"assets/icon.{icon_ext}\"\n        if Path(icon_path).exists():\n            cmd += f\" --icon={icon_path}\"\n\n        # Entry point\n        cmd += \" src/myapp/main.py\"\n\n    run_command(cmd, \"Building executable\")\n\ndef test_executable():\n    \"\"\"Test the built executable\"\"\"\n    system = platform.system().lower()\n    exe_name = \"myapp\"\n    if system == \"windows\":\n        exe_name += \".exe\"\n\n    # Find executable\n    exe_path = None\n    if Path(f\"dist/{exe_name}\").exists():\n        exe_path = f\"dist/{exe_name}\"\n    elif Path(f\"dist/myapp/{exe_name}\").exists():\n        exe_path = f\"dist/myapp/{exe_name}\"\n\n    if exe_path:\n        print(f\"\ud83e\uddea Testing executable: {exe_path}\")\n        # Test with --help flag\n        try:\n            result = subprocess.run([exe_path, \"--help\"], \n                                  capture_output=True, text=True, timeout=30)\n            if result.returncode == 0:\n                print(\"\u2705 Executable test passed\")\n                return True\n            else:\n                print(f\"\u274c Executable test failed: {result.stderr}\")\n                return False\n        except subprocess.TimeoutExpired:\n            print(\"\u274c Executable test timed out\")\n            return False\n        except Exception as e:\n            print(f\"\u274c Executable test error: {e}\")\n            return False\n    else:\n        print(\"\u274c Executable not found\")\n        return False\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"PyInstaller build script\")\n    parser.add_argument(\"--mode\", choices=['onefile', 'onedir'], \n                       default='onefile', help=\"Build mode\")\n    parser.add_argument(\"--gui\", action='store_true', \n                       help=\"Build GUI application\")\n    parser.add_argument(\"--optimize\", action='store_true', \n                       help=\"Enable optimizations\")\n    parser.add_argument(\"--debug\", action='store_true', \n                       help=\"Enable debug mode\")\n    parser.add_argument(\"--clean\", action='store_true', \n                       help=\"Clean before build\")\n    parser.add_argument(\"--test\", action='store_true', \n                       help=\"Test executable after build\")\n    parser.add_argument(\"--spec\", help=\"Use spec file instead of command line\")\n\n    args = parser.parse_args()\n\n    if args.clean:\n        clean_build()\n\n    build_executable(\n        mode=args.mode,\n        gui=args.gui,\n        optimize=args.optimize,\n        debug=args.debug,\n        spec_file=args.spec\n    )\n\n    if args.test:\n        success = test_executable()\n        if not success:\n            sys.exit(1)\n\n    print(\"\ud83c\udf89 Build completed successfully!\")\n\nif __name__ == \"__main__\":\n    main()\n</code></pre><p></p> <p>Usage: </p><pre><code># Basic build\nuv run python build.py\n\n# Optimized GUI build\nuv run python build.py --gui --optimize --clean\n\n# Debug build with testing\nuv run python build.py --debug --test\n\n# Use spec file\nuv run python build.py --spec myapp.spec --test\n</code></pre><p></p>"},{"location":"resources/pyinstaller_deep_dive/#cicd-integration","title":"CI/CD Integration","text":"<p>GitHub Actions example: </p><pre><code>name: Build Executables\n\non:\n  push:\n    tags: ['v*']\n  pull_request:\n    branches: [main]\n\njobs:\n  build:\n    runs-on: ${{ matrix.os }}\n    strategy:\n      matrix:\n        include:\n          - os: windows-latest\n            platform: windows\n            executable: myapp.exe\n          - os: macos-latest\n            platform: macos\n            executable: myapp\n          - os: ubuntu-latest\n            platform: linux\n            executable: myapp\n\n    steps:\n    - uses: actions/checkout@v4\n\n    - name: Install uv\n      uses: astral-sh/setup-uv@v2\n      with:\n        version: \"latest\"\n\n    - name: Sync dependencies\n      run: uv sync\n\n    - name: Build executable\n      run: uv run python build.py --optimize --clean\n\n    - name: Test executable\n      run: uv run python build.py --test\n\n    - name: Upload executable\n      uses: actions/upload-artifact@v4\n      with:\n        name: executable-${{ matrix.platform }}\n        path: |\n          dist/${{ matrix.executable }}\n          dist/myapp/\n        retention-days: 30\n\n    - name: Create Release\n      if: startsWith(github.ref, 'refs/tags/')\n      uses: softprops/action-gh-release@v1\n      with:\n        files: dist/${{ matrix.executable }}\n        draft: false\n        prerelease: false\n</code></pre><p></p>"},{"location":"resources/pyinstaller_deep_dive/#complete-real-world-example","title":"Complete Real-World Example","text":""},{"location":"resources/pyinstaller_deep_dive/#project-structure_1","title":"Project Structure","text":"<pre><code>flask-desktop-app/\n\u251c\u2500\u2500 pyproject.toml\n\u251c\u2500\u2500 uv.lock\n\u251c\u2500\u2500 myapp.spec                  # PyInstaller spec file\n\u251c\u2500\u2500 build.py                    # Build automation script\n\u251c\u2500\u2500 src/\n\u2502   \u2514\u2500\u2500 desktop_app/\n\u2502       \u251c\u2500\u2500 __init__.py\n\u2502       \u251c\u2500\u2500 main.py            # Entry point\n\u2502       \u251c\u2500\u2500 app.py             # Flask application\n\u2502       \u251c\u2500\u2500 models.py          # Data models\n\u2502       \u251c\u2500\u2500 utils.py           # Utilities\n\u2502       \u251c\u2500\u2500 static/            # Web assets\n\u2502       \u2502   \u251c\u2500\u2500 css/\n\u2502       \u2502   \u251c\u2500\u2500 js/\n\u2502       \u2502   \u2514\u2500\u2500 images/\n\u2502       \u2514\u2500\u2500 templates/         # HTML templates\n\u2502           \u251c\u2500\u2500 base.html\n\u2502           \u2514\u2500\u2500 index.html\n\u251c\u2500\u2500 assets/                    # Build assets\n\u2502   \u251c\u2500\u2500 icon.ico              # Windows icon\n\u2502   \u251c\u2500\u2500 icon.icns             # macOS icon\n\u2502   \u2514\u2500\u2500 version_info.py       # Windows version info\n\u251c\u2500\u2500 hooks/                     # Custom PyInstaller hooks\n\u2502   \u2514\u2500\u2500 hook-custom_module.py\n\u2514\u2500\u2500 dist/                      # Built executables\n</code></pre>"},{"location":"resources/pyinstaller_deep_dive/#flask-desktop-application","title":"Flask Desktop Application","text":"<p>src/desktop_app/main.py: </p><pre><code>#!/usr/bin/env python3\n\"\"\"\nDesktop Flask application with PyInstaller\n\"\"\"\nimport sys\nimport os\nimport threading\nimport webbrowser\nimport time\nfrom pathlib import Path\n\n# Add the application directory to Python path\nif hasattr(sys, '_MEIPASS'):\n    # Running as PyInstaller bundle\n    app_dir = Path(sys._MEIPASS)\nelse:\n    # Running in development\n    app_dir = Path(__file__).parent\n\n# Import Flask app\nfrom .app import create_app\n\ndef get_resource_path(relative_path):\n    \"\"\"Get absolute path to resource for PyInstaller\"\"\"\n    if hasattr(sys, '_MEIPASS'):\n        return os.path.join(sys._MEIPASS, relative_path)\n    return os.path.join(os.path.abspath(\".\"), relative_path)\n\ndef find_free_port():\n    \"\"\"Find a free port for the Flask server\"\"\"\n    import socket\n    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n        s.bind(('', 0))\n        s.listen(1)\n        port = s.getsockname()[1]\n    return port\n\ndef run_flask_app(port):\n    \"\"\"Run Flask application in a separate thread\"\"\"\n    app = create_app()\n\n    # Configure static and template paths for PyInstaller\n    if hasattr(sys, '_MEIPASS'):\n        app.static_folder = os.path.join(sys._MEIPASS, 'static')\n        app.template_folder = os.path.join(sys._MEIPASS, 'templates')\n\n    app.run(host='127.0.0.1', port=port, debug=False, use_reloader=False)\n\ndef open_browser(url):\n    \"\"\"Open browser after a short delay\"\"\"\n    time.sleep(1.5)  # Wait for Flask to start\n    webbrowser.open(url)\n\ndef main():\n    \"\"\"Main application entry point\"\"\"\n    print(\"Starting Desktop Flask Application...\")\n\n    # Find free port\n    port = find_free_port()\n    url = f\"http://127.0.0.1:{port}\"\n\n    print(f\"Server will run on: {url}\")\n\n    # Start Flask in background thread\n    flask_thread = threading.Thread(target=run_flask_app, args=(port,))\n    flask_thread.daemon = True\n    flask_thread.start()\n\n    # Open browser\n    browser_thread = threading.Thread(target=open_browser, args=(url,))\n    browser_thread.daemon = True\n    browser_thread.start()\n\n    print(\"Application started! Press Ctrl+C to exit.\")\n\n    try:\n        # Keep main thread alive\n        while True:\n            time.sleep(1)\n    except KeyboardInterrupt:\n        print(\"\\nShutting down...\")\n        sys.exit(0)\n\nif __name__ == \"__main__\":\n    main()\n</code></pre><p></p> <p>src/desktop_app/app.py: </p><pre><code>\"\"\"\nFlask application factory\n\"\"\"\nfrom flask import Flask, render_template, jsonify\nimport os\nimport sys\n\ndef create_app():\n    \"\"\"Create and configure Flask application\"\"\"\n    app = Flask(__name__)\n\n    # Configure secret key\n    app.config['SECRET_KEY'] = 'your-secret-key-here'\n\n    @app.route('/')\n    def index():\n        \"\"\"Main page\"\"\"\n        return render_template('index.html', title='Desktop Flask App')\n\n    @app.route('/api/info')\n    def api_info():\n        \"\"\"API endpoint returning system info\"\"\"\n        return jsonify({\n            'platform': sys.platform,\n            'python_version': sys.version,\n            'executable': sys.executable,\n            'frozen': hasattr(sys, '_MEIPASS'),\n            'temp_path': getattr(sys, '_MEIPASS', 'Not bundled')\n        })\n\n    @app.route('/api/files')\n    def api_files():\n        \"\"\"API endpoint listing bundled files\"\"\"\n        if hasattr(sys, '_MEIPASS'):\n            files = []\n            for root, dirs, filenames in os.walk(sys._MEIPASS):\n                for filename in filenames:\n                    rel_path = os.path.relpath(os.path.join(root, filename), sys._MEIPASS)\n                    files.append(rel_path)\n            return jsonify({'files': sorted(files)})\n        else:\n            return jsonify({'files': ['Running in development mode']})\n\n    return app\n</code></pre><p></p>"},{"location":"resources/pyinstaller_deep_dive/#advanced-pyinstaller-spec-file","title":"Advanced PyInstaller Spec File","text":"<p>myapp.spec: </p><pre><code># -*- mode: python ; coding: utf-8 -*-\nimport sys\nimport os\nfrom PyInstaller.utils.hooks import collect_data_files, collect_submodules\n\n# Application configuration\napp_name = 'DesktopFlaskApp'\nblock_cipher = None\n\n# Collect Flask templates and static files\ndatas = [\n    ('src/desktop_app/templates', 'templates'),\n    ('src/desktop_app/static', 'static'),\n]\n\n# Collect data files from packages\ndatas += collect_data_files('flask')\ndatas += collect_data_files('jinja2')\ndatas += collect_data_files('werkzeug')\n\n# Hidden imports for Flask and related packages\nhiddenimports = [\n    # Flask essentials\n    'flask',\n    'flask.json',\n    'jinja2',\n    'jinja2.ext',\n    'werkzeug',\n    'werkzeug.security',\n    'werkzeug.serving',\n    'werkzeug.routing',\n\n    # Standard library modules often missed\n    'pkg_resources.py2_warn',\n    'pkg_resources.markers',\n\n    # Threading support\n    'threading',\n    'queue',\n\n    # Network support\n    'socket',\n    'socketserver',\n    'http.server',\n\n    # JSON support\n    'json',\n    'simplejson',\n]\n\n# Collect all Flask submodules\nhiddenimports += collect_submodules('flask')\nhiddenimports += collect_submodules('jinja2')\nhiddenimports += collect_submodules('werkzeug')\n\n# Binaries (if needed)\nbinaries = []\n\n# Modules to exclude (reduce size)\nexcludes = [\n    'tkinter',\n    'matplotlib',\n    'scipy',\n    'numpy',\n    'pandas',\n    'PIL',\n    'PyQt5',\n    'PyQt6',\n    'PySide2',\n    'PySide6',\n    'test',\n    'tests',\n    'unittest',\n    'doctest',\n    'pdb',\n    'pydoc',\n]\n\na = Analysis(\n    ['src/desktop_app/main.py'],\n    pathex=[],\n    binaries=binaries,\n    datas=datas,\n    hiddenimports=hiddenimports,\n    hookspath=['hooks'],\n    hooksconfig={},\n    runtime_hooks=[],\n    excludes=excludes,\n    win_no_prefer_redirects=False,\n    win_private_assemblies=False,\n    cipher=block_cipher,\n    noarchive=False,\n)\n\n# Filter out unnecessary files\ndef filter_binaries(binaries_list):\n    \"\"\"Remove unnecessary binary files\"\"\"\n    filtered = []\n    skip_patterns = ['api-ms-win', 'ucrtbase', 'msvcp', 'vcruntime']\n\n    for name, path, type_info in binaries_list:\n        skip = any(pattern in name.lower() for pattern in skip_patterns)\n        if not skip:\n            filtered.append((name, path, type_info))\n\n    return filtered\n\na.binaries = filter_binaries(a.binaries)\n\npyz = PYZ(a.pure, a.zipped_data, cipher=block_cipher)\n\nexe = EXE(\n    pyz,\n    a.scripts,\n    a.binaries,\n    a.zipfiles,\n    a.datas,\n    [],\n    name=app_name,\n    debug=False,                    # Set to True for debugging\n    bootloader_ignore_signals=False,\n    strip=True,                     # Strip debug symbols\n    upx=True,                       # Enable UPX compression\n    upx_exclude=[\n        'vcruntime140.dll',\n        'msvcp140.dll',\n        'api-ms-win-*.dll',\n    ],\n    runtime_tmpdir=None,\n    console=True,                   # Change to False for GUI-only\n    disable_windowed_traceback=False,\n    target_arch=None,\n    codesign_identity=None,\n    entitlements_file=None,\n    icon='assets/icon.ico' if sys.platform.startswith('win') else 'assets/icon.icns',\n    version='assets/version_info.py' if sys.platform.startswith('win') else None,\n)\n\n# Create macOS app bundle\nif sys.platform.startswith('darwin'):\n    app = BUNDLE(\n        exe,\n        name=f'{app_name}.app',\n        icon='assets/icon.icns',\n        bundle_identifier='com.mycompany.desktopflaskapp',\n        info_plist={\n            'NSHighResolutionCapable': 'True',\n            'CFBundleShortVersionString': '1.0.0',\n            'CFBundleVersion': '1.0.0',\n            'LSMinimumSystemVersion': '10.13.0',\n            'NSAppTransportSecurity': {\n                'NSAllowsArbitraryLoads': True\n            },\n        },\n    )\n</code></pre><p></p>"},{"location":"resources/pyinstaller_deep_dive/#custom-pyinstaller-hook","title":"Custom PyInstaller Hook","text":"<p>hooks/hook-custom_module.py: </p><pre><code>\"\"\"\nCustom PyInstaller hook for additional modules\n\"\"\"\nfrom PyInstaller.utils.hooks import collect_data_files, collect_submodules\n\n# Collect data files\ndatas = collect_data_files('my_custom_package')\n\n# Collect hidden imports\nhiddenimports = collect_submodules('my_custom_package')\n\n# Additional hidden imports\nhiddenimports += [\n    'my_custom_package.submodule',\n    'my_custom_package.plugins',\n]\n</code></pre><p></p>"},{"location":"resources/pyinstaller_deep_dive/#windows-version-information","title":"Windows Version Information","text":"<p>assets/version_info.py: </p><pre><code>\"\"\"\nWindows version information for PyInstaller\n\"\"\"\nVSVersionInfo(\n  ffi=FixedFileInfo(\n    filevers=(1, 0, 0, 0),\n    prodvers=(1, 0, 0, 0),\n    mask=0x3f,\n    flags=0x0,\n    OS=0x4,\n    fileType=0x1,\n    subtype=0x0,\n    date=(0, 0)\n  ),\n  kids=[\n    StringFileInfo([\n      StringTable('040904B0', [\n        StringStruct('CompanyName', 'My Company'),\n        StringStruct('FileDescription', 'Desktop Flask Application'),\n        StringStruct('FileVersion', '1.0.0.0'),\n        StringStruct('InternalName', 'DesktopFlaskApp'),\n        StringStruct('LegalCopyright', 'Copyright \u00a9 2024 My Company'),\n        StringStruct('OriginalFilename', 'DesktopFlaskApp.exe'),\n        StringStruct('ProductName', 'Desktop Flask Application'),\n        StringStruct('ProductVersion', '1.0.0.0')\n      ])\n    ]),\n    VarFileInfo([VarStruct('Translation', [1033, 1200])])\n  ]\n)\n</code></pre><p></p>"},{"location":"resources/pyinstaller_deep_dive/#troubleshooting-common-issues","title":"Troubleshooting Common Issues","text":""},{"location":"resources/pyinstaller_deep_dive/#importerror-no-module-named-module_name","title":"ImportError: No module named 'module_name'","text":"<p>Solution: </p><pre><code># Add to hiddenimports in spec file\nhiddenimports = ['missing_module_name']\n\n# Or use command line\nuv run pyinstaller --hidden-import=missing_module_name src/app/main.py\n</code></pre><p></p>"},{"location":"resources/pyinstaller_deep_dive/#filenotfounderror-no-such-file-or-directory","title":"FileNotFoundError: No such file or directory","text":"<p>Solution: </p><pre><code># Add data files to spec file\ndatas = [('path/to/file', 'destination')]\n\n# Or use command line\nuv run pyinstaller --add-data \"src_path;dest_path\" src/app/main.py  # Windows\nuv run pyinstaller --add-data \"src_path:dest_path\" src/app/main.py  # Unix\n</code></pre><p></p>"},{"location":"resources/pyinstaller_deep_dive/#ssl-certificate-issues","title":"SSL Certificate Issues","text":"<p>Solution: </p><pre><code># Include certificates in spec file\nimport certifi\ndatas = [(certifi.where(), 'certifi')]\n\n# Or set environment variable in code\nimport os\nimport certifi\nos.environ['SSL_CERT_FILE'] = certifi.where()\n</code></pre><p></p>"},{"location":"resources/pyinstaller_deep_dive/#large-executable-size","title":"Large Executable Size","text":"<p>Solutions: </p><pre><code># Exclude unnecessary modules\nexcludes = [\n    'tkinter', 'matplotlib', 'scipy', 'numpy.tests',\n    'pandas', 'PIL', 'PyQt5', 'test', 'tests'\n]\n\n# Use UPX compression\nupx=True\n\n# Use onedir instead of onefile for faster startup\n# (though it creates a directory instead of single file)\n</code></pre><p></p>"},{"location":"resources/pyinstaller_deep_dive/#slow-startup-time","title":"Slow Startup Time","text":"<p>Solutions: </p><pre><code># Use onedir distribution instead of onefile\nuv run pyinstaller --onedir src/app/main.py\n\n# Reduce number of imports\n# Use lazy imports in your code\n\n# Set noarchive=True in spec file for faster imports\nnoarchive=True\n</code></pre><p></p>"},{"location":"resources/pyinstaller_deep_dive/#best-practices-summary","title":"Best Practices Summary","text":""},{"location":"resources/pyinstaller_deep_dive/#development-best-practices","title":"Development Best Practices","text":"<ol> <li>Use uv for dependency management - Fast, reproducible environments</li> <li>Test early and often - Build executables during development</li> <li>Use spec files - Better control and reproducibility</li> <li>Version control spec files - Track build configuration changes</li> <li>Automate builds - Use scripts and CI/CD pipelines</li> </ol>"},{"location":"resources/pyinstaller_deep_dive/#build-configuration-best-practices","title":"Build Configuration Best Practices","text":"<ol> <li>Start simple - Begin with basic builds, add complexity gradually</li> <li>Use appropriate build mode - onefile vs onedir based on needs</li> <li>Optimize for target audience - Consider size vs startup time trade-offs</li> <li>Include necessary data files - Don't forget templates, configs, etc.</li> <li>Handle platform differences - Test on target platforms</li> </ol>"},{"location":"resources/pyinstaller_deep_dive/#distribution-best-practices","title":"Distribution Best Practices","text":"<ol> <li>Code sign executables - Reduces antivirus false positives</li> <li>Test on clean systems - Virtual machines without Python</li> <li>Document system requirements - Minimum OS versions, dependencies</li> <li>Provide installation instructions - Help users install and run</li> <li>Monitor for issues - Be prepared to update builds for compatibility</li> </ol> <p>This comprehensive guide covers everything you need to know about using PyInstaller with uv for creating robust, distributable Python applications. The combination of uv's speed and PyInstaller's maturity provides an excellent foundation for Python application distribution.</p>"},{"location":"resources/pyinstaller_deep_dive/#pyinstaller-alternatives-and-comparisons","title":"PyInstaller Alternatives and Comparisons","text":"<p>While PyInstaller is the most popular choice for creating Python executables, several alternatives exist, each with unique strengths and trade-offs. Here's a comprehensive comparison to help you choose the right tool.</p>"},{"location":"resources/pyinstaller_deep_dive/#1-nuitka-the-performance-champion","title":"1. Nuitka - The Performance Champion","text":"<p>What it is: Nuitka is a Python compiler that translates Python code to C++ and then compiles to native machine code.</p> <p>Key Features: - True compilation (not bundling like PyInstaller) - Faster execution than interpreted Python - Smaller executables than PyInstaller - Gradual optimization - can compile individual modules</p> <p>Using Nuitka with uv: </p><pre><code># Install Nuitka\nuv add --dev nuitka\n\n# Basic compilation\nuv run python -m nuitka --onefile src/myapp/main.py\n\n# Optimized compilation\nuv run python -m nuitka --onefile --remove-output --assume-yes-for-downloads src/myapp/main.py\n\n# GUI application\nuv run python -m nuitka --onefile --windows-disable-console src/myapp/main.py  # Windows\nuv run python -m nuitka --onefile --macos-create-app-bundle src/myapp/main.py  # macOS\n\n# Advanced optimization\nuv run python -m nuitka --onefile --lto=yes --plugin-enable=numpy src/myapp/main.py\n</code></pre><p></p> <p>Pros vs PyInstaller:</p> <ul> <li>\u2705 Faster execution (10-50% performance improvement)</li> <li>\u2705 Smaller file sizes (often 2-3x smaller)</li> <li>\u2705 Better startup time (no extraction needed)</li> <li>\u2705 True compilation provides some code obfuscation</li> <li>\u2705 Plugin system for optimizing specific packages</li> </ul> <p>Cons vs PyInstaller:</p> <ul> <li>\u274c Longer compile times (can be 5-10x slower)</li> <li>\u274c C++ compiler required on build machine</li> <li>\u274c Less mature ecosystem and fewer workarounds</li> <li>\u274c Some Python features not supported (eval, exec limitations)</li> <li>\u274c Debugging is harder when issues occur</li> </ul> <p>Best for: Performance-critical applications, production software where execution speed matters, applications that will be run frequently.</p>"},{"location":"resources/pyinstaller_deep_dive/#2-cx_freeze-the-cross-platform-veteran","title":"2. cx_Freeze - The Cross-Platform Veteran","text":"<p>What it is: Cross-platform freezing tool that bundles Python applications, similar to PyInstaller but with different architecture.</p> <p>Using cx_Freeze with uv: </p><pre><code># Install cx_Freeze\nuv add --dev cx_freeze\n\n# Create setup script\n# setup.py\nfrom cx_Freeze import setup, Executable\n\nbuild_options = {\n    'packages': ['requests', 'click'],\n    'excludes': ['tkinter', 'unittest'],\n    'include_files': [('data/', 'data/')],\n}\n\nexecutables = [\n    Executable('src/myapp/main.py', target_name='myapp')\n]\n\nsetup(\n    name='MyApp',\n    version='1.0',\n    description='My Application',\n    options={'build_exe': build_options},\n    executables=executables\n)\n\n# Build\nuv run python setup.py build_exe\n</code></pre><p></p> <p>Pros vs PyInstaller: - \u2705 Simpler architecture - easier to understand - \u2705 Good cross-platform support - \u2705 Fine-grained control over what gets included - \u2705 Modular design - can bundle specific modules only</p> <p>Cons vs PyInstaller: - \u274c Less automatic dependency detection - \u274c More manual configuration required - \u274c Smaller community and fewer resources - \u274c Less sophisticated hiding of imports</p> <p>Best for: Developers who want more control over the bundling process, projects with well-understood dependencies.</p>"},{"location":"resources/pyinstaller_deep_dive/#3-auto-py-to-exe-the-gui-wrapper","title":"3. auto-py-to-exe - The GUI Wrapper","text":"<p>What it is: A graphical user interface wrapper around PyInstaller that makes it easier to configure builds.</p> <p>Using auto-py-to-exe with uv: </p><pre><code># Install auto-py-to-exe\nuv add --dev auto-py-to-exe\n\n# Launch GUI\nuv run auto-py-to-exe\n\n# Or use JSON configuration\nuv run auto-py-to-exe --config config.json\n</code></pre><p></p> <p>Pros vs PyInstaller: - \u2705 User-friendly GUI for beginners - \u2705 Visual configuration of build options - \u2705 JSON export for reproducible builds - \u2705 All PyInstaller features available</p> <p>Cons vs PyInstaller: - \u274c Additional dependency for simple projects - \u274c Less suitable for automation and CI/CD - \u274c GUI overhead for experienced users</p> <p>Best for: Beginners, one-off builds, developers who prefer GUI tools.</p>"},{"location":"resources/pyinstaller_deep_dive/#4-py2exe-windows-specialist","title":"4. py2exe - Windows Specialist","text":"<p>What it is: Windows-only tool for creating executable files from Python scripts.</p> <p>Using py2exe with uv: </p><pre><code># Install py2exe (Windows only)\nuv add --dev py2exe\n\n# Create setup script\n# setup.py\nfrom distutils.core import setup\nimport py2exe\n\nsetup(\n    console=['src/myapp/main.py'],\n    options={\n        'py2exe': {\n            'bundle_files': 1,\n            'compressed': True,\n            'excludes': ['tkinter'],\n        }\n    },\n    zipfile=None,\n)\n\n# Build\nuv run python setup.py py2exe\n</code></pre><p></p> <p>Pros vs PyInstaller: - \u2705 Windows-optimized with good OS integration - \u2705 Smaller executables on Windows - \u2705 Mature and stable for Windows deployment</p> <p>Cons vs PyInstaller: - \u274c Windows-only (major limitation) - \u274c Less active development - \u274c Limited Python version support</p> <p>Best for: Windows-only applications where you need optimal Windows integration.</p>"},{"location":"resources/pyinstaller_deep_dive/#5-py2app-macos-specialist","title":"5. py2app - macOS Specialist","text":"<p>What it is: macOS-only tool for creating application bundles (.app files).</p> <p>Using py2app with uv: </p><pre><code># Install py2app (macOS only)\nuv add --dev py2app\n\n# Create setup script\n# setup.py\nfrom setuptools import setup\n\nAPP = ['src/myapp/main.py']\nDATA_FILES = []\nOPTIONS = {\n    'argv_emulation': True,\n    'includes': ['requests', 'click'],\n    'excludes': ['tkinter'],\n}\n\nsetup(\n    app=APP,\n    data_files=DATA_FILES,\n    options={'py2app': OPTIONS},\n    setup_requires=['py2app'],\n)\n\n# Build\nuv run python setup.py py2app\n</code></pre><p></p> <p>Pros vs PyInstaller: - \u2705 Native macOS app bundles with proper integration - \u2705 Better macOS-specific features (Info.plist, etc.) - \u2705 Optimized for macOS deployment</p> <p>Cons vs PyInstaller: - \u274c macOS-only (major limitation) - \u274c Less flexible than PyInstaller - \u274c Steeper learning curve for complex apps</p> <p>Best for: macOS-only applications that need native app bundle features.</p>"},{"location":"resources/pyinstaller_deep_dive/#6-pyoxidizer-the-rust-powered-solution","title":"6. PyOxidizer - The Rust-Powered Solution","text":"<p>What it is: A modern tool written in Rust that embeds Python interpreters into applications.</p> <p>Using PyOxidizer with uv: </p><pre><code># Install PyOxidizer\nuv add --dev pyoxidizer\n\n# Initialize project\nuv run pyoxidizer init-rust-project myapp\n\n# Configure pyoxidizer.bzl\n# pyoxidizer.bzl\ndef make_exe():\n    config = default_python_config()\n    config.run_command = \"from myapp import main; main()\"\n\n    exe = PythonExecutable(\n        name = \"myapp\",\n        config = config,\n    )\n\n    for resource in find_resources_in_path(\"src\"):\n        exe.add_python_resource(resource)\n\n    return exe\n\ndef make_install(exe):\n    files = FileManifest()\n    files.add_python_resource(\".\", exe)\n    return files\n\nregister_target(\"exe\", make_exe)\nregister_target(\"install\", make_install, depends=[\"exe\"])\nresolve_targets()\n\n# Build\nuv run pyoxidizer build\n</code></pre><p></p> <p>Pros vs PyInstaller: - \u2705 Modern architecture built with Rust - \u2705 Flexible configuration with Python-like syntax - \u2705 Good performance and small sizes - \u2705 Custom Python distributions possible</p> <p>Cons vs PyInstaller: - \u274c Complex setup and configuration - \u274c Smaller community and fewer examples - \u274c Rust knowledge helpful for advanced usage - \u274c Less mature than PyInstaller</p> <p>Best for: Advanced users who want cutting-edge technology and maximum control.</p>"},{"location":"resources/pyinstaller_deep_dive/#7-briefcase-the-mobile-ready-solution","title":"7. Briefcase - The Mobile-Ready Solution","text":"<p>What it is: Part of the BeeWare suite, designed for creating native applications across multiple platforms including mobile.</p> <p>Using Briefcase with uv: </p><pre><code># Install briefcase\nuv add --dev briefcase\n\n# Initialize project\nuv run briefcase new\n\n# Configure pyproject.toml\n[tool.briefcase]\nproject_name = \"My App\"\nbundle = \"com.example\"\nversion = \"0.0.1\"\ndescription = \"My Application\"\n\n[tool.briefcase.app.myapp]\nformal_name = \"My App\"\ndescription = \"My Application Description\"\nsources = [\"src/myapp\"]\nrequires = [\"requests\", \"click\"]\n\n# Build\nuv run briefcase create\nuv run briefcase build\nuv run briefcase package\n</code></pre><p></p> <p>Pros vs PyInstaller: - \u2705 Multi-platform support including mobile (iOS, Android) - \u2705 Native app packaging for each platform - \u2705 Modern toolchain and active development - \u2705 Web deployment support</p> <p>Cons vs PyInstaller: - \u274c More complex setup for simple desktop apps - \u274c Less mature for traditional desktop deployment - \u274c Requires platform-specific SDKs for mobile</p> <p>Best for: Applications targeting multiple platforms including mobile, modern cross-platform development.</p>"},{"location":"resources/pyinstaller_deep_dive/#comprehensive-comparison-table","title":"Comprehensive Comparison Table","text":"Feature PyInstaller Nuitka cx_Freeze py2exe py2app PyOxidizer Briefcase Platforms All All All Windows macOS All All + Mobile Ease of Use \u2b50\u2b50\u2b50\u2b50\u2b50 \u2b50\u2b50\u2b50 \u2b50\u2b50\u2b50 \u2b50\u2b50\u2b50 \u2b50\u2b50\u2b50 \u2b50\u2b50 \u2b50\u2b50\u2b50\u2b50 Performance \u2b50\u2b50\u2b50 \u2b50\u2b50\u2b50\u2b50\u2b50 \u2b50\u2b50\u2b50 \u2b50\u2b50\u2b50 \u2b50\u2b50\u2b50 \u2b50\u2b50\u2b50\u2b50 \u2b50\u2b50\u2b50 Binary Size \u2b50\u2b50 \u2b50\u2b50\u2b50\u2b50 \u2b50\u2b50\u2b50 \u2b50\u2b50\u2b50\u2b50 \u2b50\u2b50\u2b50 \u2b50\u2b50\u2b50\u2b50 \u2b50\u2b50\u2b50 Build Speed \u2b50\u2b50\u2b50\u2b50 \u2b50\u2b50 \u2b50\u2b50\u2b50\u2b50 \u2b50\u2b50\u2b50 \u2b50\u2b50\u2b50 \u2b50\u2b50\u2b50 \u2b50\u2b50\u2b50 Startup Time \u2b50\u2b50 \u2b50\u2b50\u2b50\u2b50\u2b50 \u2b50\u2b50\u2b50 \u2b50\u2b50\u2b50 \u2b50\u2b50\u2b50 \u2b50\u2b50\u2b50\u2b50 \u2b50\u2b50\u2b50 Community \u2b50\u2b50\u2b50\u2b50\u2b50 \u2b50\u2b50\u2b50 \u2b50\u2b50 \u2b50\u2b50 \u2b50\u2b50 \u2b50\u2b50 \u2b50\u2b50\u2b50 Documentation \u2b50\u2b50\u2b50\u2b50\u2b50 \u2b50\u2b50\u2b50\u2b50 \u2b50\u2b50\u2b50 \u2b50\u2b50\u2b50 \u2b50\u2b50\u2b50 \u2b50\u2b50\u2b50 \u2b50\u2b50\u2b50\u2b50 Auto Detection \u2b50\u2b50\u2b50\u2b50\u2b50 \u2b50\u2b50\u2b50\u2b50 \u2b50\u2b50 \u2b50\u2b50\u2b50 \u2b50\u2b50\u2b50 \u2b50\u2b50\u2b50 \u2b50\u2b50\u2b50\u2b50"},{"location":"resources/pyinstaller_deep_dive/#use-case-recommendations","title":"Use Case Recommendations","text":""},{"location":"resources/pyinstaller_deep_dive/#choose-pyinstaller-when","title":"Choose PyInstaller when:","text":"<ul> <li>\ud83c\udfaf General-purpose desktop applications</li> <li>\ud83d\udd04 Cross-platform deployment required</li> <li>\ud83d\udc65 Large development team (good docs/community)</li> <li>\u26a1 Fast development cycles needed</li> <li>\ud83d\udce6 Complex dependencies (many packages)</li> <li>\ud83d\udee0 Mature toolchain required</li> </ul>"},{"location":"resources/pyinstaller_deep_dive/#choose-nuitka-when","title":"Choose Nuitka when:","text":"<ul> <li>\ud83d\ude80 Performance is critical</li> <li>\ud83d\udce6 Smaller executable size needed</li> <li>\ud83d\udd12 Code obfuscation desired</li> <li>\ud83d\udcb0 Commercial applications (worth the compile time)</li> <li>\ud83c\udfae Games or real-time applications</li> </ul>"},{"location":"resources/pyinstaller_deep_dive/#choose-cx_freeze-when","title":"Choose cx_Freeze when:","text":"<ul> <li>\ud83c\udf9b Fine control over bundling process</li> <li>\ud83d\udcda Well-understood dependencies</li> <li>\ud83d\udd27 Custom build processes needed</li> <li>\ud83d\udc68\u200d\ud83d\udcbb Experienced with packaging</li> </ul>"},{"location":"resources/pyinstaller_deep_dive/#choose-platform-specific-tools-when","title":"Choose Platform-Specific Tools when:","text":"<ul> <li>\ud83e\ude9f py2exe: Windows-only with tight OS integration</li> <li>\ud83c\udf4e py2app: macOS-only with native app bundles</li> <li>\ud83d\udcf1 Briefcase: Need mobile deployment</li> </ul>"},{"location":"resources/pyinstaller_deep_dive/#choose-pyoxidizer-when","title":"Choose PyOxidizer when:","text":"<ul> <li>\ud83e\udd80 Cutting-edge technology acceptable</li> <li>\ud83c\udf9b Maximum control over Python distribution</li> <li>\ud83d\udce6 Custom Python builds needed</li> <li>\ud83d\udd27 Advanced packaging requirements</li> </ul>"},{"location":"resources/pyinstaller_deep_dive/#performance-comparison","title":"Performance Comparison","text":""},{"location":"resources/pyinstaller_deep_dive/#real-world-benchmarks","title":"Real-World Benchmarks","text":"<p>Test Application: Flask web app with requests, click, rich dependencies</p> Tool Build Time Binary Size Startup Time Runtime Performance PyInstaller 45s 28 MB 2.1s Baseline Nuitka 3m 20s 12 MB 0.3s +25% faster cx_Freeze 1m 10s 25 MB 1.8s Baseline py2exe 55s 18 MB 1.5s Baseline <p>Memory Usage During Build:</p> Tool Peak Memory Disk Space (temp) PyInstaller 450 MB 120 MB Nuitka 1.2 GB 300 MB cx_Freeze 320 MB 80 MB"},{"location":"resources/pyinstaller_deep_dive/#advanced-hybrid-approach","title":"Advanced Hybrid Approach","text":"<p>You can combine tools for optimal results:</p> <pre><code># Use uv for dependency management\nuv sync\n\n# Try multiple tools and compare\nuv run pyinstaller --onefile src/app/main.py\nuv run python -m nuitka --onefile src/app/main.py\n\n# Benchmark and choose the best for your use case\n</code></pre> <p>Multi-tool build script: </p><pre><code>#!/usr/bin/env python3\n\"\"\"\nCompare multiple packaging tools\n\"\"\"\nimport subprocess\nimport time\nimport os\nfrom pathlib import Path\n\ndef benchmark_tool(tool_name, build_command, binary_path):\n    \"\"\"Benchmark a packaging tool\"\"\"\n    print(f\"\ud83d\udd28 Testing {tool_name}...\")\n\n    # Clean previous builds\n    if Path(binary_path).exists():\n        os.remove(binary_path)\n\n    # Measure build time\n    start_time = time.time()\n    result = subprocess.run(build_command, shell=True, capture_output=True)\n    build_time = time.time() - start_time\n\n    if result.returncode != 0:\n        print(f\"\u274c {tool_name} build failed\")\n        return None\n\n    # Measure binary size\n    if Path(binary_path).exists():\n        binary_size = Path(binary_path).stat().st_size / (1024 * 1024)  # MB\n    else:\n        print(f\"\u274c {tool_name} binary not found\")\n        return None\n\n    # Measure startup time\n    start_time = time.time()\n    startup_result = subprocess.run([binary_path, '--help'], \n                                  capture_output=True, timeout=30)\n    startup_time = time.time() - start_time\n\n    return {\n        'tool': tool_name,\n        'build_time': build_time,\n        'binary_size': binary_size,\n        'startup_time': startup_time,\n        'success': startup_result.returncode == 0\n    }\n\ndef main():\n    tools = [\n        ('PyInstaller', 'uv run pyinstaller --onefile src/app/main.py', 'dist/main'),\n        ('Nuitka', 'uv run python -m nuitka --onefile src/app/main.py', 'main.bin'),\n        ('cx_Freeze', 'uv run python setup_cx.py build_exe', 'build/exe.*/main'),\n    ]\n\n    results = []\n    for tool_name, command, binary_path in tools:\n        result = benchmark_tool(tool_name, command, binary_path)\n        if result:\n            results.append(result)\n\n    # Display comparison\n    print(\"\\n\ud83d\udcca Comparison Results:\")\n    print(f\"{'Tool':&lt;12} {'Build Time':&lt;12} {'Size (MB)':&lt;10} {'Startup (s)':&lt;12}\")\n    print(\"-\" * 50)\n\n    for result in results:\n        print(f\"{result['tool']:&lt;12} {result['build_time']:&lt;12.1f} \"\n              f\"{result['binary_size']:&lt;10.1f} {result['startup_time']:&lt;12.2f}\")\n\nif __name__ == \"__main__\":\n    main()\n</code></pre><p></p>"},{"location":"resources/pyinstaller_deep_dive/#linux-distribution-compatibility","title":"Linux Distribution Compatibility","text":""},{"location":"resources/pyinstaller_deep_dive/#understanding-linux-binary-dependencies","title":"Understanding Linux Binary Dependencies","text":"<p>Critical Issue: PyInstaller binaries on Linux ARE dependent on the distribution and glibc version where they're built.</p> <p>The Core Problem: PyInstaller does not bundle libc (the C standard library, usually glibc) with the app. Instead, the app expects to link dynamically to the libc from the local OS where it runs. The interface between any app and libc is forward compatible to newer releases, but it is not backward compatible to older releases.</p> <p>What this means: - \u2705 Forward compatible: Binary built on Ubuntu 18.04 (glibc 2.27) \u2192 runs on Ubuntu 22.04 (glibc 2.35) - \u274c NOT backward compatible: Binary built on Ubuntu 22.04 (glibc 2.35) \u2192 fails on Ubuntu 18.04 (glibc 2.27)</p>"},{"location":"resources/pyinstaller_deep_dive/#common-error-messages","title":"Common Error Messages","text":"<pre><code># Typical glibc version error\nError loading Python lib '/tmp/_MEI.../libpython3.8.so.1.0': \n/lib/x86_64-linux-gnu/libc.so.6: version `GLIBC_2.25' not found\n\n# Architecture mismatch error\ncannot execute binary file: Exec format error\n\n# Missing shared library error\nerror while loading shared libraries: libz.so.1: \nfailed to map segment from shared object\n</code></pre>"},{"location":"resources/pyinstaller_deep_dive/#distribution-compatibility-matrix","title":"Distribution Compatibility Matrix","text":"Build System glibc Version Compatible Target Systems CentOS 7 2.17 CentOS 7+, RHEL 7+, Ubuntu 16.04+, Debian 9+ Ubuntu 18.04 2.27 Ubuntu 18.04+, Debian 10+, CentOS 8+ Ubuntu 20.04 2.31 Ubuntu 20.04+, Debian 11+, Fedora 32+ Ubuntu 22.04 2.35 Ubuntu 22.04+, Debian 12+, Fedora 36+ Ubuntu 24.04 2.39 Ubuntu 24.04+, Debian 13+, Fedora 40+"},{"location":"resources/pyinstaller_deep_dive/#architecture-dependencies","title":"Architecture Dependencies","text":"<p>The GNU/Linux standard libraries such as glibc are distributed in 64-bit and 32-bit versions, and these are not compatible:</p> <ul> <li>x86_64 (64-bit Intel/AMD) \u2190 Most common</li> <li>aarch64 (64-bit ARM) \u2190 Growing (Apple M1, AWS Graviton)</li> <li>armv7l (32-bit ARM) \u2190 Raspberry Pi, IoT devices</li> <li>i386 (32-bit Intel) \u2190 Legacy systems</li> </ul> <p>Rule: You cannot bundle your app on a 32-bit system and run it on a 64-bit installation, nor vice-versa.</p>"},{"location":"resources/pyinstaller_deep_dive/#linux-compatibility-solutions","title":"Linux Compatibility Solutions","text":""},{"location":"resources/pyinstaller_deep_dive/#1-build-on-oldest-target-system-recommended","title":"1. Build on Oldest Target System (Recommended)","text":"<p>Strategy: Always build your app on the oldest version of GNU/Linux you mean to support.</p> <pre><code># For wide compatibility: Use CentOS 7 or Ubuntu 18.04\n# For modern systems: Use Ubuntu 20.04+\n# For latest features: Use current Ubuntu/Fedora\n\n# Example: Building for CentOS 7 compatibility\nuv sync\nuv run pyinstaller --onefile src/app/main.py\n</code></pre>"},{"location":"resources/pyinstaller_deep_dive/#2-docker-based-multi-distro-builds","title":"2. Docker-Based Multi-Distro Builds","text":"<p>Create compatibility-focused Docker images:</p> <pre><code># Dockerfile.ubuntu18-uv (for wide compatibility)\nFROM ubuntu:18.04\n\n# Install uv\nRUN apt-get update &amp;&amp; apt-get install -y curl software-properties-common binutils\nRUN curl -LsSf https://astral.sh/uv/install.sh | sh\nENV PATH=\"/root/.local/bin:$PATH\"\n\n# Add deadsnakes PPA for newer Python on old Ubuntu\nRUN add-apt-repository ppa:deadsnakes/ppa\nRUN apt-get update &amp;&amp; apt-get install -y \\\n    python3.11 \\\n    python3.11-dev \\\n    python3.11-venv\n\nWORKDIR /app\nCOPY . .\n\n# Build with uv\nRUN uv python install 3.11\nRUN uv sync\nRUN uv run pyinstaller --onefile --strip src/app/main.py\n\n# Test the binary\nRUN ./dist/app --version\n</code></pre> <pre><code># Dockerfile.centos7-uv (for maximum compatibility)\nFROM centos:7\n\n# Install dependencies\nRUN yum update -y &amp;&amp; yum install -y curl gcc binutils\n\n# Install uv\nRUN curl -LsSf https://astral.sh/uv/install.sh | sh\nENV PATH=\"/root/.local/bin:$PATH\"\n\n# Install Python 3.9 via Software Collections\nRUN yum install -y centos-release-scl\nRUN yum install -y rh-python39 rh-python39-python-devel\nRUN echo \"source /opt/rh/rh-python39/enable\" &gt;&gt; ~/.bashrc\n\nWORKDIR /app\nCOPY . .\n\n# Build with uv (note: need to source SCL environment)\nRUN source /opt/rh/rh-python39/enable &amp;&amp; \\\n    uv python install 3.9 &amp;&amp; \\\n    uv sync &amp;&amp; \\\n    uv run pyinstaller --onefile src/app/main.py\n</code></pre> <p>Build with Docker: </p><pre><code># Build for wide compatibility\ndocker build -f Dockerfile.ubuntu18-uv -t myapp-ubuntu18 .\ndocker run --rm -v $(pwd)/dist:/app/dist myapp-ubuntu18\n\n# Build for maximum compatibility\ndocker build -f Dockerfile.centos7-uv -t myapp-centos7 .\ndocker run --rm -v $(pwd)/dist:/app/dist myapp-centos7\n</code></pre><p></p>"},{"location":"resources/pyinstaller_deep_dive/#3-compatibility-check-script","title":"3. Compatibility Check Script","text":"<p>build_linux_compatible.py: </p><pre><code>#!/usr/bin/env python3\n\"\"\"\nLinux-compatible PyInstaller build script using uv\n\"\"\"\nimport subprocess\nimport platform\nimport sys\nfrom pathlib import Path\n\ndef get_glibc_version():\n    \"\"\"Get current glibc version\"\"\"\n    try:\n        result = subprocess.run(['ldd', '--version'], \n                              capture_output=True, text=True)\n        for line in result.stdout.split('\\n'):\n            if 'ldd' in line and 'GNU libc' in line:\n                version = line.split()[-1]\n                return version\n    except:\n        pass\n    return \"unknown\"\n\ndef check_compatibility():\n    \"\"\"Check system compatibility for distribution\"\"\"\n    print(\"\ud83d\udc27 Linux Distribution Compatibility Check\")\n    print(f\"System: {platform.system()} {platform.release()}\")\n    print(f\"Architecture: {platform.machine()}\")\n    print(f\"glibc version: {get_glibc_version()}\")\n\n    # Common glibc versions by distro\n    glibc_distros = {\n        \"2.17\": \"CentOS 7, RHEL 7\",\n        \"2.23\": \"Ubuntu 16.04\",\n        \"2.27\": \"Ubuntu 18.04\", \n        \"2.31\": \"Ubuntu 20.04\",\n        \"2.35\": \"Ubuntu 22.04\",\n        \"2.39\": \"Ubuntu 24.04\"\n    }\n\n    current_version = get_glibc_version()\n    print(f\"\\n\ud83d\udccb Target compatibility:\")\n    print(f\"Building with glibc {current_version}\")\n    print(\"Will be compatible with:\")\n\n    for version, distros in glibc_distros.items():\n        if version &gt;= current_version:\n            print(f\"  \u2705 {distros} (glibc {version})\")\n        else:\n            print(f\"  \u274c {distros} (glibc {version})\")\n\ndef test_on_multiple_distros(binary_path):\n    \"\"\"Test binary on multiple distributions using Docker\"\"\"\n    test_distros = [\n        \"ubuntu:18.04\",\n        \"ubuntu:20.04\", \n        \"ubuntu:22.04\",\n        \"centos:7\",\n        \"debian:10\",\n        \"debian:11\"\n    ]\n\n    print(f\"\\n\ud83e\uddea Testing {binary_path} on multiple distributions:\")\n\n    for distro in test_distros:\n        try:\n            cmd = [\n                \"docker\", \"run\", \"--rm\", \n                \"-v\", f\"{Path.cwd()}:/app\",\n                distro, \n                f\"/app/{binary_path}\", \"--version\"\n            ]\n\n            result = subprocess.run(cmd, capture_output=True, \n                                  text=True, timeout=30)\n\n            if result.returncode == 0:\n                print(f\"  \u2705 {distro}: Working\")\n            else:\n                print(f\"  \u274c {distro}: Failed - {result.stderr.strip()}\")\n\n        except subprocess.TimeoutExpired:\n            print(f\"  \u23f0 {distro}: Timeout\")\n        except FileNotFoundError:\n            print(f\"  \u26a0\ufe0f  Docker not available for testing\")\n            break\n        except Exception as e:\n            print(f\"  \u274c {distro}: Error - {e}\")\n\ndef build_with_checks():\n    \"\"\"Build with compatibility checks\"\"\"\n    check_compatibility()\n\n    print(\"\\n\ud83d\udd28 Building with uv...\")\n\n    # Sync dependencies\n    subprocess.run([\"uv\", \"sync\"], check=True)\n\n    # Build executable with optimization flags\n    subprocess.run([\n        \"uv\", \"run\", \"pyinstaller\", \n        \"--onefile\", \n        \"--strip\",              # Remove debug symbols\n        \"--exclude-module\", \"tkinter\",  # Common exclusions for size\n        \"--exclude-module\", \"matplotlib\",\n        \"src/app/main.py\"\n    ], check=True)\n\n    # Test binary locally\n    binary_path = Path(\"dist/main\")\n    if binary_path.exists():\n        print(f\"\u2705 Binary created: {binary_path}\")\n        print(f\"\ud83d\udce6 Size: {binary_path.stat().st_size / (1024*1024):.1f} MB\")\n\n        # Test execution locally\n        try:\n            result = subprocess.run([str(binary_path), \"--version\"], \n                                  capture_output=True, timeout=10, text=True)\n            if result.returncode == 0:\n                print(\"\u2705 Local binary test passed\")\n\n                # Test on multiple distros if Docker available\n                test_on_multiple_distros(binary_path)\n\n            else:\n                print(f\"\u274c Local binary test failed: {result.stderr}\")\n        except subprocess.TimeoutExpired:\n            print(\"\u274c Binary test timed out\")\n        except Exception as e:\n            print(f\"\u274c Binary test error: {e}\")\n\nif __name__ == \"__main__\":\n    build_with_checks()\n</code></pre><p></p> <p>Usage: </p><pre><code># Check compatibility and build\nuv run python build_linux_compatible.py\n\n# Example output:\n# \ud83d\udc27 Linux Distribution Compatibility Check\n# System: Linux 5.4.0-74-generic\n# Architecture: x86_64  \n# glibc version: 2.31\n# \n# \ud83d\udccb Target compatibility:\n# Building with glibc 2.31\n# Will be compatible with:\n#   \u274c CentOS 7, RHEL 7 (glibc 2.17)\n#   \u274c Ubuntu 16.04 (glibc 2.23)\n#   \u274c Ubuntu 18.04 (glibc 2.27)\n#   \u2705 Ubuntu 20.04 (glibc 2.31)\n#   \u2705 Ubuntu 22.04 (glibc 2.35)\n</code></pre><p></p>"},{"location":"resources/pyinstaller_deep_dive/#4-github-actions-multi-distro-strategy","title":"4. GitHub Actions Multi-Distro Strategy","text":"<p>.github/workflows/build-linux-compatible.yml: </p><pre><code>name: Build Linux Compatible Binaries\n\non:\n  push:\n    branches: [main]\n    tags: ['v*']\n  pull_request:\n    branches: [main]\n\njobs:\n  build-linux:\n    runs-on: ubuntu-latest\n    strategy:\n      matrix:\n        # Build on different base systems for compatibility\n        include:\n          - container: ubuntu:18.04\n            name: \"ubuntu-18.04-glibc-2.27\"\n            python: \"3.11\"\n            compatibility: \"wide\"\n          - container: ubuntu:20.04  \n            name: \"ubuntu-20.04-glibc-2.31\"\n            python: \"3.11\"\n            compatibility: \"modern\"\n          - container: centos:7\n            name: \"centos-7-glibc-2.17\"\n            python: \"3.9\"\n            compatibility: \"maximum\"\n\n    container: ${{ matrix.container }}\n\n    steps:\n    - uses: actions/checkout@v4\n\n    - name: Install dependencies (Ubuntu)\n      if: startsWith(matrix.container, 'ubuntu')\n      run: |\n        apt-get update\n        apt-get install -y curl software-properties-common binutils\n        add-apt-repository ppa:deadsnakes/ppa\n        apt-get update\n        apt-get install -y python${{ matrix.python }} python${{ matrix.python }}-dev python${{ matrix.python }}-venv\n\n    - name: Install dependencies (CentOS)\n      if: startsWith(matrix.container, 'centos')\n      run: |\n        yum update -y\n        yum install -y curl gcc binutils centos-release-scl\n        yum install -y rh-python39 rh-python39-python-devel\n\n    - name: Install uv\n      run: curl -LsSf https://astral.sh/uv/install.sh | sh\n\n    - name: Build binary (Ubuntu)\n      if: startsWith(matrix.container, 'ubuntu')\n      run: |\n        export PATH=\"/github/home/.local/bin:$PATH\"\n        uv python install ${{ matrix.python }}\n        uv sync\n        uv run pyinstaller --onefile --strip src/app/main.py\n        mv dist/main dist/main-${{ matrix.name }}\n\n    - name: Build binary (CentOS)\n      if: startsWith(matrix.container, 'centos')\n      run: |\n        source /opt/rh/rh-python39/enable\n        export PATH=\"/github/home/.local/bin:$PATH\"\n        uv python install ${{ matrix.python }}\n        uv sync\n        uv run pyinstaller --onefile --strip src/app/main.py\n        mv dist/main dist/main-${{ matrix.name }}\n\n    - name: Test binary and show info\n      run: |\n        ./dist/main-${{ matrix.name }} --version\n        echo \"glibc version:\"\n        ldd --version | head -1\n        echo \"Binary size:\"\n        ls -lh dist/main-${{ matrix.name }}\n\n    - name: Upload binary\n      uses: actions/upload-artifact@v4\n      with:\n        name: binary-${{ matrix.name }}\n        path: dist/main-${{ matrix.name }}\n        retention-days: 30\n\n    - name: Create Release\n      if: startsWith(github.ref, 'refs/tags/') &amp;&amp; matrix.compatibility == 'maximum'\n      uses: softprops/action-gh-release@v1\n      with:\n        files: dist/main-${{ matrix.name }}\n        name: Release ${{ github.ref_name }}\n        body: |\n          ## Linux Compatibility\n\n          - **maximum**: Built on CentOS 7 (glibc 2.17) - works on most Linux systems\n          - **wide**: Built on Ubuntu 18.04 (glibc 2.27) - works on modern systems  \n          - **modern**: Built on Ubuntu 20.04 (glibc 2.31) - latest systems only\n\n          Choose the binary with the best compatibility for your target systems.\n</code></pre><p></p>"},{"location":"resources/pyinstaller_deep_dive/#5-special-considerations","title":"5. Special Considerations","text":""},{"location":"resources/pyinstaller_deep_dive/#red-hat-systems-and-tmp-execution","title":"Red Hat Systems and <code>/tmp</code> Execution","text":"<p>Some Red Hat-based systems don't allow execution from <code>/tmp</code> as a security measure, which affects PyInstaller's onefile mode:</p> <pre><code># Error on RHEL/CentOS\nerror while loading shared libraries: libz.so.1: \nfailed to map segment from shared object\n\n# Solution: Override temp directory\nexport _MEIPASS2=/path/to/executable/temp\n./myapp\n\n# Or use onedir mode instead of onefile\nuv run pyinstaller --onedir src/app/main.py\n</code></pre>"},{"location":"resources/pyinstaller_deep_dive/#static-linking-with-staticx","title":"Static Linking with staticx","text":"<p>For maximum compatibility, you can post-process PyInstaller binaries:</p> <pre><code># Install staticx\nuv add --dev staticx\n\n# Build with PyInstaller first\nuv run pyinstaller --onefile src/app/main.py\n\n# Post-process with staticx for static linking\nuv run staticx dist/main dist/main-static\n\n# Test static binary\n./dist/main-static --version\n</code></pre>"},{"location":"resources/pyinstaller_deep_dive/#6-testing-strategy","title":"6. Testing Strategy","text":"<p>Comprehensive testing approach:</p> <pre><code># Test locally built binary on multiple distros\ndocker run --rm -v $(pwd):/app ubuntu:18.04 /app/dist/main --version\ndocker run --rm -v $(pwd):/app ubuntu:20.04 /app/dist/main --version  \ndocker run --rm -v $(pwd):/app ubuntu:22.04 /app/dist/main --version\ndocker run --rm -v $(pwd):/app centos:7 /app/dist/main --version\ndocker run --rm -v $(pwd):/app debian:10 /app/dist/main --version\ndocker run --rm -v $(pwd):/app fedora:35 /app/dist/main --version\n\n# Check dependencies\nldd dist/main\nobjdump -p dist/main | grep NEEDED\n\n# Check glibc version requirements\nobjdump -T dist/main | grep GLIBC\n</code></pre>"},{"location":"resources/pyinstaller_deep_dive/#linux-compatibility-best-practices","title":"Linux Compatibility Best Practices","text":""},{"location":"resources/pyinstaller_deep_dive/#1-choose-build-strategy-based-on-target","title":"1. Choose Build Strategy Based on Target","text":"<ul> <li>Maximum Compatibility: Build on CentOS 7 (glibc 2.17)</li> <li>Wide Compatibility: Build on Ubuntu 18.04 (glibc 2.27)  </li> <li>Modern Systems: Build on Ubuntu 20.04+ (glibc 2.31+)</li> </ul>"},{"location":"resources/pyinstaller_deep_dive/#2-optimization-for-linux","title":"2. Optimization for Linux","text":"<pre><code># Linux-optimized build flags\nuv run pyinstaller \\\n    --onefile \\\n    --strip \\                    # Remove debug symbols\n    --exclude-module tkinter \\   # Exclude GUI if not needed\n    --exclude-module test \\      # Exclude test modules\n    src/app/main.py\n</code></pre>"},{"location":"resources/pyinstaller_deep_dive/#3-size-and-performance-considerations","title":"3. Size and Performance Considerations","text":"<pre><code># Check binary dependencies\nldd dist/myapp\n\n# Analyze size contributors  \nuv run pyinstaller --onefile --analyze src/app/main.py\n\n# Consider onedir for faster startup on slower systems\nuv run pyinstaller --onedir src/app/main.py\n</code></pre>"},{"location":"resources/pyinstaller_deep_dive/#4-distribution-recommendations","title":"4. Distribution Recommendations","text":"<ol> <li>Provide multiple binaries for different compatibility levels</li> <li>Clearly document glibc requirements in releases</li> <li>Test on actual target systems before release</li> <li>Consider AppImage for universal Linux distribution</li> <li>Use semantic versioning for binary compatibility</li> </ol> <p>The combination of uv's fast dependency management with proper Linux compatibility strategies ensures your PyInstaller binaries work reliably across diverse Linux environments.</p>"},{"location":"resources/python-dictionary-cheatsheet/","title":"Python Dictionary Methods Cheatsheet","text":""},{"location":"resources/python-dictionary-cheatsheet/#python-dictionary-methods-cheatsheet","title":"Python Dictionary Methods Cheatsheet","text":"<p>Dictionaries are versatile data structures in Python that store key-value pairs. This cheatsheet covers all essential dictionary methods with clear examples and explanations.</p>"},{"location":"resources/python-dictionary-cheatsheet/#creating-dictionaries","title":"Creating Dictionaries","text":"<p>There are several ways to create dictionaries in Python:</p> <pre><code># Empty dictionary\nempty_dict = {}\nempty_dict = dict()\n\n# Dictionary with initial values\nuser = {\"name\": \"John\", \"age\": 30, \"is_admin\": False}\n\n# Using dict() constructor\nuser = dict(name=\"John\", age=30, is_admin=False)\n\n# From a list of tuples\nuser = dict([(\"name\", \"John\"), (\"age\", 30), (\"is_admin\", False)])\n\n# Dictionary comprehension\nsquares = {x: x*x for x in range(6)}  # {0: 0, 1: 1, 2: 4, 3: 9, 4: 16, 5: 25}\n</code></pre>"},{"location":"resources/python-dictionary-cheatsheet/#basic-operations","title":"Basic Operations","text":""},{"location":"resources/python-dictionary-cheatsheet/#accessing-values","title":"Accessing Values","text":"<pre><code>user = {\"name\": \"John\", \"age\": 30, \"is_admin\": False}\n\n# Using square brackets (raises KeyError if key doesn't exist)\nname = user[\"name\"]  # \"John\"\n\n# Using get() (returns None or a default value if key doesn't exist)\nage = user.get(\"age\")  # 30\nscore = user.get(\"score\")  # None\nrole = user.get(\"role\", \"user\")  # \"user\" (default value)\n</code></pre>"},{"location":"resources/python-dictionary-cheatsheet/#addingupdating-items","title":"Adding/Updating Items","text":"<pre><code>user = {\"name\": \"John\", \"age\": 30}\n\n# Using square brackets\nuser[\"email\"] = \"john@example.com\"  # Adds new key-value pair\nuser[\"age\"] = 31  # Updates existing value\n\n# Using update() - add/update multiple key-value pairs\nuser.update({\"phone\": \"555-1234\", \"age\": 32})\nuser.update(country=\"USA\", zip=\"10001\")\n</code></pre>"},{"location":"resources/python-dictionary-cheatsheet/#removing-items","title":"Removing Items","text":"<pre><code>user = {\"name\": \"John\", \"age\": 30, \"email\": \"john@example.com\", \"phone\": \"555-1234\"}\n\n# pop() - removes item and returns its value\nemail = user.pop(\"email\")  # Returns \"john@example.com\" and removes the key-value pair\nnonexistent = user.pop(\"address\", \"Not found\")  # Returns \"Not found\" (default value)\n\n# popitem() - removes and returns the last inserted key-value pair\nlast_item = user.popitem()  # Returns (\"phone\", \"555-1234\") in Python 3.7+\n\n# del statement - removes specified key\ndel user[\"age\"]\n\n# clear() - removes all items\nuser.clear()  # user is now {}\n</code></pre>"},{"location":"resources/python-dictionary-cheatsheet/#dictionary-methods","title":"Dictionary Methods","text":""},{"location":"resources/python-dictionary-cheatsheet/#copying-dictionaries","title":"Copying Dictionaries","text":"<pre><code>original = {\"name\": \"John\", \"age\": 30, \"scores\": [85, 90, 78]}\n\n# Shallow copy - nested objects are references\nshallow_copy = original.copy()\nshallow_copy = dict(original)  # Alternative\n\n# Deep copy - completely independent copy\nimport copy\ndeep_copy = copy.deepcopy(original)\n\n# Demonstrating the difference\nshallow_copy[\"scores\"].append(92)  # Also affects original[\"scores\"]\ndeep_copy[\"scores\"].append(95)     # Does NOT affect original[\"scores\"]\n</code></pre>"},{"location":"resources/python-dictionary-cheatsheet/#getting-keys-values-and-items","title":"Getting Keys, Values, and Items","text":"<pre><code>user = {\"name\": \"John\", \"age\": 30, \"is_admin\": False}\n\n# Get all keys\nkeys = user.keys()  # dict_keys(['name', 'age', 'is_admin'])\n\n# Get all values\nvalues = user.values()  # dict_values(['John', 30, False])\n\n# Get all key-value pairs as tuples\nitems = user.items()  # dict_items([('name', 'John'), ('age', 30), ('is_admin', False)])\n\n# These return dynamic view objects that update when the dictionary changes\nuser[\"email\"] = \"john@example.com\"\n# Now keys contains 'email', values contains 'john@example.com', and items contains the new pair\n\n# Convert to lists if needed\nkeys_list = list(user.keys())\n</code></pre>"},{"location":"resources/python-dictionary-cheatsheet/#checking-dictionary-contents","title":"Checking Dictionary Contents","text":"<pre><code>user = {\"name\": \"John\", \"age\": 30, \"is_admin\": False}\n\n# Check if key exists\n\"name\" in user  # True\n\"email\" in user  # False\n\"email\" not in user  # True\n\n# Check for a specific value (less efficient, searches all values)\n30 in user.values()  # True\n\n# Length of dictionary\nlen(user)  # 3\n</code></pre>"},{"location":"resources/python-dictionary-cheatsheet/#dictionary-merging-and-unpacking-python-35","title":"Dictionary Merging and Unpacking (Python 3.5+)","text":"<pre><code># Using update() to merge (modifies the first dictionary)\ndict1 = {\"a\": 1, \"b\": 2}\ndict2 = {\"b\": 3, \"c\": 4}\ndict1.update(dict2)  # dict1 is now {\"a\": 1, \"b\": 3, \"c\": 4}\n\n# Using unpacking operator ** to merge (creates a new dictionary)\ndict1 = {\"a\": 1, \"b\": 2}\ndict2 = {\"b\": 3, \"c\": 4}\nmerged = {**dict1, **dict2}  # {\"a\": 1, \"b\": 3, \"c\": 4}\n\n# Python 3.9+ uses the | operator\nmerged = dict1 | dict2  # {\"a\": 1, \"b\": 3, \"c\": 4}\ndict1 |= dict2          # In-place update, equivalent to dict1.update(dict2)\n</code></pre>"},{"location":"resources/python-dictionary-cheatsheet/#default-values-with-setdefault","title":"Default Values with setdefault()","text":"<pre><code>user = {\"name\": \"John\", \"age\": 30}\n\n# If key exists, returns its value; otherwise, sets the key to default and returns default\nemail = user.setdefault(\"email\", \"not provided\")  # \"not provided\", adds to dictionary\nname = user.setdefault(\"name\", \"Unknown\")  # \"John\", doesn't change existing value\n\n# Great for counting occurrences\ntext = \"mississippi\"\nchar_count = {}\nfor char in text:\n    char_count.setdefault(char, 0)\n    char_count[char] += 1\n# char_count is {'m': 1, 'i': 4, 's': 4, 'p': 2}\n</code></pre>"},{"location":"resources/python-dictionary-cheatsheet/#dictionary-iteration","title":"Dictionary Iteration","text":"<pre><code>user = {\"name\": \"John\", \"age\": 30, \"is_admin\": False}\n\n# Iterate over keys (default)\nfor key in user:\n    print(key)  # \"name\", \"age\", \"is_admin\"\n\n# Explicit iterations\nfor key in user.keys():\n    print(key)\n\nfor value in user.values():\n    print(value)  # \"John\", 30, False\n\nfor key, value in user.items():\n    print(f\"{key}: {value}\")\n</code></pre>"},{"location":"resources/python-dictionary-cheatsheet/#advanced-dictionary-features","title":"Advanced Dictionary Features","text":""},{"location":"resources/python-dictionary-cheatsheet/#dictionary-comprehensions","title":"Dictionary Comprehensions","text":"<pre><code># Basic dictionary comprehension\nsquares = {x: x*x for x in range(6)}\n# {0: 0, 1: 1, 2: 4, 3: 9, 4: 16, 5: 25}\n\n# With conditional logic\neven_squares = {x: x*x for x in range(10) if x % 2 == 0}\n# {0: 0, 2: 4, 4: 16, 6: 36, 8: 64}\n\n# Transforming dictionaries\nprices = {\"apple\": 0.75, \"banana\": 0.5, \"orange\": 0.8}\ncents = {fruit: int(price * 100) for fruit, price in prices.items()}\n# {\"apple\": 75, \"banana\": 50, \"orange\": 80}\n</code></pre>"},{"location":"resources/python-dictionary-cheatsheet/#fromkeys-method","title":"fromkeys() Method","text":"<pre><code># Create a dictionary with specified keys and a default value\nkeys = [\"name\", \"age\", \"email\"]\nuser = dict.fromkeys(keys, \"unknown\")\n# {\"name\": \"unknown\", \"age\": \"unknown\", \"email\": \"unknown\"}\n\n# Default value is None if not specified\nempty_user = dict.fromkeys(keys)\n# {\"name\": None, \"age\": None, \"email\": None}\n</code></pre>"},{"location":"resources/python-dictionary-cheatsheet/#ordered-dictionaries-python-37","title":"Ordered Dictionaries (Python 3.7+)","text":"<p>As of Python 3.7, regular dictionaries maintain insertion order.</p> <pre><code># Dictionaries remember the order in which keys were inserted\ncolors = {}\ncolors[\"red\"] = \"#FF0000\"\ncolors[\"green\"] = \"#00FF00\"\ncolors[\"blue\"] = \"#0000FF\"\n\nfor color in colors:\n    print(color)  # Prints \"red\", \"green\", \"blue\" in that order\n\n# For Python 3.6 and earlier, use collections.OrderedDict\nfrom collections import OrderedDict\nordered = OrderedDict([(\"red\", \"#FF0000\"), (\"green\", \"#00FF00\"), (\"blue\", \"#0000FF\")])\n</code></pre>"},{"location":"resources/python-dictionary-cheatsheet/#defaultdict-dictionaries-with-default-values","title":"defaultdict - Dictionaries with Default Values","text":"<pre><code>from collections import defaultdict\n\n# Creates a dictionary where missing keys get a default value from the specified factory\nint_dict = defaultdict(int)  # Default value is 0\nint_dict[\"a\"] += 1  # No KeyError, sets int_dict[\"a\"] to 1\n\nlist_dict = defaultdict(list)  # Default value is an empty list\nlist_dict[\"colors\"].append(\"red\")  # No KeyError, adds \"red\" to the list\n\nset_dict = defaultdict(set)  # Default value is an empty set\nset_dict[\"fruits\"].add(\"apple\")\nset_dict[\"fruits\"].add(\"banana\")\nset_dict[\"fruits\"].add(\"apple\")  # Duplicate ignored (set property)\nprint(set_dict[\"fruits\"])  # {'apple', 'banana'}\nprint(set_dict[\"vegetables\"])  # Empty set() created automatically\n\n# Example use case: grouping unique items by category\nlog_entries = [\n    (\"error\", \"Connection timeout\"),\n    (\"warning\", \"Disk space low\"),\n    (\"error\", \"Permission denied\"),\n    (\"error\", \"Connection timeout\"),  # Duplicate error\n    (\"info\", \"Process started\"),\n    (\"warning\", \"Disk space low\")     # Duplicate warning\n]\n\n# Group unique messages by log level\nlogs_by_level = defaultdict(set)\nfor level, message in log_entries:\n    logs_by_level[level].add(message)\n\n# Result: {'error': {'Connection timeout', 'Permission denied'}, \n#          'warning': {'Disk space low'}, \n#          'info': {'Process started'}}\n\n# Custom factory function\ndef get_user_template():\n    return {\"name\": \"Guest\", \"access_level\": 0}\n\nusers = defaultdict(get_user_template)\nprint(users[\"user1\"])  # {\"name\": \"Guest\", \"access_level\": 0}\n</code></pre>"},{"location":"resources/python-dictionary-cheatsheet/#counter-dictionary-for-counting","title":"Counter - Dictionary for Counting","text":"<pre><code>from collections import Counter\n\n# Count occurrences of elements\ntext = \"mississippi\"\ncounter = Counter(text)\n# Counter({'i': 4, 's': 4, 'p': 2, 'm': 1})\n\n# Most common elements\ncounter.most_common(2)  # [('i', 4), ('s', 4)] - top 2 most common\n\n# Update counter\ncounter.update(\"missouri\")\n# Counter({'i': 7, 's': 5, 'm': 2, 'p': 2, 'o': 1, 'u': 1, 'r': 1})\n\n# Arithmetic operations\nc1 = Counter(\"hello\")\nc2 = Counter(\"world\")\nprint(c1 + c2)  # Sum of counts\nprint(c1 - c2)  # Subtract counts (keeping only positive counts)\n</code></pre>"},{"location":"resources/python-dictionary-cheatsheet/#practical-dictionary-examples","title":"Practical Dictionary Examples","text":""},{"location":"resources/python-dictionary-cheatsheet/#grouping-data","title":"Grouping Data","text":"<pre><code># Group people by their role\npeople = [\n    {\"name\": \"Alice\", \"role\": \"developer\"},\n    {\"name\": \"Bob\", \"role\": \"manager\"},\n    {\"name\": \"Charlie\", \"role\": \"developer\"},\n    {\"name\": \"Diana\", \"role\": \"designer\"}\n]\n\nroles = {}\nfor person in people:\n    role = person[\"role\"]\n    if role not in roles:\n        roles[role] = []\n    roles[role].append(person[\"name\"])\n\n# roles is {'developer': ['Alice', 'Charlie'], 'manager': ['Bob'], 'designer': ['Diana']}\n\n# Cleaner with defaultdict\nfrom collections import defaultdict\nroles = defaultdict(list)\nfor person in people:\n    roles[person[\"role\"]].append(person[\"name\"])\n</code></pre>"},{"location":"resources/python-dictionary-cheatsheet/#memoization-caching-results","title":"Memoization (Caching Results)","text":"<pre><code># Cache results of expensive function calls\nfibonacci_cache = {}\n\ndef fibonacci(n):\n    # If we have cached the result, return it\n    if n in fibonacci_cache:\n        return fibonacci_cache[n]\n\n    # Otherwise calculate it\n    if n &lt;= 1:\n        result = n\n    else:\n        result = fibonacci(n-1) + fibonacci(n-2)\n\n    # Cache the result before returning\n    fibonacci_cache[n] = result\n    return result\n\n# Calculate fibonacci(30) much faster with caching\nfibonacci(30)  # This would be very slow without caching\n</code></pre>"},{"location":"resources/python-dictionary-cheatsheet/#transforming-lists-to-dictionaries","title":"Transforming Lists to Dictionaries","text":"<pre><code># List of tuples to dictionary\nitems = [(\"a\", 1), (\"b\", 2), (\"c\", 3)]\nitem_dict = dict(items)  # {\"a\": 1, \"b\": 2, \"c\": 3}\n\n# Two parallel lists to dictionary\nkeys = [\"name\", \"age\", \"job\"]\nvalues = [\"Alice\", 28, \"Developer\"]\nuser_dict = dict(zip(keys, values))\n# {\"name\": \"Alice\", \"age\": 28, \"job\": \"Developer\"}\n\n# Creating a lookup table\nusers = [\n    {\"id\": 1, \"name\": \"Alice\"},\n    {\"id\": 2, \"name\": \"Bob\"},\n    {\"id\": 3, \"name\": \"Charlie\"}\n]\nuser_lookup = {user[\"id\"]: user for user in users}\n# Quick access by ID: user_lookup[2][\"name\"] == \"Bob\"\n</code></pre>"},{"location":"resources/python-dictionary-cheatsheet/#sorting-dictionaries","title":"Sorting Dictionaries","text":"<pre><code>grades = {\"Alice\": 85, \"Bob\": 92, \"Charlie\": 78, \"Diana\": 95}\n\n# Sort by keys\nsorted_by_name = dict(sorted(grades.items()))\n# {'Alice': 85, 'Bob': 92, 'Charlie': 78, 'Diana': 95}\n\n# Sort by values (ascending)\nsorted_by_grade = dict(sorted(grades.items(), key=lambda item: item[1]))\n# {'Charlie': 78, 'Alice': 85, 'Bob': 92, 'Diana': 95}\n\n# Sort by values (descending)\ntop_students = dict(sorted(grades.items(), key=lambda item: item[1], reverse=True))\n# {'Diana': 95, 'Bob': 92, 'Alice': 85, 'Charlie': 78}\n</code></pre>"},{"location":"resources/python-dictionary-cheatsheet/#performance-tips","title":"Performance Tips","text":"<ol> <li> <p>Dictionary lookups are fast: Accessing keys has O(1) average time complexity.</p> </li> <li> <p>Use <code>in</code> operator to check for key existence instead of catching KeyError:    </p><pre><code># Faster and cleaner\nif key in my_dict:\n    value = my_dict[key]\n</code></pre><p></p> </li> <li> <p>Use get() with default rather than checking and setting:    </p><pre><code># Instead of\nif key not in my_dict:\n    my_dict[key] = default_value\nvalue = my_dict[key]\n\n# Use\nvalue = my_dict.get(key, default_value)\n</code></pre><p></p> </li> <li> <p>Use dictionary comprehensions instead of loops for transformations:    </p><pre><code># Faster than manual loop\nsquared = {k: v**2 for k, v in numbers.items()}\n</code></pre><p></p> </li> <li> <p>Consider specialized dictionary types for specific use cases:</p> </li> <li><code>defaultdict</code> for automatic default values</li> <li><code>Counter</code> for counting occurrences</li> <li><code>OrderedDict</code> for maintaining insertion order (pre-Python 3.7)</li> </ol>"},{"location":"resources/python-lists-cheatsheet/","title":"Python Lists Cheatsheet","text":""},{"location":"resources/python-lists-cheatsheet/#python-lists-cheatsheet","title":"Python Lists Cheatsheet","text":"<p>Lists are one of Python's most versatile and commonly used data structures. They store ordered collections of items that can be of any type. This cheatsheet covers all essential list operations and methods with clear examples and explanations.</p>"},{"location":"resources/python-lists-cheatsheet/#creating-lists","title":"Creating Lists","text":"<pre><code># Empty list\nempty_list = []\nempty_list = list()\n\n# List with initial values\nnumbers = [1, 2, 3, 4, 5]\nmixed = [1, \"hello\", 3.14, True, [1, 2]]\n\n# Using the list() constructor\nchars = list(\"hello\")  # ['h', 'e', 'l', 'l', 'o']\n\n# List of repeated values\nzeros = [0] * 5  # [0, 0, 0, 0, 0]\nrepeated = [\"ab\"] * 3  # ['ab', 'ab', 'ab']\n\n# Using range\nrange_list = list(range(5))  # [0, 1, 2, 3, 4]\neven_numbers = list(range(0, 10, 2))  # [0, 2, 4, 6, 8]\n\n# List comprehension\nsquares = [x**2 for x in range(5)]  # [0, 1, 4, 9, 16]\neven_squares = [x**2 for x in range(10) if x % 2 == 0]  # [0, 4, 16, 36, 64]\n</code></pre>"},{"location":"resources/python-lists-cheatsheet/#accessing-elements","title":"Accessing Elements","text":"<pre><code>fruits = [\"apple\", \"banana\", \"cherry\", \"date\", \"elderberry\"]\n\n# Indexing (0-based)\nfirst = fruits[0]  # \"apple\"\nlast = fruits[-1]  # \"elderberry\"\n\n# Slicing [start:stop:step]\nfirst_three = fruits[0:3]  # [\"apple\", \"banana\", \"cherry\"]\nfirst_three = fruits[:3]   # Same as above (start defaults to 0)\nlast_three = fruits[2:]    # [\"cherry\", \"date\", \"elderberry\"] (stop defaults to length)\ncopy_list = fruits[:]      # Creates a shallow copy of the list\n\n# Extended slicing\nevery_second = fruits[::2]  # [\"apple\", \"cherry\", \"elderberry\"]\nreversed_list = fruits[::-1]  # [\"elderberry\", \"date\", \"cherry\", \"banana\", \"apple\"]\n</code></pre>"},{"location":"resources/python-lists-cheatsheet/#understanding-slice-notation-startstopstep","title":"Understanding Slice Notation [start:stop:step]","text":"<p>The slice notation <code>[start:stop:step]</code> is a powerful feature for accessing portions of lists:</p> <pre><code>numbers = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n\n# Slice syntax: list[start:stop:step]\n# - start: the index to start from (inclusive)\n# - stop: the index to stop before (exclusive)\n# - step: the stride between elements\n\n# Important: Slicing ALWAYS creates a new list with copied elements\noriginal = [1, 2, 3, 4, 5]\nsliced = original[1:4]  # Creates a new list [2, 3, 4]\noriginal[1] = 20        # Modifying original doesn't affect sliced\nprint(sliced)           # Still [2, 3, 4]\n\n# Basic slice with all parameters\nsubset = numbers[1:8:2]  # [1, 3, 5, 7]\n# Starts at index 1, stops before index 8, takes every 2nd element\n\n# Omitting parameters (uses defaults)\nfirst_five = numbers[:5]    # [0, 1, 2, 3, 4] (start defaults to 0)\nfrom_three = numbers[3:]    # [3, 4, 5, 6, 7, 8, 9] (stop defaults to len(numbers))\nevery_third = numbers[::3]  # [0, 3, 6, 9] (start=0, stop=len(numbers))\n\n# Negative indices count from the end\nlast_three = numbers[-3:]   # [7, 8, 9] (start at 3rd from end)\nexcept_last_two = numbers[:-2]  # [0, 1, 2, 3, 4, 5, 6, 7] (stop 2 from end)\n\n# Negative step reverses the slice direction\nreversed_numbers = numbers[::-1]  # [9, 8, 7, 6, 5, 4, 3, 2, 1, 0]\nreversed_subset = numbers[7:2:-1]  # [7, 6, 5, 4, 3] (start at 7, stop before 2, go backwards)\nlast_three_reversed = numbers[-1:-4:-1]  # [9, 8, 7]\n\n# Empty slices\nempty1 = numbers[5:5]      # [] (start equals stop)\nempty2 = numbers[7:3]      # [] (start &gt; stop with positive step)\nempty3 = numbers[3:7:-1]   # [] (start &lt; stop with negative step)\n\n# Slice bounds are automatically adjusted if out of range\nlarge_stop = numbers[5:100]  # [5, 6, 7, 8, 9] (stop is truncated to list length)\nnegative_start = numbers[-100:3]  # [0, 1, 2] (start is truncated to list beginning)\n\n# Memory usage consideration:\n# Since slicing creates a new list, be cautious with very large lists\n# If you don't need a copy, consider using itertools.islice for iteration only\nimport itertools\nfor item in itertools.islice(large_list, 1, 8, 2):\n    process(item)  # More memory efficient than looping through a slice\n</code></pre>"},{"location":"resources/python-lists-cheatsheet/#common-slice-patterns","title":"Common Slice Patterns","text":"<pre><code>items = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n\n# Get first n elements\nfirst_3 = items[:3]  # [0, 1, 2]\n\n# Get last n elements\nlast_3 = items[-3:]  # [7, 8, 9]\n\n# Get all except first n elements\nskip_first_2 = items[2:]  # [2, 3, 4, 5, 6, 7, 8, 9]\n\n# Get all except last n elements\nskip_last_2 = items[:-2]  # [0, 1, 2, 3, 4, 5, 6, 7]\n\n# Reverse a list\nreversed_items = items[::-1]  # [9, 8, 7, 6, 5, 4, 3, 2, 1, 0]\n\n# Every nth element\nevery_3rd = items[::3]  # [0, 3, 6, 9]\n\n# Every nth element, starting from mth\nfrom_2nd_every_3rd = items[1::3]  # [1, 4, 7]\n\n# Middle slice\nmiddle = items[3:7]  # [3, 4, 5, 6]\n\n# Reversing a specific section\nreversed_section = items[:5] + items[5:8][::-1] + items[8:]\n# [0, 1, 2, 3, 4, 7, 6, 5, 8, 9]\n</code></pre>"},{"location":"resources/python-lists-cheatsheet/#modifying-lists","title":"Modifying Lists","text":""},{"location":"resources/python-lists-cheatsheet/#adding-elements","title":"Adding Elements","text":"<pre><code>fruits = [\"apple\", \"banana\"]\n\n# Append a single element to the end\nfruits.append(\"cherry\")  # [\"apple\", \"banana\", \"cherry\"]\n\n# Insert an element at a specific position\nfruits.insert(1, \"apricot\")  # [\"apple\", \"apricot\", \"banana\", \"cherry\"]\n\n# Extend the list with another iterable\nfruits.extend([\"date\", \"elderberry\"])  # [\"apple\", \"apricot\", \"banana\", \"cherry\", \"date\", \"elderberry\"]\n# Equivalent to:\nfruits += [\"fig\", \"grape\"]  # [\"apple\", \"apricot\", \"banana\", \"cherry\", \"date\", \"elderberry\", \"fig\", \"grape\"]\n</code></pre>"},{"location":"resources/python-lists-cheatsheet/#removing-elements","title":"Removing Elements","text":"<pre><code>fruits = [\"apple\", \"banana\", \"cherry\", \"date\", \"banana\", \"elderberry\"]\n\n# Remove by value (first occurrence)\nfruits.remove(\"banana\")  # [\"apple\", \"cherry\", \"date\", \"banana\", \"elderberry\"]\n\n# Remove by index and get the value\ncherry = fruits.pop(1)  # cherry = \"cherry\", fruits = [\"apple\", \"date\", \"banana\", \"elderberry\"]\nlast = fruits.pop()     # last = \"elderberry\", fruits = [\"apple\", \"date\", \"banana\"]\n\n# Remove by index without returning the value\ndel fruits[1]  # fruits = [\"apple\", \"banana\"]\n\n# Remove a slice\nfruits = [\"apple\", \"banana\", \"cherry\", \"date\", \"elderberry\"]\ndel fruits[1:3]  # fruits = [\"apple\", \"date\", \"elderberry\"]\n\n# Clear the entire list\nfruits.clear()  # fruits = []\n</code></pre>"},{"location":"resources/python-lists-cheatsheet/#modifying-elements","title":"Modifying Elements","text":"<pre><code>fruits = [\"apple\", \"banana\", \"cherry\"]\n\n# Change a single element\nfruits[1] = \"blueberry\"  # [\"apple\", \"blueberry\", \"cherry\"]\n\n# Change multiple elements with slice assignment\nfruits[0:2] = [\"apricot\", \"blackberry\"]  # [\"apricot\", \"blackberry\", \"cherry\"]\n\n# Insert multiple elements with slice assignment\nfruits = [\"apple\", \"cherry\"]\nfruits[1:1] = [\"banana\", \"blackberry\"]  # [\"apple\", \"banana\", \"blackberry\", \"cherry\"]\n\n# Replace with fewer elements\nfruits = [\"apple\", \"banana\", \"blackberry\", \"cherry\"]\nfruits[1:3] = [\"blueberry\"]  # [\"apple\", \"blueberry\", \"cherry\"]\n</code></pre>"},{"location":"resources/python-lists-cheatsheet/#list-operations","title":"List Operations","text":""},{"location":"resources/python-lists-cheatsheet/#concatenation-and-repetition","title":"Concatenation and Repetition","text":"<pre><code>list1 = [1, 2, 3]\nlist2 = [4, 5, 6]\n\n# Concatenation\ncombined = list1 + list2  # [1, 2, 3, 4, 5, 6]\n\n# Repetition\nrepeated = list1 * 3  # [1, 2, 3, 1, 2, 3, 1, 2, 3]\n</code></pre>"},{"location":"resources/python-lists-cheatsheet/#checking-membership","title":"Checking Membership","text":"<pre><code>fruits = [\"apple\", \"banana\", \"cherry\"]\n\n# Check if an element is in the list\n\"banana\" in fruits  # True\n\"mango\" in fruits   # False\n\"mango\" not in fruits  # True\n</code></pre>"},{"location":"resources/python-lists-cheatsheet/#list-methods-for-sorting-and-reversing","title":"List Methods for Sorting and Reversing","text":"<pre><code>numbers = [3, 1, 4, 1, 5, 9, 2]\n\n# Sort the list in-place\nnumbers.sort()  # [1, 1, 2, 3, 4, 5, 9]\n\n# Sort in descending order\nnumbers.sort(reverse=True)  # [9, 5, 4, 3, 2, 1, 1]\n\n# Sort with a custom key function\nwords = [\"apple\", \"Banana\", \"cherry\"]\nwords.sort()  # [\"Banana\", \"apple\", \"cherry\"] (uppercase comes before lowercase)\nwords.sort(key=str.lower)  # [\"apple\", \"Banana\", \"cherry\"] (case-insensitive sort)\n\n# Create a new sorted list without modifying the original\nnumbers = [3, 1, 4, 1, 5, 9, 2]\nsorted_numbers = sorted(numbers)  # sorted_numbers = [1, 1, 2, 3, 4, 5, 9], numbers unchanged\ndesc_numbers = sorted(numbers, reverse=True)  # [9, 5, 4, 3, 2, 1, 1]\n\n# Reverse the list in-place\nnumbers.reverse()  # [2, 9, 5, 1, 4, 1, 3]\n\n# Create a reversed iterator without modifying the original\nnumbers = [1, 2, 3, 4, 5]\nrev_iter = reversed(numbers)  # returns an iterator\nrev_list = list(rev_iter)  # rev_list = [5, 4, 3, 2, 1], numbers unchanged\n</code></pre>"},{"location":"resources/python-lists-cheatsheet/#finding-elements","title":"Finding Elements","text":"<pre><code>fruits = [\"apple\", \"banana\", \"cherry\", \"banana\", \"elderberry\"]\n\n# Find the index of the first occurrence\nbanana_index = fruits.index(\"banana\")  # 1\n\n# Find with a start and end range\nbanana_index = fruits.index(\"banana\", 2)  # 3 (search starts from index 2)\nbanana_index = fruits.index(\"banana\", 2, 4)  # 3 (search from index 2 up to index 4)\n\n# Count occurrences\nbanana_count = fruits.count(\"banana\")  # 2\n</code></pre>"},{"location":"resources/python-lists-cheatsheet/#common-operations-with-lists","title":"Common Operations with Lists","text":"<pre><code>numbers = [1, 2, 3, 4, 5]\n\n# Length\nlength = len(numbers)  # 5\n\n# Min and max values\nmin_value = min(numbers)  # 1\nmax_value = max(numbers)  # 5\n\n# Sum of all elements\ntotal = sum(numbers)  # 15\n\n# Sum with a start value\ntotal = sum(numbers, 10)  # 25 (10 + 1 + 2 + 3 + 4 + 5)\n\n# Any and all\nhas_any_even = any(x % 2 == 0 for x in numbers)  # True\nall_positive = all(x &gt; 0 for x in numbers)  # True\n</code></pre>"},{"location":"resources/python-lists-cheatsheet/#list-iteration","title":"List Iteration","text":""},{"location":"resources/python-lists-cheatsheet/#forward-iteration","title":"Forward Iteration","text":"<pre><code>fruits = [\"apple\", \"banana\", \"cherry\", \"date\"]\n\n# Using a for loop (simplest approach)\nfor fruit in fruits:\n    print(fruit)\n\n# With index using enumerate\nfor i, fruit in enumerate(fruits):\n    print(f\"{i}: {fruit}\")  # \"0: apple\", \"1: banana\", etc.\n\n# Using range and index\nfor i in range(len(fruits)):\n    print(fruits[i])\n</code></pre>"},{"location":"resources/python-lists-cheatsheet/#reverse-iteration","title":"Reverse Iteration","text":"<p>There are several ways to iterate through a list in reverse order:</p> <pre><code>fruits = [\"apple\", \"banana\", \"cherry\", \"date\"]\n\n# Method 1: Using reversed() function\n# Creates an iterator that accesses the list in reverse order\n# Most Pythonic and generally preferred approach\nfor fruit in reversed(fruits):\n    print(fruit)  # \"date\", \"cherry\", \"banana\", \"apple\"\n\n# Method 2: Using negative indexing with range\n# Explicitly calculates each index in reverse\nfor i in range(len(fruits)-1, -1, -1):\n    print(fruits[i])  # \"date\", \"cherry\", \"banana\", \"apple\"\n\n# Method 3: Using a reversed slice\n# Creates a new reversed list in memory\nfor fruit in fruits[::-1]:\n    print(fruit)  # \"date\", \"cherry\", \"banana\", \"apple\"\n\n# Method 4: Using a while loop with decreasing index\n# Rarely used but shows explicit index manipulation\ni = len(fruits) - 1\nwhile i &gt;= 0:\n    print(fruits[i])  # \"date\", \"cherry\", \"banana\", \"apple\"\n    i -= 1\n\n# Method 5: With index using enumerate and reversed\nfor i, fruit in enumerate(reversed(fruits)):\n    print(f\"{len(fruits)-1-i}: {fruit}\")  # \"3: date\", \"2: cherry\", etc.\n</code></pre>"},{"location":"resources/python-lists-cheatsheet/#performance-and-use-case-comparison","title":"Performance and Use Case Comparison","text":"<pre><code># For most cases, choose based on what you need:\n\n# 1. reversed(): Best for simple iteration\n# - Memory efficient (doesn't create a copy)\n# - Clean and idiomatic Python\n# - Use when you just need to process items in reverse\n\nfor item in reversed(my_list):\n    process(item)\n\n# 2. Range with negative indices: Best when you need precise index control\n# - Slightly more verbose\n# - Gives you the exact reverse position\n# - Use when you need the reverse index for calculations\n\nfor i in range(len(my_list)-1, -1, -1):\n    process(my_list[i], i)\n\n# 3. Slicing with [::-1]: Best when you need the reversed list for further operations\n# - Creates a new list in memory (higher memory usage)\n# - Convenient when you need the reversed list for multiple operations\n# - Avoid for very large lists if memory is a concern\n\nreversed_list = my_list[::-1]\nfor item in reversed_list:\n    process(item)\n# You can continue using reversed_list after the loop\n\n# 4. While loop: Rarely needed\n# - Most verbose and least Pythonic\n# - Useful only in specific scenarios where you need complex index manipulation\n# - Generally avoid unless you have a specific reason\n\n# 5. Reversed with enumerate: Best when you need both item and original position\n# - Slightly more complex but very versatile\n# - Great for when position matters in reverse order\n\nfor i, item in enumerate(reversed(my_list)):\n    original_index = len(my_list) - 1 - i\n    process(item, original_index)\n</code></pre>"},{"location":"resources/python-lists-cheatsheet/#iterating-through-multiple-lists","title":"Iterating Through Multiple Lists","text":"<pre><code># Zip multiple lists (parallel iteration)\nnames = [\"Alice\", \"Bob\", \"Charlie\"]\nages = [25, 30, 35]\n\nfor name, age in zip(names, ages):\n    print(f\"{name} is {age} years old\")\n\n# Zip with different length lists (stops at shortest)\nnumbers = [1, 2]\nletters = [\"a\", \"b\", \"c\"]\nfor num, letter in zip(numbers, letters):\n    print(num, letter)  # Only prints \"1 a\", \"2 b\"\n\n# Use zip_longest from itertools for different length lists\nfrom itertools import zip_longest\nfor num, letter in zip_longest(numbers, letters, fillvalue=0):\n    print(num, letter)  # \"1 a\", \"2 b\", \"0 c\"\n\n# Nested iteration\nmatrix = [[1, 2, 3], [4, 5, 6]]\nfor row in matrix:\n    for element in row:\n        print(element)  # 1, 2, 3, 4, 5, 6\n</code></pre>"},{"location":"resources/python-lists-cheatsheet/#functions-similar-to-zip-for-list-operations","title":"Functions Similar to zip() for List Operations","text":"<p>Python's standard library offers several functions that, like <code>zip()</code>, help with sophisticated list operations:</p> <pre><code>from itertools import *\n\n# 1. enumerate() - Adds counter to an iterable\nfor i, item in enumerate([\"a\", \"b\", \"c\"]):\n    print(f\"{i}: {item}\")  # \"0: a\", \"1: b\", \"2: c\"\n\n# Start counting from a specific number\nfor i, item in enumerate([\"a\", \"b\", \"c\"], start=1):\n    print(f\"{i}: {item}\")  # \"1: a\", \"2: b\", \"3: c\"\n\n# 2. zip_longest() - Like zip but doesn't stop at the shortest iterable\nlist1 = [1, 2]\nlist2 = [\"a\", \"b\", \"c\"]\nfor pair in zip_longest(list1, list2, fillvalue=0):\n    print(pair)  # (1, 'a'), (2, 'b'), (0, 'c')\n\n# 3. chain() - Combines multiple iterables sequentially\nfor item in chain([1, 2], [3, 4], [5, 6]):\n    print(item)  # 1, 2, 3, 4, 5, 6\n\n# 4. product() - Cartesian product of iterables (all combinations)\nfor pair in product([1, 2], [\"a\", \"b\"]):\n    print(pair)  # (1, 'a'), (1, 'b'), (2, 'a'), (2, 'b')\n\n# 5. combinations() - All possible r-length combinations\nfor combo in combinations([1, 2, 3, 4], 2):\n    print(combo)  # (1, 2), (1, 3), (1, 4), (2, 3), (2, 4), (3, 4)\n\n# 6. permutations() - All possible r-length orderings\nfor perm in permutations([1, 2, 3], 2):\n    print(perm)  # (1, 2), (1, 3), (2, 1), (2, 3), (3, 1), (3, 2)\n\n# 7. groupby() - Groups consecutive items by a key function\nanimals = [\"duck\", \"dog\", \"deer\", \"cat\", \"cow\"]\n# Must be sorted first for groupby to work as expected\nanimals.sort(key=lambda x: x[0])  # Sort by first letter\nfor key, group in groupby(animals, key=lambda x: x[0]):\n    print(key, list(group))  # 'c' ['cat', 'cow'], 'd' ['deer', 'dog', 'duck']\n\n# 8. islice() - Like slice notation but for any iterable\nfor item in islice(range(10), 2, 8, 2):\n    print(item)  # 2, 4, 6\n\n# 9. map() - Apply a function to each item in one or more iterables\nfor item in map(lambda x, y: x + y, [1, 2, 3], [10, 20, 30]):\n    print(item)  # 11, 22, 33\n\n# 10. filter() - Keep only items that pass a test function\nfor item in filter(lambda x: x % 2 == 0, [1, 2, 3, 4, 5, 6]):\n    print(item)  # 2, 4, 6\n\n# 11. starmap() - Apply function using arguments unpacked from each iterable\nfor item in starmap(pow, [(2, 3), (4, 2), (10, 3)]):\n    print(item)  # 8, 16, 1000\n</code></pre>"},{"location":"resources/python-lists-cheatsheet/#combining-iteration-functions-for-advanced-operations","title":"Combining Iteration Functions for Advanced Operations","text":"<p>These functions can be composed to create powerful data processing pipelines:</p> <pre><code># Example 1: Process pairs of adjacent items in a list\nnumbers = [1, 2, 3, 4, 5]\nadjacent_pairs = list(zip(numbers, numbers[1:]))\n# [(1, 2), (2, 3), (3, 4), (4, 5)]\n\n# Example 2: Create a sliding window of a specified size\ndef sliding_window(iterable, size):\n    \"\"\"Create a sliding window of `size` elements over the iterable.\"\"\"\n    iterables = tee(iterable, size)\n    return zip(*[islice(it, i, None) for i, it in enumerate(iterables)])\n\nlist(sliding_window([1, 2, 3, 4, 5, 6], 3))\n# [(1, 2, 3), (2, 3, 4), (3, 4, 5), (4, 5, 6)]\n\n# Example 3: Group items with a specific property together\ndata = [\n    {\"name\": \"Alice\", \"role\": \"developer\"},\n    {\"name\": \"Bob\", \"role\": \"manager\"},\n    {\"name\": \"Charlie\", \"role\": \"developer\"}\n]\n\nfrom operator import itemgetter\n# Sort by role first (groupby requires sorted data)\ndata.sort(key=itemgetter(\"role\"))\nfor role, group in groupby(data, key=itemgetter(\"role\")):\n    print(f\"{role}: {[person['name'] for person in group]}\")\n# developer: ['Alice', 'Charlie']\n# manager: ['Bob']\n\n# Example 4: Filter and transform data in one pipeline\nimport itertools as it\nnumbers = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\nresult = list(map(lambda x: x**2, \n                 filter(lambda x: x % 2 == 0, numbers)))\n# [4, 16, 36, 64, 100]\n\n# Same operation with list comprehension (usually more readable)\nresult = [x**2 for x in numbers if x % 2 == 0]\n</code></pre>"},{"location":"resources/python-lists-cheatsheet/#list-comprehensions","title":"List Comprehensions","text":"<pre><code># Basic syntax: [expression for item in iterable if condition]\n\n# Transform elements\nnumbers = [1, 2, 3, 4, 5]\nsquares = [x**2 for x in numbers]  # [1, 4, 9, 16, 25]\n\n# Filter elements\neven_numbers = [x for x in numbers if x % 2 == 0]  # [2, 4]\n\n# Both transform and filter\neven_squares = [x**2 for x in numbers if x % 2 == 0]  # [4, 16]\n\n# Nested loops\npairs = [(x, y) for x in [1, 2] for y in [3, 4]]\n# [(1, 3), (1, 4), (2, 3), (2, 4)]\n\n# Conditional expressions (ternary)\nvalues = [1, -2, 3, -4, 5]\nabs_values = [x if x &gt; 0 else -x for x in values]  # [1, 2, 3, 4, 5]\n</code></pre>"},{"location":"resources/python-lists-cheatsheet/#nested-lists-and-matrices","title":"Nested Lists and Matrices","text":"<pre><code># Creating a nested list\nmatrix = [\n    [1, 2, 3],\n    [4, 5, 6],\n    [7, 8, 9]\n]\n\n# Accessing elements\ntop_left = matrix[0][0]  # 1\nmiddle = matrix[1][1]    # 5\nbottom_right = matrix[2][2]  # 9\n\n# Modifying elements\nmatrix[0][1] = 20  # [[1, 20, 3], [4, 5, 6], [7, 8, 9]]\n\n# Creating a matrix with list comprehension\nmatrix = [[i * 3 + j + 1 for j in range(3)] for i in range(3)]\n# [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\n\n# Flattening a nested list\nflat = [x for sublist in matrix for x in sublist]\n# [1, 2, 3, 4, 5, 6, 7, 8, 9]\n\n# Transpose a matrix (convert rows to columns)\ntransposed = [[row[i] for row in matrix] for i in range(len(matrix[0]))]\n# [[1, 4, 7], [2, 5, 8], [3, 6, 9]]\n</code></pre>"},{"location":"resources/python-lists-cheatsheet/#copying-lists","title":"Copying Lists","text":"<pre><code>original = [1, 2, [3, 4]]\n\n# Shallow copy (copies references to nested objects)\ncopy1 = original.copy()\ncopy2 = original[:]\ncopy3 = list(original)\n\n# Changing a nested list in a shallow copy\ncopy1[2][0] = 30  # This also affects original and other copies\n# original = [1, 2, [30, 4]]\n# copy1 = [1, 2, [30, 4]]\n# copy2 = [1, 2, [30, 4]]\n\n# Deep copy (completely independent copy)\nimport copy\ndeep_copy = copy.deepcopy(original)\ndeep_copy[2][0] = 100  # Only affects deep_copy\n# deep_copy = [1, 2, [100, 4]]\n# original = [1, 2, [30, 4]]\n</code></pre>"},{"location":"resources/python-lists-cheatsheet/#advanced-list-techniques","title":"Advanced List Techniques","text":""},{"location":"resources/python-lists-cheatsheet/#list-slicing-tricks","title":"List Slicing Tricks","text":"<pre><code>numbers = [1, 2, 3, 4, 5]\n\n# Reverse a list with slicing (creates a new list)\nreversed_list = numbers[::-1]  # [5, 4, 3, 2, 1]\n\n# Create a shallow copy with slicing\ncopy_list = numbers[:]    # [1, 2, 3, 4, 5]\ncopy_list2 = numbers[::]  # [1, 2, 3, 4, 5] (same as [:])\n\n# Verifying that slicing creates shallow copies, not deep copies\nnested = [[1, 2], [3, 4]]\nsliced_copy = nested[:]    # Creates a shallow copy\nsliced_copy2 = nested[::]  # Also creates a shallow copy (identical to [:])\n\n# Proof 1: Modifying nested elements in the original affects all shallow copies\nprint(\"Original nested list:\", nested)              # [[1, 2], [3, 4]]\nprint(\"Shallow copy before modification:\", sliced_copy)  # [[1, 2], [3, 4]]\n\nnested[0][0] = 99  # Modify a nested element in the original\nprint(\"Original after modification:\", nested)            # [[99, 2], [3, 4]]\nprint(\"Shallow copy after modification:\", sliced_copy)   # [[99, 2], [3, 4]] - nested elements are shared\nprint(\"Shallow copy2 after modification:\", sliced_copy2) # [[99, 2], [3, 4]] - same behavior with [::]\n\n# Proof 2: Using 'is' to check if nested objects are the same objects in memory\nprint(nested[0] is sliced_copy[0])  # True - they reference the same object\nprint(nested[1] is sliced_copy[1])  # True - they reference the same object\n\n# Contrast with a deep copy\nimport copy\ndeep_copy = copy.deepcopy(nested)\nnested[0][1] = 88\nprint(\"Original after second modification:\", nested)     # [[99, 88], [3, 4]]\nprint(\"Shallow copy after second mod:\", sliced_copy)     # [[99, 88], [3, 4]] - affected by the change\nprint(\"Deep copy after second mod:\", deep_copy)          # [[99, 2], [3, 4]] - not affected by the change\n\n# Proof that deep copy creates different objects\nprint(nested[0] is deep_copy[0])  # False - they are different objects\nprint(nested[1] is deep_copy[1])  # False - they are different objects\n\n# Important: Shallow vs. Deep Copy with Immutable Types\nnumbers = [1, 2, 3, 4, 5]  # List of immutable integers\nshallow_copy = numbers[:]\ndeep_copy = copy.deepcopy(numbers)\n\n# For lists containing only immutable types, shallow and deep copies behave the same\nnumbers[0] = 99\nprint(\"Original after modification:\", numbers)        # [99, 2, 3, 4, 5]\nprint(\"Shallow copy after modification:\", shallow_copy)  # [1, 2, 3, 4, 5] - unaffected\nprint(\"Deep copy after modification:\", deep_copy)        # [1, 2, 3, 4, 5] - unaffected\n\n# Why? Immutable objects cannot be changed in-place\n# So even though shallow copies share references to the same immutable objects,\n# when you modify the list, you're replacing the objects, not modifying them\nstrings = [\"hello\", \"world\"]\nstrings_shallow = strings[:]\nstrings[0] = \"goodbye\"  # This replaces the string, not modifies it\nprint(strings)          # [\"goodbye\", \"world\"]\nprint(strings_shallow)  # [\"hello\", \"world\"] - unaffected\n\n# This distinction only matters with mutable types (lists, dicts, custom objects, etc.)\n# For immutable types, shallow copies are sufficient and more efficient\n\n# For a true deep copy, use the copy module\nimport copy\ndeep_copy = copy.deepcopy(nested)\nnested[0][1] = 88\nprint(nested)      # [[99, 88], [3, 4]]\nprint(deep_copy)   # [[99, 2], [3, 4]] - changes in original don't affect deep copy\n\n# Get every nth element\nevery_second = numbers[::2]  # [1, 3, 5]\nevery_third = numbers[::3]   # [1, 4]\n\n# Reverse every nth element\nreverse_every_second = numbers[::-2]  # [5, 3, 1]\n\n# Partial slice with step\nmiddle_reversed = numbers[3:0:-1]  # [4, 3, 2]\n\n# Replace a slice\nnumbers[1:4] = [20, 30, 40]  # [1, 20, 30, 40, 5]\n\n# Delete a slice\nnumbers = [1, 2, 3, 4, 5]\nnumbers[1:3] = []  # [1, 4, 5]\n\n# Insert without replacing\nnumbers = [1, 2, 3]\nnumbers[1:1] = [10, 11]  # [1, 10, 11, 2, 3]\n</code></pre>"},{"location":"resources/python-lists-cheatsheet/#using-zip-with-lists","title":"Using zip() with Lists","text":"<pre><code>names = [\"Alice\", \"Bob\", \"Charlie\"]\nages = [25, 30, 35]\n\n# Combine multiple lists element-wise\ncombined = list(zip(names, ages))\n# [('Alice', 25), ('Bob', 30), ('Charlie', 35)]\n\n# Unzip a list of tuples\nnames, ages = zip(*combined)\n# names = ('Alice', 'Bob', 'Charlie')\n# ages = (25, 30, 35)\n\n# Iterate over multiple lists simultaneously\nfor name, age in zip(names, ages):\n    print(f\"{name} is {age} years old\")\n\n# Create a dictionary from two lists\nname_to_age = dict(zip(names, ages))\n# {'Alice': 25, 'Bob': 30, 'Charlie': 35}\n</code></pre>"},{"location":"resources/python-lists-cheatsheet/#list-as-a-stack-and-queue","title":"List as a Stack and Queue","text":"<pre><code># Using a list as a stack (last-in, first-out)\nstack = []\nstack.append(1)  # Add to top: [1]\nstack.append(2)  # Add to top: [1, 2]\nstack.append(3)  # Add to top: [1, 2, 3]\ntop_item = stack.pop()  # top_item = 3, stack = [1, 2]\n\n# Using a list as a queue (first-in, first-out) - inefficient for large lists\nqueue = []\nqueue.append(1)  # Add to end: [1]\nqueue.append(2)  # Add to end: [1, 2]\nqueue.append(3)  # Add to end: [1, 2, 3]\nfirst_item = queue.pop(0)  # first_item = 1, queue = [2, 3]\n# Note: For efficient queues, use collections.deque instead\n</code></pre>"},{"location":"resources/python-lists-cheatsheet/#list-unpacking","title":"List Unpacking","text":"<p>List unpacking (also called sequence unpacking) allows you to assign multiple variables from the values in a list or other iterable in a single operation. Let's explore how it works:</p> <pre><code># Basic unpacking\nfirst, second, third = [1, 2, 3]\n# first = 1, second = 2, third = 3\n\n# How unpacking works behind the scenes:\n# 1. Python sees multiple targets on the left side of =\n# 2. It evaluates the expression on the right side ([1, 2, 3])\n# 3. It checks if the number of targets matches the number of values\n# 4. It assigns each value to its corresponding target variable\n\n# Must have exactly the same number of variables as elements\n# This raises ValueError: too many values to unpack\n# a, b = [1, 2, 3]\n\n# This also raises ValueError: not enough values to unpack\n# a, b, c, d = [1, 2, 3]\n\n# Unpacking with * (rest/starred unpacking)\nfirst, *rest = [1, 2, 3, 4, 5]\n# first = 1, rest = [2, 3, 4, 5]\n\n# The * operator collects \"the rest\" of the values into a list\n# It can appear anywhere in the unpacking:\nhead, *middle, tail = [1, 2, 3, 4, 5]\n# head = 1, middle = [2, 3, 4], tail = 5\n\n*start, end = [1, 2, 3]\n# start = [1, 2], end = 3\n\n# Only one starred expression is allowed in an assignment\n# This raises SyntaxError:\n# *start, *end = [1, 2, 3, 4]\n\n# Ignoring values with _\nfirst, *_, last = [1, 2, 3, 4, 5]\n# first = 1, last = 5, middle values collected but ignored\n# Using _ is a convention indicating the value won't be used\n\n# Unpacking nested structures\n(a, b), (c, d) = [(1, 2), (3, 4)]\n# a = 1, b = 2, c = 3, d = 4\n\n# Unpacking in for loops\npoints = [(1, 2), (3, 4), (5, 6)]\nfor x, y in points:\n    print(f\"Point: ({x}, {y})\")\n\n# Swapping values (a classic Python idiom)\na, b = 1, 2\na, b = b, a  # a = 2, b = 1\n# This works because the right side is evaluated first as a tuple (2, 1),\n# then unpacked into the variables on the left\n\n# Unpacking and functions\ndef get_coordinates():\n    return (3, 4)  # Returns a tuple\n\nx, y = get_coordinates()  # Unpacks the returned tuple\n# x = 3, y = 4\n\n# Merging lists with unpacking (Python 3.5+)\nlist1 = [1, 2, 3]\nlist2 = [4, 5, 6]\ncombined = [*list1, *list2]  # [1, 2, 3, 4, 5, 6]\n\n# Can combine with normal elements\ncombined = [0, *list1, 3.5, *list2, 7]  # [0, 1, 2, 3, 3.5, 4, 5, 6, 7]\n\n# Works with any iterable on the right side\ncombined = [*list1, *\"abc\"]  # [1, 2, 3, 'a', 'b', 'c']\n\n# Unpacking dictionaries (with ** operator)\ndict1 = {\"a\": 1, \"b\": 2}\ndict2 = {\"c\": 3, \"d\": 4}\ncombined_dict = {**dict1, **dict2}  # {'a': 1, 'b': 2, 'c': 3, 'd': 4}\n\n# In function calls\nnumbers = [1, 2, 3]\nprint(*numbers)  # Equivalent to print(1, 2, 3)\n\n# Extended unpacking in function calls with keywords\npoint = {\"x\": 1, \"y\": 2}\ndraw_point(**point)  # Equivalent to draw_point(x=1, y=2)\n</code></pre>"},{"location":"resources/python-lists-cheatsheet/#how-unpacking-differs-from-indexing","title":"How Unpacking Differs from Indexing","text":"<p>Unpacking offers several advantages over accessing list elements by index:</p> <pre><code>items = [1, 2, 3]\n\n# Using indexing\nfirst = items[0]\nsecond = items[1]\nthird = items[2]\n\n# Using unpacking\nfirst, second, third = items\n\n# Unpacking is more concise and clearly communicates intent\n# It also validates the expected structure - unpacking will fail\n# if the list doesn't have exactly the expected number of elements\n\n# With extended unpacking, we get even more flexibility\nfirst, *rest = items  # Adapts to lists of different lengths\n</code></pre>"},{"location":"resources/python-lists-cheatsheet/#connection-to-args-and-kwargs-in-functions","title":"Connection to args and *kwargs in Functions","text":"<p>Unpacking is closely related to how <code>*args</code> and <code>**kwargs</code> work in Python functions. The same unpacking mechanisms are at play, just in different contexts:</p> <pre><code># Regular unpacking in assignment\na, *b = [1, 2, 3, 4]  # a = 1, b = [2, 3, 4]\n\n# *args in function definition collects positional arguments into a tuple\ndef print_all(*args):\n    print(f\"You passed {len(args)} arguments: {args}\")\n    # args is a tuple containing all positional arguments\n\nprint_all(1, 2, 3)  # You passed 3 arguments: (1, 2, 3)\n\n# The * in *args is the same concept as in unpacking, but:\n# - In assignment (a, *b = ...), * collects values into a list\n# - In function params (*args), * collects arguments into a tuple\n\n# Unpacking in function calls (the inverse of *args)\nnumbers = [1, 2, 3]\nprint_all(*numbers)  # Unpacks the list into separate arguments\n                    # Equivalent to print_all(1, 2, 3)\n\n# **kwargs collects keyword arguments into a dictionary\ndef user_info(**kwargs):\n    print(f\"User information: {kwargs}\")\n    # kwargs is a dictionary containing all keyword arguments\n\nuser_info(name=\"Alice\", age=30, city=\"New York\")\n# User information: {'name': 'Alice', 'age': 30, 'city': 'New York'}\n\n# Dictionary unpacking in function calls (the inverse of **kwargs)\nuser_data = {\"name\": \"Bob\", \"age\": 25, \"city\": \"Boston\"}\nuser_info(**user_data)  # Unpacks the dictionary into keyword arguments\n                       # Equivalent to user_info(name=\"Bob\", age=25, city=\"Boston\")\n\n# Combining *args and **kwargs gives maximum flexibility\ndef flexible_function(*args, **kwargs):\n    print(f\"Positional args: {args}\")\n    print(f\"Keyword args: {kwargs}\")\n\nflexible_function(1, 2, 3, name=\"Alice\", age=30)\n# Positional args: (1, 2, 3)\n# Keyword args: {'name': 'Alice', 'age': 30}\n\n# Real-world example: Wrapper functions that pass through arguments\ndef log_and_call(func, *args, **kwargs):\n    print(f\"Calling {func.__name__} with {args} and {kwargs}\")\n    return func(*args, **kwargs)\n\ndef add(a, b):\n    return a + b\n\nresult = log_and_call(add, 5, 3)\n# Calling add with (5, 3) and {}\n# result = 8\n</code></pre> <p>This symmetry in Python's design makes the language very consistent: - When defining functions: <code>*args</code> and <code>**kwargs</code> collect multiple arguments - When calling functions: <code>*iterable</code> and <code>**dict</code> unpack collections into arguments - In assignments: <code>*variable</code> collects multiple values</p>"},{"location":"resources/python-lists-cheatsheet/#behind-the-scenes-of-star-unpacking","title":"Behind the Scenes of Star Unpacking","text":"<p>The <code>*</code> operator in unpacking works by: 1. Collecting all \"extra\" items not assigned to regular variables 2. Creating a new list with those items 3. Assigning that list to the starred variable</p> <p>If there are no \"extra\" items for the starred variable, it gets an empty list:</p> <pre><code>a, *b, c = [1, 2]  # a = 1, b = [], c = 2\n</code></pre>"},{"location":"resources/python-lists-cheatsheet/#filtering-and-transforming-with-map-and-filter","title":"Filtering and Transforming with map() and filter()","text":"<pre><code>numbers = [1, 2, 3, 4, 5]\n\n# Map: Apply a function to each element\nsquares = list(map(lambda x: x**2, numbers))\n# [1, 4, 9, 16, 25]\n\n# Filter: Keep elements that satisfy a condition\neven = list(filter(lambda x: x % 2 == 0, numbers))\n# [2, 4]\n\n# Combining map and filter\neven_squares = list(map(lambda x: x**2, filter(lambda x: x % 2 == 0, numbers)))\n# [4, 16]\n\n# List comprehension equivalents (generally more readable)\nsquares = [x**2 for x in numbers]\neven = [x for x in numbers if x % 2 == 0]\neven_squares = [x**2 for x in numbers if x % 2 == 0]\n</code></pre>"},{"location":"resources/python-lists-cheatsheet/#performance-considerations","title":"Performance Considerations","text":"<ol> <li>Appending vs. Prepending:</li> <li>Appending to a list with <code>append()</code> is O(1) (constant time)</li> <li>Prepending with <code>insert(0, item)</code> is O(n) (linear time)</li> <li> <p>For frequent prepends, consider using <code>collections.deque</code></p> </li> <li> <p>List Comprehensions:</p> </li> <li>Generally faster than equivalent for loops</li> <li> <p>More memory-efficient than building a list with multiple <code>append()</code> calls</p> </li> <li> <p>Copying:</p> </li> <li>Slicing <code>[:]</code> and <code>copy()</code> create shallow copies (faster but share nested objects)</li> <li> <p><code>copy.deepcopy()</code> creates independent copies (slower but safer for nested structures)</p> </li> <li> <p>Searching:</p> </li> <li><code>x in my_list</code> is O(n) - must check each element</li> <li> <p>For frequent lookups, consider using a set or dictionary instead</p> </li> <li> <p>Sorting:</p> </li> <li><code>sort()</code> is in-place and modifies the original list</li> <li><code>sorted()</code> creates a new list and leaves the original unchanged</li> <li> <p>Both use Timsort algorithm with O(n log n) time complexity</p> </li> <li> <p>Concatenation:</p> </li> <li><code>+</code> creates a new list, which can be inefficient for repeated operations</li> <li><code>extend()</code> or <code>+=</code> modifies in-place and is more efficient for repeated operations</li> </ol>"},{"location":"resources/python-lists-cheatsheet/#common-patterns-and-recipes","title":"Common Patterns and Recipes","text":""},{"location":"resources/python-lists-cheatsheet/#removing-duplicates-while-preserving-order","title":"Removing Duplicates While Preserving Order","text":"<pre><code>def remove_duplicates(items):\n    seen = set()\n    result = []\n    for item in items:\n        if item not in seen:\n            seen.add(item)\n            result.append(item)\n    return result\n\n# Usage\nunique = remove_duplicates([1, 2, 3, 1, 2, 4, 5])  # [1, 2, 3, 4, 5]\n\n# One-liner using dict.fromkeys() (Python 3.6+ preserves order)\nunique = list(dict.fromkeys([1, 2, 3, 1, 2, 4, 5]))  # [1, 2, 3, 4, 5]\n</code></pre>"},{"location":"resources/python-lists-cheatsheet/#counting-occurrences","title":"Counting Occurrences","text":"<pre><code>from collections import Counter\n\nitems = [\"apple\", \"banana\", \"apple\", \"orange\", \"banana\", \"apple\"]\n\n# Using Counter\ncounts = Counter(items)\n# Counter({'apple': 3, 'banana': 2, 'orange': 1})\n\n# Most common elements\nmost_common = counts.most_common(2)  # [('apple', 3), ('banana', 2)]\n\n# Manual counting with dictionary\ncount_dict = {}\nfor item in items:\n    count_dict[item] = count_dict.get(item, 0) + 1\n# {'apple': 3, 'banana': 2, 'orange': 1}\n</code></pre>"},{"location":"resources/python-lists-cheatsheet/#grouping-items","title":"Grouping Items","text":"<pre><code>from itertools import groupby\nfrom operator import itemgetter\n\n# Grouping sorted data\ndata = [\n    {\"name\": \"Alice\", \"department\": \"HR\"},\n    {\"name\": \"Bob\", \"department\": \"IT\"},\n    {\"name\": \"Charlie\", \"department\": \"HR\"},\n    {\"name\": \"David\", \"department\": \"IT\"},\n]\n\n# Data must be sorted by the grouping key\ndata.sort(key=itemgetter(\"department\"))\n\n# Group by department\nfor department, group in groupby(data, key=itemgetter(\"department\")):\n    print(f\"{department}: {list(group)}\")\n# HR: [{'name': 'Alice', 'department': 'HR'}, {'name': 'Charlie', 'department': 'HR'}]\n# IT: [{'name': 'Bob', 'department': 'IT'}, {'name': 'David', 'department': 'IT'}]\n\n# Using defaultdict (no sorting required)\nfrom collections import defaultdict\n\ngroups = defaultdict(list)\nfor item in data:\n    groups[item[\"department\"]].append(item)\n\nfor department, members in groups.items():\n    print(f\"{department}: {members}\")\n</code></pre>"},{"location":"resources/python-lists-cheatsheet/#finding-indices-of-all-occurrences","title":"Finding Indices of All Occurrences","text":"<pre><code>def find_all_indices(lst, value):\n    return [i for i, x in enumerate(lst) if x == value]\n\n# Usage\nindices = find_all_indices([1, 2, 3, 1, 2, 1], 1)  # [0, 3, 5]\n</code></pre>"},{"location":"resources/python-lists-cheatsheet/#chunking-a-list","title":"Chunking a List","text":"<pre><code>def chunk_list(lst, chunk_size):\n    \"\"\"Split a list into chunks of specified size.\"\"\"\n    return [lst[i:i + chunk_size] for i in range(0, len(lst), chunk_size)]\n\n# Usage\nchunks = chunk_list([1, 2, 3, 4, 5, 6, 7, 8, 9, 10], 3)\n# [[1, 2, 3], [4, 5, 6], [7, 8, 9], [10]]\n</code></pre>"},{"location":"resources/python-lists-cheatsheet/#rotating-a-list","title":"Rotating a List","text":"<pre><code>def rotate_list(lst, k):\n    \"\"\"Rotate a list k positions to the right.\"\"\"\n    if not lst:\n        return lst\n    k = k % len(lst)  # Handle cases where k &gt; len(lst)\n    return lst[-k:] + lst[:-k]\n\n# Usage\nrotated = rotate_list([1, 2, 3, 4, 5], 2)  # [4, 5, 1, 2, 3]\n</code></pre>"},{"location":"resources/python-lists-cheatsheet/#interacting-with-other-data-structures","title":"Interacting with Other Data Structures","text":""},{"location":"resources/python-lists-cheatsheet/#converting-between-lists-and-other-data-types","title":"Converting Between Lists and Other Data Types","text":"<pre><code># List to string\nitems = [\"a\", \"b\", \"c\"]\njoined = \"\".join(items)  # \"abc\"\njoined_comma = \", \".join(items)  # \"a, b, c\"\n\n# String to list\ntext = \"apple,banana,cherry\"\nitems = text.split(\",\")  # [\"apple\", \"banana\", \"cherry\"]\n\n# List to tuple\ntuple_items = tuple([1, 2, 3])  # (1, 2, 3)\n\n# List to set (removes duplicates)\nunique_items = set([1, 2, 2, 3, 3, 3])  # {1, 2, 3}\n\n# List to dictionary\nkeys = [\"a\", \"b\", \"c\"]\nvalues = [1, 2, 3]\ndictionary = dict(zip(keys, values))  # {\"a\": 1, \"b\": 2, \"c\": 3}\n</code></pre>"},{"location":"resources/python-lists-cheatsheet/#working-with-csv-data","title":"Working with CSV Data","text":"<pre><code>import csv\n\n# Writing a list to a CSV file\ndata = [\n    [\"Name\", \"Age\", \"City\"],\n    [\"Alice\", 25, \"New York\"],\n    [\"Bob\", 30, \"Los Angeles\"],\n    [\"Charlie\", 35, \"Chicago\"]\n]\n\nwith open(\"people.csv\", \"w\", newline=\"\") as file:\n    writer = csv.writer(file)\n    writer.writerows(data)\n\n# Reading a CSV file into a list\nrows = []\nwith open(\"people.csv\", \"r\", newline=\"\") as file:\n    reader = csv.reader(file)\n    rows = list(reader)\n</code></pre>"},{"location":"resources/python-lists-cheatsheet/#appendix-list-methods-summary","title":"Appendix: List Methods Summary","text":"<p>Here's a quick reference of all list methods:</p> Method Description Example <code>append(x)</code> Add item to the end <code>lst.append(5)</code> <code>extend(iterable)</code> Add items from iterable to the end <code>lst.extend([5, 6])</code> <code>insert(i, x)</code> Insert item at position <code>lst.insert(0, 'start')</code> <code>remove(x)</code> Remove first occurrence of value <code>lst.remove(5)</code> <code>pop([i])</code> Remove &amp; return item at position (default: last) <code>lst.pop()</code>, <code>lst.pop(0)</code> <code>clear()</code> Remove all items <code>lst.clear()</code> <code>index(x[, start[, end]])</code> Return first index of value <code>lst.index(5)</code> <code>count(x)</code> Count occurrences of value <code>lst.count(5)</code> <code>sort(*, key=None, reverse=False)</code> Sort in-place <code>lst.sort()</code> <code>reverse()</code> Reverse in-place <code>lst.reverse()</code> <code>copy()</code> Return a shallow copy <code>lst.copy()</code>"},{"location":"resources/python-regex-cheatsheet/","title":"Python Regular Expressions Cheatsheet","text":""},{"location":"resources/python-regex-cheatsheet/#python-regular-expressions-cheatsheet","title":"Python Regular Expressions Cheatsheet","text":""},{"location":"resources/python-regex-cheatsheet/#basic-patterns","title":"Basic Patterns","text":""},{"location":"resources/python-regex-cheatsheet/#literal-characters","title":"Literal Characters","text":"<p>Regular expressions match literal characters by default.</p> <pre><code>import re\ntext = \"Python is powerful\"\nresult = re.search(\"Python\", text)  # Matches \"Python\"\n</code></pre>"},{"location":"resources/python-regex-cheatsheet/#metacharacters","title":"Metacharacters","text":"<p>Special characters with meaning in regex must be escaped with a backslash <code>\\</code> to match literally.</p> <pre><code># Metacharacters: . ^ $ * + ? { } [ ] \\ | ( )\ntext = \"Cost: $25.99\"\nresult = re.search(\"\\$\\d+\\.\\d+\", text)  # Matches \"$25.99\"\n</code></pre>"},{"location":"resources/python-regex-cheatsheet/#character-classes","title":"Character Classes","text":""},{"location":"resources/python-regex-cheatsheet/#single-character-matchers","title":"Single Character Matchers","text":"<p><code>.</code> (dot) matches any character except a newline.</p> <pre><code>pattern = \"c.t\"\n# Matches \"cat\", \"cut\", \"c@t\", etc.\n</code></pre>"},{"location":"resources/python-regex-cheatsheet/#character-sets","title":"Character Sets","text":"<p><code>[ ]</code> define a character set - matches any single character within the brackets.</p> <pre><code>pattern = \"[aeiou]\"  # Matches any single vowel\nre.findall(pattern, \"apple\")  # Returns ['a', 'e']\n\n# Range of characters\npattern = \"[a-z]\"  # Matches any lowercase letter\npattern = \"[A-Za-z]\"  # Matches any letter\npattern = \"[0-9]\"  # Matches any digit\n</code></pre>"},{"location":"resources/python-regex-cheatsheet/#negated-character-sets","title":"Negated Character Sets","text":"<p><code>[^ ]</code> matches any character NOT in the set.</p> <pre><code>pattern = \"[^0-9]\"  # Matches any non-digit\nre.findall(pattern, \"abc123\")  # Returns ['a', 'b', 'c']\n</code></pre>"},{"location":"resources/python-regex-cheatsheet/#predefined-character-classes","title":"Predefined Character Classes","text":"<p>Shorthand notation for common character sets:</p> <pre><code>\\d  # Matches any digit [0-9]\n\\D  # Matches any non-digit [^0-9]\n\\w  # Matches any word character [a-zA-Z0-9_]\n\\W  # Matches any non-word character\n\\s  # Matches any whitespace character (space, tab, newline)\n\\S  # Matches any non-whitespace character\n\n# Example\npattern = r\"\\d\\s\\w+\"  # Digit, followed by whitespace, followed by 1+ word chars\nre.search(pattern, \"7 apples\")  # Matches \"7 apples\"\n</code></pre>"},{"location":"resources/python-regex-cheatsheet/#anchors-and-boundaries","title":"Anchors and Boundaries","text":"<p>Anchors match positions rather than characters:</p> <pre><code>^  # Matches the start of a string\n$  # Matches the end of a string\n\\b  # Matches a word boundary\n\\B  # Matches a non-word boundary\n\n# Examples\npattern = r\"^Python\"  # Matches \"Python\" only at the start\nre.search(pattern, \"Python is great\")  # Match\nre.search(pattern, \"I love Python\")    # No match\n\npattern = r\"Python$\"  # Matches \"Python\" only at the end\nre.search(pattern, \"I love Python\")    # Match\nre.search(pattern, \"Python is great\")  # No match\n\npattern = r\"\\bcat\\b\"  # Matches the word \"cat\" with boundaries\nre.search(pattern, \"The cat sits\")     # Match\nre.search(pattern, \"category\")         # No match\n</code></pre>"},{"location":"resources/python-regex-cheatsheet/#the-dual-meaning-of","title":"The Dual Meaning of ^","text":"<p>It's important to note that the caret symbol <code>^</code> has two distinct meanings in regular expressions:</p> <ol> <li> <p>Outside square brackets: When used at the beginning of a pattern, it's an anchor that matches the start of a string or line.    </p><pre><code>pattern = r\"^abc\"  # Matches \"abc\" only at the start of the string\n</code></pre><p></p> </li> <li> <p>Inside square brackets: When used as the first character inside square brackets, it negates the character class, meaning \"match any character EXCEPT these.\"    </p><pre><code>pattern = r\"[^0-9]\"  # Matches any character that is NOT a digit\n</code></pre><p></p> </li> </ol> <p>This distinction is crucial when reading and writing regex patterns. For example, in an email validation pattern: </p><pre><code>email_pattern = r\"^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$\"\n</code></pre> The first <code>^</code> indicates the match must start at the beginning of the string, while in <code>[a-zA-Z0-9._%+-]</code> the characters are simply a positive character class defining what's allowed in the username portion of the email.<p></p>"},{"location":"resources/python-regex-cheatsheet/#quantifiers","title":"Quantifiers","text":"<p>Quantifiers specify how many times a pattern should match:</p> <pre><code>*      # 0 or more repetitions\n+      # 1 or more repetitions\n?      # 0 or 1 repetition\n{n}    # Exactly n repetitions\n{n,}   # n or more repetitions\n{n,m}  # Between n and m repetitions\n\n# Examples\npattern = r\"ab*c\"     # Matches \"ac\", \"abc\", \"abbc\", etc.\npattern = r\"ab+c\"     # Matches \"abc\", \"abbc\", etc. (not \"ac\")\npattern = r\"colou?r\"  # Matches \"color\" or \"colour\"\npattern = r\"\\d{2,4}\"  # Matches 2 to 4 digits\n</code></pre>"},{"location":"resources/python-regex-cheatsheet/#greedy-vs-non-greedy","title":"Greedy vs. Non-Greedy","text":"<p>Quantifiers are greedy by default (match as much as possible). Adding <code>?</code> after a quantifier makes it non-greedy.</p> <pre><code>text = \"&lt;div&gt;Content&lt;/div&gt;&lt;div&gt;More&lt;/div&gt;\"\n\n# Greedy matching\npattern = r\"&lt;div&gt;.*&lt;/div&gt;\"\nre.findall(pattern, text)  # Returns ['&lt;div&gt;Content&lt;/div&gt;&lt;div&gt;More&lt;/div&gt;']\n\n# Non-greedy matching\npattern = r\"&lt;div&gt;.*?&lt;/div&gt;\"\nre.findall(pattern, text)  # Returns ['&lt;div&gt;Content&lt;/div&gt;', '&lt;div&gt;More&lt;/div&gt;']\n</code></pre>"},{"location":"resources/python-regex-cheatsheet/#grouping-and-capturing","title":"Grouping and Capturing","text":"<p>Parentheses <code>( )</code> create capture groups:</p> <pre><code>pattern = r\"(\\d{3})-(\\d{3})-(\\d{4})\"\nmatch = re.search(pattern, \"Phone: 123-456-7890\")\nmatch.group(0)  # Entire match: \"123-456-7890\"\nmatch.group(1)  # First group: \"123\"\nmatch.group(2)  # Second group: \"456\"\nmatch.group(3)  # Third group: \"7890\"\nmatch.groups()  # All groups: (\"123\", \"456\", \"7890\")\n</code></pre>"},{"location":"resources/python-regex-cheatsheet/#named-groups","title":"Named Groups","text":"<p>Use <code>(?P&lt;name&gt;...)</code> for named groups:</p> <pre><code>pattern = r\"(?P&lt;area&gt;\\d{3})-(?P&lt;prefix&gt;\\d{3})-(?P&lt;line&gt;\\d{4})\"\nmatch = re.search(pattern, \"Phone: 123-456-7890\")\nmatch.group(\"area\")    # \"123\"\nmatch.group(\"prefix\")  # \"456\"\nmatch.group(\"line\")    # \"7890\"\n</code></pre>"},{"location":"resources/python-regex-cheatsheet/#non-capturing-groups","title":"Non-Capturing Groups","text":"<p>Use <code>(?:...)</code> for non-capturing groups:</p> <pre><code>pattern = r\"(?:\\d{3})-\\d{3}-(\\d{4})\"\nmatch = re.search(pattern, \"123-456-7890\")\nmatch.groups()  # Only contains (\"7890\"), the first part isn't captured\n</code></pre>"},{"location":"resources/python-regex-cheatsheet/#alternation","title":"Alternation","text":"<p>The pipe symbol <code>|</code> works as an OR operator:</p> <pre><code>pattern = r\"cat|dog\"\nre.findall(pattern, \"I have a cat and a dog\")  # Returns ['cat', 'dog']\n\n# Grouped alternation\npattern = r\"(cat|dog)s?\"\nre.findall(pattern, \"I have cats and a dog\")  # Returns ['cat', 'dog']\n</code></pre>"},{"location":"resources/python-regex-cheatsheet/#lookahead-and-lookbehind","title":"Lookahead and Lookbehind","text":"<p>These are zero-width assertions that don't consume characters:</p> <pre><code># Positive lookahead (?=...): Match if followed by pattern\npattern = r\"\\w+(?=\\s+is)\"\nre.findall(pattern, \"Python is great, Java is powerful\")  # ['Python', 'Java']\n\n# Negative lookahead (?!...): Match if NOT followed by pattern\npattern = r\"Python(?!\\s+3)\"\nre.search(pattern, \"Python 2.7\")  # Match\nre.search(pattern, \"Python 3.9\")  # No match\n\n# Positive lookbehind (?&lt;=...): Match if preceded by pattern\npattern = r\"(?&lt;=\\$)\\d+\"\nre.findall(pattern, \"Items: $10, $25, \u20ac30\")  # ['10', '25']\n\n# Negative lookbehind (?&lt;!...): Match if NOT preceded by pattern\npattern = r\"(?&lt;!\\$)\\d+\"\nre.findall(pattern, \"$10, 20, $30\")  # ['0', '20', '0']\n</code></pre>"},{"location":"resources/python-regex-cheatsheet/#common-functions","title":"Common Functions","text":""},{"location":"resources/python-regex-cheatsheet/#research","title":"re.search()","text":"<p>Finds the first match of the pattern:</p> <pre><code>result = re.search(r\"\\d+\", \"abc123def456\")\nif result:\n    print(result.group())  # \"123\"\n</code></pre>"},{"location":"resources/python-regex-cheatsheet/#rematch","title":"re.match()","text":"<p>Matches pattern only at the beginning of the string:</p> <pre><code>re.match(r\"\\d+\", \"123abc\")   # Match\nre.match(r\"\\d+\", \"abc123\")   # No match\n</code></pre>"},{"location":"resources/python-regex-cheatsheet/#refindall","title":"re.findall()","text":"<p>Returns all non-overlapping matches as a list:</p> <pre><code>re.findall(r\"\\d+\", \"abc123def456\")  # Returns ['123', '456']\n</code></pre>"},{"location":"resources/python-regex-cheatsheet/#refinditer","title":"re.finditer()","text":"<p>Returns an iterator of match objects:</p> <pre><code>for match in re.finditer(r\"\\d+\", \"abc123def456\"):\n    print(match.group(), match.span())  # \"123\" (3, 6), \"456\" (9, 12)\n</code></pre>"},{"location":"resources/python-regex-cheatsheet/#resub","title":"re.sub()","text":"<p>Substitutes matches with a replacement:</p> <pre><code># Basic substitution\nre.sub(r\"\\d+\", \"NUM\", \"abc123def456\")  # Returns \"abcNUMdefNUM\"\n\n# Using backreferences\nre.sub(r\"(\\d{3})-(\\d{3})-(\\d{4})\", r\"(\\1) \\2-\\3\", \"123-456-7890\")\n# Returns \"(123) 456-7890\"\n\n# Using a function for replacement\ndef double_digits(match):\n    return str(int(match.group()) * 2)\n\nre.sub(r\"\\d+\", double_digits, \"abc123def456\")  # Returns \"abc246def912\"\n</code></pre>"},{"location":"resources/python-regex-cheatsheet/#resplit","title":"re.split()","text":"<p>Splits a string by pattern matches:</p> <pre><code>re.split(r\"\\s+\", \"Split   these words\")  # Returns ['Split', 'these', 'words']\nre.split(r\"[,;]\", \"apple,orange;banana\")  # Returns ['apple', 'orange', 'banana']\n</code></pre>"},{"location":"resources/python-regex-cheatsheet/#flags","title":"Flags","text":"<p>Modify regex behavior with flags:</p> <pre><code>re.IGNORECASE  # or re.I: Case-insensitive matching\nre.search(r\"python\", \"Python\", re.IGNORECASE)  # Match\n\nre.MULTILINE  # or re.M: ^ and $ match start/end of each line\ntext = \"Line 1\\nLine 2\"\nre.findall(r\"^Line\", text, re.MULTILINE)  # Returns ['Line', 'Line']\n\nre.DOTALL  # or re.S: Dot matches any character including newline\nre.search(r\"Line 1.+Line 2\", \"Line 1\\nLine 2\", re.DOTALL)  # Match\n\nre.VERBOSE  # or re.X: Allow whitespace and comments in pattern\npattern = re.compile(r\"\"\"\n    \\d{3}  # Area code\n    [-.]?  # Optional separator\n    \\d{3}  # Prefix\n    [-.]?  # Optional separator\n    \\d{4}  # Line number\n    \"\"\", re.VERBOSE)\n\n# Multiple flags\nre.search(pattern, text, re.IGNORECASE | re.MULTILINE)\n</code></pre>"},{"location":"resources/python-regex-cheatsheet/#raw-strings","title":"Raw Strings","text":"<p>Use raw strings (<code>r\"...\"</code>) to avoid issues with backslashes:</p> <pre><code># Without raw string\nre.search(\"\\\\d+\", \"123\")  # Need double backslash\n\n# With raw string (recommended)\nre.search(r\"\\d+\", \"123\")  # Much cleaner\n</code></pre>"},{"location":"resources/python-regex-cheatsheet/#practical-examples","title":"Practical Examples","text":""},{"location":"resources/python-regex-cheatsheet/#email-validation","title":"Email Validation","text":"<pre><code>email_pattern = r\"^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$\"\nre.match(email_pattern, \"user@example.com\")  # Match\nre.match(email_pattern, \"invalid@email\")     # No match\n</code></pre>"},{"location":"resources/python-regex-cheatsheet/#url-extraction","title":"URL Extraction","text":"<pre><code>url_pattern = r\"https?://(?:www\\.)?[-a-zA-Z0-9@:%._+~#=]{1,256}\\.[a-zA-Z0-9()]{1,6}\\b(?:[-a-zA-Z0-9()@:%_+.~#?&amp;/=]*)\"\nre.findall(url_pattern, \"Visit https://example.com and http://test.org\")\n# Returns ['https://example.com', 'http://test.org']\n</code></pre>"},{"location":"resources/python-regex-cheatsheet/#date-formatting","title":"Date Formatting","text":"<pre><code># Convert MM/DD/YYYY to YYYY-MM-DD\ndate_text = \"Today's date: 12/25/2023\"\nre.sub(r\"(\\d{2})/(\\d{2})/(\\d{4})\", r\"\\3-\\1-\\2\", date_text)\n# Returns \"Today's date: 2023-12-25\"\n</code></pre>"},{"location":"resources/python-regex-cheatsheet/#password-validation","title":"Password Validation","text":"<pre><code># At least 8 chars with 1+ uppercase, 1+ lowercase, 1+ digit, 1+ special char\npassword_pattern = r\"^(?=.*[a-z])(?=.*[A-Z])(?=.*\\d)(?=.*[@$!%*?&amp;])[A-Za-z\\d@$!%*?&amp;]{8,}$\"\nre.match(password_pattern, \"Passw0rd!\")  # Match\nre.match(password_pattern, \"password\")   # No match\n</code></pre>"},{"location":"resources/python-regex-cheatsheet/#stripping-html-tags","title":"Stripping HTML Tags","text":"<pre><code>html_text = \"&lt;p&gt;This is &lt;b&gt;bold&lt;/b&gt; text.&lt;/p&gt;\"\nre.sub(r\"&lt;[^&gt;]*&gt;\", \"\", html_text)  # Returns \"This is bold text.\"\n</code></pre>"},{"location":"resources/python-regex-cheatsheet/#extracting-quoted-text","title":"Extracting Quoted Text","text":"<pre><code>text = 'She said \"hello\" and he replied \"goodbye\"'\nre.findall(r'\"([^\"]*)\"', text)  # Returns ['hello', 'goodbye']\n</code></pre>"},{"location":"resources/python-regex-cheatsheet/#performance-tips","title":"Performance Tips","text":"<ol> <li> <p>Compile patterns for repeated use:    </p><pre><code>pattern = re.compile(r\"\\d+\")\npattern.findall(\"123 456\")  # More efficient for multiple operations\n</code></pre><p></p> </li> <li> <p>Avoid unnecessary backtracking:    </p><pre><code># Instead of r\"a.*z\"\n# Use r\"a[^z]*z\" if possible\n</code></pre><p></p> </li> <li> <p>Use non-capturing groups when you don't need the captured text:    </p><pre><code># Instead of r\"(pattern)\"\n# Use r\"(?:pattern)\" when you don't need to reference the group\n</code></pre><p></p> </li> <li> <p>Be specific rather than using broad patterns:    </p><pre><code># Instead of r\".*\"\n# Use r\"\\d+\" if you're specifically looking for digits\n</code></pre><p></p> </li> <li> <p>Use appropriate anchors to limit search space:    </p><pre><code># Instead of r\"pattern\"\n# Use r\"^pattern$\" if you want to match the entire string\n</code></pre><p></p> </li> </ol>"},{"location":"resources/python-sets-cheatsheet/","title":"Python Sets Cheatsheet","text":""},{"location":"resources/python-sets-cheatsheet/#python-sets-cheatsheet","title":"Python Sets Cheatsheet","text":"<p>Sets are unordered collections of unique elements in Python. They offer powerful operations for mathematical set operations like unions, intersections, and differences. This cheatsheet covers all essential set operations and methods with clear examples.</p>"},{"location":"resources/python-sets-cheatsheet/#creating-sets","title":"Creating Sets","text":"<pre><code># Empty set (cannot use {} as that creates an empty dictionary)\nempty_set = set()\n\n# Set with initial values\ncolors = {\"red\", \"green\", \"blue\"}\n\n# From other iterables\nnumbers = set([1, 2, 3, 2, 1])  # {1, 2, 3} (duplicates removed)\nletters = set(\"hello\")  # {'h', 'e', 'l', 'o'} (only unique letters)\n\n# Set comprehension\neven_numbers = {x for x in range(10) if x % 2 == 0}  # {0, 2, 4, 6, 8}\n</code></pre>"},{"location":"resources/python-sets-cheatsheet/#basic-operations","title":"Basic Operations","text":""},{"location":"resources/python-sets-cheatsheet/#adding-elements","title":"Adding Elements","text":"<pre><code>fruits = {\"apple\", \"banana\"}\n\n# Add a single element\nfruits.add(\"orange\")  # {\"apple\", \"banana\", \"orange\"}\n\n# Adding a duplicate has no effect\nfruits.add(\"apple\")  # Still {\"apple\", \"banana\", \"orange\"}\n\n# Add multiple elements\nfruits.update([\"mango\", \"grape\"])  # {\"apple\", \"banana\", \"orange\", \"mango\", \"grape\"}\nfruits.update((\"pear\", \"kiwi\"), {\"plum\", \"peach\"})  # Can update from multiple iterables\n</code></pre>"},{"location":"resources/python-sets-cheatsheet/#removing-elements","title":"Removing Elements","text":"<pre><code>colors = {\"red\", \"green\", \"blue\", \"yellow\"}\n\n# remove() - raises KeyError if element doesn't exist\ncolors.remove(\"green\")  # {\"red\", \"blue\", \"yellow\"}\n\n# discard() - no error if element doesn't exist\ncolors.discard(\"purple\")  # No change, no error\ncolors.discard(\"yellow\")  # {\"red\", \"blue\"}\n\n# pop() - removes and returns an arbitrary element\nitem = colors.pop()  # Could be \"red\" or \"blue\"\n\n# clear() - removes all elements\ncolors.clear()  # set() (empty set)\n</code></pre>"},{"location":"resources/python-sets-cheatsheet/#checking-membership","title":"Checking Membership","text":"<pre><code>fruits = {\"apple\", \"banana\", \"orange\"}\n\n# Using 'in' operator\n\"apple\" in fruits  # True\n\"mango\" in fruits  # False\n\"mango\" not in fruits  # True\n\n# Checking if set is empty\nbool(fruits)  # True if not empty, False if empty\nlen(fruits) == 0  # True if empty\n</code></pre>"},{"location":"resources/python-sets-cheatsheet/#set-methods-vs-operators","title":"Set Methods vs. Operators","text":"<p>Python sets provide both methods and overloaded operators for set operations. Here's a comparison:</p> Operation Method Operator Description Union <code>a.union(b, ...)</code> <code>a \\| b \\| ...</code> Elements in either set Intersection <code>a.intersection(b, ...)</code> <code>a &amp; b &amp; ...</code> Elements in all sets Difference <code>a.difference(b, ...)</code> <code>a - b - ...</code> Elements in <code>a</code> but not in others Symmetric Difference <code>a.symmetric_difference(b)</code> <code>a ^ b</code> Elements in either set but not in both Subset <code>a.issubset(b)</code> <code>a &lt;= b</code> True if all elements of <code>a</code> are in <code>b</code> Proper Subset <code>a &lt; b</code> True if <code>a</code> is a subset of <code>b</code> and <code>a != b</code> Superset <code>a.issuperset(b)</code> <code>a &gt;= b</code> True if all elements of <code>b</code> are in <code>a</code> Proper Superset <code>a &gt; b</code> True if <code>a</code> is a superset of <code>b</code> and <code>a != b</code> Disjoint <code>a.isdisjoint(b)</code> True if <code>a</code> and <code>b</code> have no elements in common"},{"location":"resources/python-sets-cheatsheet/#set-operations-with-methods","title":"Set Operations with Methods","text":""},{"location":"resources/python-sets-cheatsheet/#union","title":"Union","text":"<pre><code>a = {1, 2, 3}\nb = {3, 4, 5}\nc = {5, 6, 7}\n\n# Union of two sets\na.union(b)  # {1, 2, 3, 4, 5}\n\n# Union of multiple sets\na.union(b, c)  # {1, 2, 3, 4, 5, 6, 7}\n\n# In-place union (modifies a)\na.update(b)  # a is now {1, 2, 3, 4, 5}\n</code></pre>"},{"location":"resources/python-sets-cheatsheet/#intersection","title":"Intersection","text":"<pre><code>a = {1, 2, 3, 4}\nb = {3, 4, 5, 6}\nc = {4, 5, 6, 7}\n\n# Intersection of two sets\na.intersection(b)  # {3, 4}\n\n# Intersection of multiple sets\na.intersection(b, c)  # {4}\n\n# In-place intersection (modifies a)\na.intersection_update(b)  # a is now {3, 4}\n</code></pre>"},{"location":"resources/python-sets-cheatsheet/#difference","title":"Difference","text":"<pre><code>a = {1, 2, 3, 4, 5}\nb = {4, 5, 6}\nc = {4, 7}\n\n# Difference between two sets\na.difference(b)  # {1, 2, 3}\n\n# Difference between multiple sets\na.difference(b, c)  # {1, 2, 3}\n\n# In-place difference (modifies a)\na.difference_update(b)  # a is now {1, 2, 3}\n</code></pre>"},{"location":"resources/python-sets-cheatsheet/#symmetric-difference","title":"Symmetric Difference","text":"<pre><code>a = {1, 2, 3, 4}\nb = {3, 4, 5, 6}\n\n# Symmetric difference (elements in either set but not in both)\na.symmetric_difference(b)  # {1, 2, 5, 6}\n\n# In-place symmetric difference (modifies a)\na.symmetric_difference_update(b)  # a is now {1, 2, 5, 6}\n</code></pre>"},{"location":"resources/python-sets-cheatsheet/#subset-superset-and-disjoint-tests","title":"Subset, Superset, and Disjoint Tests","text":"<pre><code>a = {1, 2}\nb = {1, 2, 3, 4}\nc = {5, 6}\n\n# Subset\na.issubset(b)  # True (all elements of a are in b)\nb.issubset(a)  # False\n\n# Superset\nb.issuperset(a)  # True (all elements of a are in b)\na.issuperset(b)  # False\n\n# Disjoint (no common elements)\na.isdisjoint(c)  # True (a and c have no common elements)\na.isdisjoint(b)  # False (a and b share elements)\n</code></pre>"},{"location":"resources/python-sets-cheatsheet/#set-operations-with-operators","title":"Set Operations with Operators","text":""},{"location":"resources/python-sets-cheatsheet/#union-with","title":"Union with |","text":"<pre><code>a = {1, 2, 3}\nb = {3, 4, 5}\nc = {5, 6, 7}\n\n# Union using the | operator\na | b  # {1, 2, 3, 4, 5}\n\n# Union of multiple sets\na | b | c  # {1, 2, 3, 4, 5, 6, 7}\n\n# In-place union using |=\na |= b  # a is now {1, 2, 3, 4, 5}\n</code></pre>"},{"location":"resources/python-sets-cheatsheet/#intersection-with","title":"Intersection with &amp;","text":"<pre><code>a = {1, 2, 3, 4}\nb = {3, 4, 5, 6}\nc = {4, 5, 6, 7}\n\n# Intersection using the &amp; operator\na &amp; b  # {3, 4}\n\n# Intersection of multiple sets\na &amp; b &amp; c  # {4}\n\n# In-place intersection using &amp;=\na &amp;= b  # a is now {3, 4}\n</code></pre>"},{"location":"resources/python-sets-cheatsheet/#difference-with-","title":"Difference with -","text":"<pre><code>a = {1, 2, 3, 4, 5}\nb = {4, 5, 6}\nc = {4, 7}\n\n# Difference using the - operator\na - b  # {1, 2, 3}\n\n# Difference of multiple sets\na - b - c  # {1, 2, 3}\n\n# In-place difference using -=\na -= b  # a is now {1, 2, 3}\n</code></pre>"},{"location":"resources/python-sets-cheatsheet/#symmetric-difference-with","title":"Symmetric Difference with ^","text":"<pre><code>a = {1, 2, 3, 4}\nb = {3, 4, 5, 6}\n\n# Symmetric difference using the ^ operator\na ^ b  # {1, 2, 5, 6}\n\n# In-place symmetric difference using ^=\na ^= b  # a is now {1, 2, 5, 6}\n</code></pre>"},{"location":"resources/python-sets-cheatsheet/#comparison-operators","title":"Comparison Operators","text":"<pre><code>a = {1, 2}\nb = {1, 2, 3, 4}\nc = {1, 2}\n\n# Subset\na &lt;= b  # True (a is a subset of b)\na &lt; b   # True (a is a proper subset of b)\na &lt;= c  # True (a is a subset of c)\na &lt; c   # False (a equals c, not a proper subset)\n\n# Superset\nb &gt;= a  # True (b is a superset of a)\nb &gt; a   # True (b is a proper superset of a)\nc &gt;= a  # True (c is a superset of a)\nc &gt; a   # False (c equals a, not a proper superset)\n\n# Equality\na == c  # True (a and c contain the same elements)\na != b  # True (a and b are different)\n</code></pre>"},{"location":"resources/python-sets-cheatsheet/#other-useful-set-methods","title":"Other Useful Set Methods","text":""},{"location":"resources/python-sets-cheatsheet/#copy","title":"copy()","text":"<pre><code>a = {1, 2, 3}\nb = a.copy()  # Creates a new set with the same elements\n\n# Changes to b don't affect a\nb.add(4)  # b is now {1, 2, 3, 4}, a is still {1, 2, 3}\n</code></pre>"},{"location":"resources/python-sets-cheatsheet/#immutable-sets-frozenset","title":"Immutable Sets (frozenset)","text":"<pre><code># Regular sets are mutable and can't be used as dictionary keys\nregular_set = {1, 2, 3}\n\n# Frozensets are immutable (can't be changed after creation)\nfrozen = frozenset([1, 2, 3])\n\n# This works\ndictionary = {frozen: \"This is a valid key\"}\n\n# This raises TypeError\n# dictionary = {regular_set: \"This will fail\"}\n\n# Frozensets support the same operations as regular sets,\n# except methods that modify the set\nresult = frozen.union({4, 5})  # Creates a new frozenset\n# frozen.add(4)  # This would raise an AttributeError\n</code></pre>"},{"location":"resources/python-sets-cheatsheet/#practical-examples","title":"Practical Examples","text":""},{"location":"resources/python-sets-cheatsheet/#removing-duplicates","title":"Removing Duplicates","text":"<pre><code># Remove duplicates from a list while preserving order\ndef remove_duplicates(items):\n    return list(dict.fromkeys(items))\n\n# Alternative with sets (doesn't preserve order)\ndef remove_duplicates_set(items):\n    return list(set(items))\n\nitems = [3, 1, 2, 1, 3, 4, 2]\nremove_duplicates(items)      # [3, 1, 2, 4]\nremove_duplicates_set(items)  # Order not guaranteed\n</code></pre>"},{"location":"resources/python-sets-cheatsheet/#finding-common-and-unique-elements","title":"Finding Common and Unique Elements","text":"<pre><code>list1 = [1, 2, 3, 4, 5]\nlist2 = [4, 5, 6, 7, 8]\n\n# Find elements in both lists\ncommon = set(list1) &amp; set(list2)  # {4, 5}\n\n# Find elements unique to first list\nonly_in_list1 = set(list1) - set(list2)  # {1, 2, 3}\n\n# Find elements unique to second list\nonly_in_list2 = set(list2) - set(list1)  # {6, 7, 8}\n\n# Find elements in either list but not both\nsymmetric_diff = set(list1) ^ set(list2)  # {1, 2, 3, 6, 7, 8}\n\n# Union (all unique elements from both lists)\nall_elements = set(list1) | set(list2)  # {1, 2, 3, 4, 5, 6, 7, 8}\n</code></pre>"},{"location":"resources/python-sets-cheatsheet/#set-operations-on-strings","title":"Set Operations on Strings","text":"<pre><code>string1 = \"hello\"\nstring2 = \"world\"\n\n# Find common characters\ncommon_chars = set(string1) &amp; set(string2)  # {'l', 'o'}\n\n# Find characters unique to first string\nonly_in_string1 = set(string1) - set(string2)  # {'h', 'e'}\n\n# Find characters unique to second string\nonly_in_string2 = set(string2) - set(string1)  # {'w', 'r', 'd'}\n\n# All unique characters from both strings\nall_chars = set(string1) | set(string2)  # {'h', 'e', 'l', 'o', 'w', 'r', 'd'}\n</code></pre>"},{"location":"resources/python-sets-cheatsheet/#power-set-all-possible-subsets","title":"Power Set (All Possible Subsets)","text":"<pre><code>def power_set(s):\n    \"\"\"Return the power set of set s (all possible subsets)\"\"\"\n    result = [frozenset()]  # Start with the empty set\n    for x in s:\n        # For each element, add a new subset by including it in each existing subset\n        result.extend([subset | {x} for subset in result])\n    return result\n\ns = {1, 2, 3}\nsubsets = power_set(s)\n# Returns: [frozenset(), frozenset({1}), frozenset({2}), frozenset({1, 2}), \n#           frozenset({3}), frozenset({1, 3}), frozenset({2, 3}), frozenset({1, 2, 3})]\n</code></pre>"},{"location":"resources/python-sets-cheatsheet/#grouping-elements","title":"Grouping Elements","text":"<pre><code>data = [\n    {\"name\": \"Alice\", \"department\": \"Engineering\"},\n    {\"name\": \"Bob\", \"department\": \"Marketing\"},\n    {\"name\": \"Charlie\", \"department\": \"Engineering\"},\n    {\"name\": \"Diana\", \"department\": \"HR\"}\n]\n\n# Group employees by department\ndepartments = {}\nfor employee in data:\n    dept = employee[\"department\"]\n    if dept not in departments:\n        departments[dept] = set()\n    departments[dept].add(employee[\"name\"])\n\n# Result: {'Engineering': {'Alice', 'Charlie'}, 'Marketing': {'Bob'}, 'HR': {'Diana'}}\n</code></pre>"},{"location":"resources/python-sets-cheatsheet/#performance-considerations","title":"Performance Considerations","text":"<ol> <li> <p>Set operations are optimized:    Set operations like union, intersection, and difference are highly optimized and often faster than equivalent operations with lists.</p> </li> <li> <p>Membership testing is O(1):    Checking if an element is in a set (<code>x in s</code>) is much faster than checking in a list, especially for large collections.</p> </li> <li> <p>Sets require hashable elements:    Elements in a set must be hashable (immutable). Lists, dictionaries, and other sets cannot be elements of a regular set.</p> </li> <li> <p>Memory usage:    Sets typically use more memory than lists with the same elements due to their hash table implementation.</p> </li> <li> <p>Set comprehensions:    For creating sets from other iterables with transformations, set comprehensions are often more efficient and readable than loops.</p> </li> </ol>"},{"location":"resources/python-sets-cheatsheet/#when-to-use-sets","title":"When to Use Sets","text":"<p>Sets are ideal when you need:</p> <ol> <li>Uniqueness: Automatically eliminate duplicates</li> <li>Fast membership testing: Check if elements exist in a collection</li> <li>Mathematical set operations: Union, intersection, difference</li> <li>Removing duplicates from sequences (though order is not preserved)</li> <li>Finding common or unique items between collections</li> </ol> <p>However, sets are not suitable when: 1. You need to maintain order (use lists or OrderedDict) 2. You need to store duplicate elements (use lists) 3. You need to store unhashable objects (use lists)</p>"},{"location":"resources/python-sets-cheatsheet/#appendix-understanding-subset-and-superset-relationships","title":"Appendix: Understanding Subset and Superset Relationships","text":"<p>This appendix provides a deeper understanding of subset and superset relationships, including the distinction between subset/superset and proper subset/proper superset.</p>"},{"location":"resources/python-sets-cheatsheet/#subset-vs-proper-subset","title":"Subset vs. Proper Subset","text":""},{"location":"resources/python-sets-cheatsheet/#subset","title":"Subset (\u2286)","text":"<p>A set A is a subset of set B if every element in A is also in B.</p> <p>Key characteristics: - Every element of A must be in B - A and B can be equal (contain the same elements) - An empty set is a subset of any set - Every set is a subset of itself</p> <p>In Python, subset relationship is tested with the <code>&lt;=</code> operator or the <code>issubset()</code> method:</p> <pre><code>a = {1, 2}\nb = {1, 2, 3, 4}\nc = {1, 2}\n\n# Using operators\na &lt;= b  # True (a is a subset of b)\na &lt;= c  # True (a equals c, so a is a subset of c)\n\n# Using methods\na.issubset(b)  # True\na.issubset(c)  # True\n</code></pre>"},{"location":"resources/python-sets-cheatsheet/#proper-subset","title":"Proper Subset (\u2282)","text":"<p>A set A is a proper subset of set B if A is a subset of B, but A \u2260 B.</p> <p>Key characteristics: - Every element of A must be in B - B must contain at least one element that is not in A - A and B cannot be equal - A set is never a proper subset of itself</p> <p>In Python, proper subset relationship is tested with the <code>&lt;</code> operator, but there's no direct method equivalent. You can combine methods to check for proper subset:</p> <pre><code>a = {1, 2}\nb = {1, 2, 3, 4}\nc = {1, 2}\n\n# Using operators\na &lt; b  # True (a is a proper subset of b)\na &lt; c  # False (a equals c, so not a proper subset)\n\n# Using methods (combined approach)\na.issubset(b) and a != b  # True\na.issubset(c) and a != c  # False\n\n# Alternative using length\na.issubset(b) and len(a) &lt; len(b)  # True\na.issubset(c) and len(a) &lt; len(c)  # False\n\n# Alternative using set difference\na.issubset(b) and bool(b - a)  # True\na.issubset(c) and bool(c - a)  # False\n</code></pre>"},{"location":"resources/python-sets-cheatsheet/#superset-vs-proper-superset","title":"Superset vs. Proper Superset","text":""},{"location":"resources/python-sets-cheatsheet/#superset","title":"Superset (\u2287)","text":"<p>A set A is a superset of set B if every element in B is also in A.</p> <p>Key characteristics: - Every element of B must be in A - A and B can be equal - Any set is a superset of the empty set - Every set is a superset of itself</p> <p>In Python, superset relationship is tested with the <code>&gt;=</code> operator or the <code>issuperset()</code> method:</p> <pre><code>a = {1, 2, 3, 4}\nb = {1, 2}\nc = {1, 2, 3, 4}\n\n# Using operators\na &gt;= b  # True (a is a superset of b)\na &gt;= c  # True (a equals c, so a is a superset of c)\n\n# Using methods\na.issuperset(b)  # True\na.issuperset(c)  # True\n</code></pre>"},{"location":"resources/python-sets-cheatsheet/#proper-superset","title":"Proper Superset (\u2283)","text":"<p>A set A is a proper superset of set B if A is a superset of B, but A \u2260 B.</p> <p>Key characteristics: - Every element of B must be in A - A must contain at least one element that is not in B - A and B cannot be equal - A set is never a proper superset of itself</p> <p>In Python, proper superset relationship is tested with the <code>&gt;</code> operator, but there's no direct method equivalent. You can combine methods to check for proper superset:</p> <pre><code>a = {1, 2, 3, 4}\nb = {1, 2}\nc = {1, 2, 3, 4}\n\n# Using operators\na &gt; b  # True (a is a proper superset of b)\na &gt; c  # False (a equals c, so not a proper superset)\n\n# Using methods (combined approach)\na.issuperset(b) and a != b  # True\na.issuperset(c) and a != c  # False\n\n# Alternative using length\na.issuperset(b) and len(a) &gt; len(b)  # True\na.issuperset(c) and len(a) &gt; len(c)  # False\n\n# Alternative using set difference\na.issuperset(b) and bool(a - b)  # True\na.issuperset(c) and bool(a - c)  # False\n</code></pre>"},{"location":"resources/python-sets-cheatsheet/#helper-functions-for-proper-subsetsuperset-tests","title":"Helper Functions for Proper Subset/Superset Tests","text":"<p>If you need to check for proper subset/superset relationships frequently, you might consider creating helper functions:</p> <pre><code>def is_proper_subset(a, b):\n    \"\"\"Check if set a is a proper subset of set b.\"\"\"\n    return a.issubset(b) and a != b\n\ndef is_proper_superset(a, b):\n    \"\"\"Check if set a is a proper superset of set b.\"\"\"\n    return a.issuperset(b) and a != b\n\n# Usage\na = {1, 2}\nb = {1, 2, 3, 4}\nprint(is_proper_subset(a, b))    # True\nprint(is_proper_superset(b, a))  # True\n</code></pre>"},{"location":"resources/python-sets-cheatsheet/#visual-representation","title":"Visual Representation","text":"<p>To visualize these relationships:</p> <ol> <li>Subset (A \u2286 B):</li> <li>Set A is fully contained within set B</li> <li> <p>The circles representing A and B can be identical</p> </li> <li> <p>Proper Subset (A \u2282 B):</p> </li> <li>Set A is fully contained within set B</li> <li>Set B has additional elements not in A</li> <li> <p>The circles cannot be identical</p> </li> <li> <p>Superset (A \u2287 B):</p> </li> <li>Set A fully contains set B</li> <li> <p>The circles representing A and B can be identical</p> </li> <li> <p>Proper Superset (A \u2283 B):</p> </li> <li>Set A fully contains set B</li> <li>Set A has additional elements not in B</li> <li>The circles cannot be identical</li> </ol>"},{"location":"resources/python-strings-cheatsheet/","title":"Python Strings Cheatsheet","text":""},{"location":"resources/python-strings-cheatsheet/#python-strings-cheatsheet","title":"Python Strings Cheatsheet","text":"<p>Strings in Python are immutable sequences of characters. This cheatsheet covers all essential string operations, methods, and formatting techniques with clear examples and explanations.</p>"},{"location":"resources/python-strings-cheatsheet/#creating-strings","title":"Creating Strings","text":"<pre><code># String literals\nsingle_quoted = 'Hello, world!'\ndouble_quoted = \"Hello, world!\"\n\n# How Python handles string literals internally:\n\"\"\"\nIn Python 3, all string literals are stored as Unicode by default, not with any specific encoding.\nPython strings are sequences of Unicode code points (abstract characters), not bytes.\nThis means when you type:\n    s = \"Hello, world!\"\nPython stores this as Unicode code points, not in any specific encoding like UTF-8 or ASCII.\n\nEncoding only happens when you:\n1. Convert the string to bytes using s.encode()\n2. Write the string to a file or external system\n3. Send the string over a network connection\n\nUntil then, Python handles all strings as abstract Unicode sequences internally,\nregardless of whether they contain ASCII characters, Japanese, emoji, or other symbols.\n\"\"\"\n\n# Triple-quoted strings (can span multiple lines)\nmulti_line = \"\"\"This is a string that\nspans multiple lines\nwith preserved line breaks.\"\"\"\n\n# Raw strings (ignore escape sequences)\nraw_string = r\"C:\\Users\\John\\Documents\"  # Backslashes are treated literally\n\n# Escape sequences in regular strings\nescaped_string = \"Line 1\\nLine 2\\tTabbed\"  # \\n for newline, \\t for tab\nquote_in_string = \"He said, \\\"Hello!\\\"\"    # Escaped quotes\n\n# Unicode characters\nunicode_string = \"\\u00A9 2023\"  # \u00a9 2023\nemoji = \"\\U0001F600\"            # \ud83d\ude00\n\n# Byte strings (sequence of bytes, not characters)\nbyte_string = b\"Hello\"          # Only ASCII characters allowed\nencoded = \"\u3053\u3093\u306b\u3061\u306f\".encode('utf-8')  # Convert string to bytes with specific encoding\n\n# String literals in source code files\n\"\"\"\nWhen Python reads your .py source file, it needs to know what encoding the file uses.\nBy default, Python 3 assumes UTF-8 encoding for source files.\nYou can specify a different encoding with a comment at the top of the file:\n    # -*- coding: encoding -*-\nFor example:\n    # -*- coding: latin-1 -*-\n\nThis only affects how Python reads your source code file, not how strings are stored in memory.\nOnce the string is parsed from the source file, it's stored as Unicode regardless of the\nsource file's encoding.\n\"\"\"\n</code></pre>"},{"location":"resources/python-strings-cheatsheet/#string-concatenation","title":"String Concatenation","text":"<pre><code># Using + operator\ngreeting = \"Hello\" + \", \" + \"world!\"  # \"Hello, world!\"\n\n# Using += for in-place concatenation\nmessage = \"Hello\"\nmessage += \", world!\"  # message is now \"Hello, world!\"\n\n# Using join() with a list of strings (most efficient for multiple strings)\nwords = [\"Hello\", \"world\", \"of\", \"Python\"]\nsentence = \" \".join(words)  # \"Hello world of Python\"\n\n# Implicit concatenation of string literals (no + needed)\nlong_string = (\"This is a long string that \"\n               \"is split across multiple lines \"\n               \"in the source code.\")  # Single string with no newlines\n\n# String multiplication\nrepeated = \"abc\" * 3  # \"abcabcabc\"\nseparator = \"-\" * 20  # \"--------------------\"\n</code></pre>"},{"location":"resources/python-strings-cheatsheet/#accessing-characters","title":"Accessing Characters","text":"<pre><code>message = \"Hello, world!\"\n\n# Indexing (0-based)\nfirst_char = message[0]     # \"H\"\nlast_char = message[-1]     # \"!\"\n\n# Slicing [start:stop:step]\nfirst_five = message[:5]    # \"Hello\"\nlast_five = message[-5:]    # \"orld!\"\nevery_other = message[::2]  # \"Hlo ol!\"\nreversed_str = message[::-1]  # \"!dlrow ,olleH\"\n\n# Length\nlength = len(message)  # 13\n\n# Remember: Strings are immutable\n# message[0] = \"h\"  # This raises TypeError\n</code></pre>"},{"location":"resources/python-strings-cheatsheet/#string-methods-for-searching","title":"String Methods for Searching","text":"<pre><code>text = \"Python is amazing and Python is easy to learn.\"\n\n# Finding substrings\nfirst_pos = text.find(\"Python\")    # 0 (first occurrence)\nsecond_pos = text.find(\"Python\", 1)  # 19 (starting from position 1)\nnot_found = text.find(\"Java\")      # -1 (not found)\n\n# rfind() searches from the end\nlast_pos = text.rfind(\"Python\")    # 19 (last occurrence)\n\n# index() is like find() but raises ValueError if not found\ntry:\n    pos = text.index(\"Python\")  # 0\n    pos = text.index(\"Java\")    # Raises ValueError\nexcept ValueError:\n    print(\"Substring not found\")\n\n# Counting occurrences\ncount = text.count(\"Python\")  # 2\ncount_is = text.count(\"is\")   # 2\n\n# Checking if string starts/ends with substring\nstarts_with = text.startswith(\"Python\")  # True\nends_with = text.endswith(\"learn.\")      # True\n\n# Checking prefix/suffix with offset\nstarts_with_at_19 = text.startswith(\"Python\", 19)  # True\nends_with_partial = text.endswith(\"easy\", 0, 30)   # True\n\n# Checking if substring is present\ncontains_python = \"Python\" in text  # True\ncontains_java = \"Java\" in text      # False\n</code></pre>"},{"location":"resources/python-strings-cheatsheet/#string-methods-for-case-conversion","title":"String Methods for Case Conversion","text":"<pre><code>text = \"Python is AMAZING!\"\n\n# Case conversion\nlower_case = text.lower()       # \"python is amazing!\"\nupper_case = text.upper()       # \"PYTHON IS AMAZING!\"\ntitle_case = text.title()       # \"Python Is Amazing!\"\ncapitalized = text.capitalize() # \"Python is amazing!\"\nswapped_case = text.swapcase()  # \"pYTHON IS amazing!\"\n\n# Checking case properties\nis_lower = text.islower()       # False\nis_upper = text.isupper()       # False\nis_title = text.istitle()       # False\n\n# Properly capitalizing titles\nimport string\ntitle = \"the lord of the rings\"\nwords = title.split()\ncapitalized_words = [word.capitalize() if word not in ['a', 'an', 'the', 'and', 'but', 'or', 'for', 'nor', 'on', 'at', 'to', 'from', 'by', 'in'] or i == 0 or i == len(words) - 1 else word for i, word in enumerate(words)]\nproper_title = \" \".join(capitalized_words)  # \"The Lord of the Rings\"\n</code></pre>"},{"location":"resources/python-strings-cheatsheet/#string-methods-for-stripping-and-padding","title":"String Methods for Stripping and Padding","text":"<pre><code># Removing whitespace\ntext = \"   Hello, world!   \"\nleft_strip = text.lstrip()     # \"Hello, world!   \"\nright_strip = text.rstrip()    # \"   Hello, world!\"\nboth_strip = text.strip()      # \"Hello, world!\"\n\n# Removing specific characters\ntext = \"###Hello, world!###\"\nstripped = text.strip('#')     # \"Hello, world!\"\nleft_stripped = text.lstrip('#')  # \"Hello, world!###\"\nright_stripped = text.rstrip('#') # \"###Hello, world!\"\n\n# Padding strings\ntext = \"Hello\"\nleft_pad = text.ljust(10)      # \"Hello     \"\nright_pad = text.rjust(10)     # \"     Hello\"\ncenter_pad = text.center(11)   # \"   Hello   \"\n\n# Padding with specific character\nleft_pad = text.ljust(10, '-')  # \"Hello-----\"\nright_pad = text.rjust(10, '-') # \"-----Hello\"\ncenter_pad = text.center(11, '-') # \"---Hello---\"\n\n# Zero-padding numbers\nnum = 42\nzero_padded = str(num).zfill(5)  # \"00042\"\n\n# Tabulation using expandtabs\ntabbed = \"Name\\tAge\\tCity\"\nexpanded = tabbed.expandtabs(15)  # \"Name           Age            City\"\n</code></pre>"},{"location":"resources/python-strings-cheatsheet/#string-methods-for-replacing-and-splitting","title":"String Methods for Replacing and Splitting","text":"<pre><code>text = \"Python is amazing and Python is easy to learn\"\n\n# Replacing substrings\nreplaced = text.replace(\"Python\", \"JavaScript\")  # \"JavaScript is amazing and JavaScript is easy to learn\"\nreplace_once = text.replace(\"Python\", \"JavaScript\", 1)  # \"JavaScript is amazing and Python is easy to learn\"\n\n# Splitting strings\nwords = text.split()            # ['Python', 'is', 'amazing', 'and', 'Python', 'is', 'easy', 'to', 'learn']\nphrases = text.split(\" and \")   # ['Python is amazing', 'Python is easy to learn']\n\n# Splitting with max splits\nlimited_split = text.split(\" \", 3)  # ['Python', 'is', 'amazing', 'and Python is easy to learn']\n\n# Splitting by lines\nmulti_line = \"Line 1\\nLine 2\\nLine 3\"\nlines = multi_line.splitlines()   # ['Line 1', 'Line 2', 'Line 3']\nlines_with_breaks = multi_line.splitlines(True)  # ['Line 1\\n', 'Line 2\\n', 'Line 3']\n\n# Splitting from the right\nrsplit_result = text.rsplit(\" \", 2)  # ['Python is amazing and Python is easy', 'to', 'learn']\n\n# Joining strings\nwords = [\"Python\", \"is\", \"amazing\"]\njoined = \" \".join(words)  # \"Python is amazing\"\ncsv_line = \",\".join(words)  # \"Python,is,amazing\"\n</code></pre>"},{"location":"resources/python-strings-cheatsheet/#string-methods-for-checking-content","title":"String Methods for Checking Content","text":"<pre><code># Checking character types\nnum_str = \"12345\"\nalpha_str = \"Hello\"\nalnum_str = \"Hello123\"\nspace_str = \"   \\t\\n\"\nwhitespace_with_text = \"  Hello  \"\n\n# Differences between digit/decimal/numeric methods:\n# isdecimal(): Only decimal digits (0-9 and equivalents)\n# isdigit(): Decimal digits plus other digit characters (like superscripts)\n# isnumeric(): Any character that represents a number (including fractions, Roman numerals)\n\n# Decimal digits (0-9)\n\"123\".isdecimal()  # True\n\"123\".isdigit()    # True\n\"123\".isnumeric()  # True\n\n# Superscript/subscript numbers\n\"\u00b2\u00b3\".isdecimal()   # False - not standard decimal digits\n\"\u00b2\u00b3\".isdigit()     # True - these are digit characters\n\"\u00b2\u00b3\".isnumeric()   # True - these represent numeric values\n\n# Vulgar fractions\n\"\u00bd\".isdecimal()    # False - not a standard decimal digit\n\"\u00bd\".isdigit()      # False - not a digit character\n\"\u00bd\".isnumeric()    # True - represents a numeric value\n\n# Roman numerals\n\"\u2166\".isdecimal()    # False\n\"\u2166\".isdigit()      # False\n\"\u2166\".isnumeric()    # True - represents a numeric value\n\n# Non-Western digits (e.g., Arabic-Indic digits)\n\"\u0663\u0664\u0665\".isdecimal()  # True - these are decimal digits, just from a different script\n\"\u0663\u0664\u0665\".isdigit()    # True\n\"\u0663\u0664\u0665\".isnumeric()  # True\n\n# Other character type checks\nalpha_str.isalpha()    # True (only alphabetic characters)\nalnum_str.isalnum()    # True (alphabetic or numeric)\nalpha_str.isascii()    # True (only ASCII characters)\n\nspace_str.isspace()    # True (only whitespace)\nwhitespace_with_text.isspace()  # False (contains non-whitespace)\n\n# Checking identifiers\nvariable_name = \"my_var_1\"\nvariable_name.isidentifier()  # True (valid Python identifier)\n\"1invalid\".isidentifier()     # False (starts with a digit)\n\n# Checking printable characters\n\"Hello\\n\".isprintable()  # False (contains non-printable newline)\n\"Hello!\".isprintable()   # True (all printable)\n\n# Complex example: validating user input\ndef validate_username(username):\n    \"\"\"Check if username meets requirements: alphanumeric, 3-20 chars.\"\"\"\n    if not (3 &lt;= len(username) &lt;= 20):\n        return False, \"Username must be 3-20 characters.\"\n    if not username.isalnum():\n        return False, \"Username must contain only letters and numbers.\"\n    if username[0].isdigit():\n        return False, \"Username must start with a letter.\"\n    return True, \"Username is valid.\"\n</code></pre>"},{"location":"resources/python-strings-cheatsheet/#string-formatting","title":"String Formatting","text":""},{"location":"resources/python-strings-cheatsheet/#-formatting-old-style","title":"%-formatting (old style)","text":"<pre><code>name = \"Alice\"\nage = 30\n\n# Basic formatting\nmessage = \"Hello, %s. You are %d years old.\" % (name, age)\n# \"Hello, Alice. You are 30 years old.\"\n\n# Format specifiers\nfloat_value = 3.14159\nformatted = \"Value: %.2f\" % float_value  # \"Value: 3.14\"\n\n# Width and alignment\nformatted = \"%-10s | %10s\" % (\"Left\", \"Right\")  # \"Left       |      Right\"\n\n# Named placeholders\nformatted = \"%(name)s is %(age)d years old.\" % {\"name\": \"Bob\", \"age\": 25}\n# \"Bob is 25 years old.\"\n</code></pre>"},{"location":"resources/python-strings-cheatsheet/#strformat-new-style","title":"str.format() (new style)","text":"<pre><code>name = \"Alice\"\nage = 30\n\n# Basic formatting\nmessage = \"Hello, {}. You are {} years old.\".format(name, age)\n# \"Hello, Alice. You are 30 years old.\"\n\n# Positional arguments\nmessage = \"Hello, {0}. In 5 years, you'll be {1} years old.\".format(name, age + 5)\n# \"Hello, Alice. In 5 years, you'll be 35 years old.\"\n\n# Named placeholders\nmessage = \"Hello, {name}. You are {age} years old.\".format(name=name, age=age)\n# \"Hello, Alice. You are 30 years old.\"\n\n# Format specifiers\npi = 3.14159\nformatted = \"Pi: {:.2f}\".format(pi)  # \"Pi: 3.14\"\n\n# Width, alignment, and fill characters\nleft = \"{:&lt;10}\".format(\"Left\")    # \"Left      \"\nright = \"{:&gt;10}\".format(\"Right\")  # \"     Right\"\ncenter = \"{:^10}\".format(\"Center\")  # \"  Center  \"\ncustom = \"{:*^10}\".format(\"Custom\")  # \"**Custom**\"\n\n# Accessing object attributes and dictionary items\nperson = {\"name\": \"Alice\", \"age\": 30}\nmessage = \"{p[name]} is {p[age]} years old.\".format(p=person)\n# \"Alice is 30 years old.\"\n\n# Formatting integers\ndecimal = \"{:d}\".format(42)       # \"42\"\nbinary = \"{:b}\".format(42)        # \"101010\"\noctal = \"{:o}\".format(42)         # \"52\"\nhexadecimal = \"{:x}\".format(42)   # \"2a\"\nhexadecimal_upper = \"{:X}\".format(42)  # \"2A\"\n\n# Formatting with commas for thousands\nlarge_num = \"{:,}\".format(1234567)  # \"1,234,567\"\n\n# Formatting dates\nimport datetime\nnow = datetime.datetime.now()\nformatted_date = \"{:%Y-%m-%d %H:%M:%S}\".format(now)\n# e.g., \"2023-06-27 15:30:45\"\n</code></pre>"},{"location":"resources/python-strings-cheatsheet/#f-strings-python-36","title":"f-strings (Python 3.6+)","text":"<pre><code>name = \"Alice\"\nage = 30\n\n# Basic formatting\nmessage = f\"Hello, {name}. You are {age} years old.\"\n# \"Hello, Alice. You are 30 years old.\"\n\n# Expressions in placeholders\nmessage = f\"In 5 years, {name} will be {age + 5} years old.\"\n# \"In 5 years, Alice will be 35 years old.\"\n\n# Format specifiers\npi = 3.14159\nformatted = f\"Pi: {pi:.2f}\"  # \"Pi: 3.14\"\n\n# Width, alignment, and fill characters\nleft = f\"{'Left':&lt;10}\"    # \"Left      \"\nright = f\"{'Right':&gt;10}\"  # \"     Right\"\ncenter = f\"{'Center':^10}\"  # \"  Center  \"\ncustom = f\"{'Custom':*^10}\"  # \"**Custom**\"\n\n# Dynamic width\nwidth = 10\ndynamic = f\"{'Dynamic':^{width}}\"  # \"  Dynamic  \"\n\n# Accessing object attributes and dictionary items\nperson = {\"name\": \"Alice\", \"age\": 30}\nmessage = f\"{person['name']} is {person['age']} years old.\"\n# \"Alice is 30 years old.\"\n\n# Using ! for conversion types\nmessage = f\"Representation: {name!r}\"  # \"Representation: 'Alice'\"\n\n# Using = to display expression and result (Python 3.8+)\ncalculation = f\"{age=}\"  # \"age=30\"\nexpression = f\"{age + 5=}\"  # \"age + 5=35\"\n\n# Multiline f-strings\nquery = f\"\"\"\nSELECT *\nFROM users\nWHERE name = '{name}'\n  AND age &gt; {age - 5}\n\"\"\"\n</code></pre>"},{"location":"resources/python-strings-cheatsheet/#string-formatting-using-template-strings","title":"String Formatting Using Template Strings","text":"<pre><code>from string import Template\n\n# Creating a template\nt = Template(\"$name is $age years old\")\n\n# Substituting values\nresult = t.substitute(name=\"Alice\", age=30)\n# \"Alice is 30 years old\"\n\n# Safe substitution (doesn't raise KeyError)\nresult = t.safe_substitute(name=\"Alice\")\n# \"Alice is $age years old\"\n\n# Using a dictionary\nvalues = {\"name\": \"Bob\", \"age\": 25}\nresult = t.substitute(values)\n# \"Bob is 25 years old\"\n\n# Using $ literally\nt = Template(\"Amount: $$${amount}\")\nresult = t.substitute(amount=100)\n# \"Amount: $100\"\n</code></pre>"},{"location":"resources/python-strings-cheatsheet/#advanced-string-operations","title":"Advanced String Operations","text":""},{"location":"resources/python-strings-cheatsheet/#string-comparison","title":"String Comparison","text":"<pre><code># Case-sensitive comparison\n\"apple\" == \"apple\"   # True\n\"apple\" == \"Apple\"   # False\n\n# Case-insensitive comparison\n\"apple\".lower() == \"Apple\".lower()  # True\n\"apple\".casefold() == \"Apple\".casefold()  # True (safer for international text)\n\n# Comparing strings lexicographically\n\"apple\" &lt; \"banana\"  # True (a comes before b)\n\"apple\" &lt; \"BANANA\"  # False (uppercase comes before lowercase in ASCII)\n\n# Sorting strings with a custom key\n# More Pythonic approach using a tuple key with lambda\nstrings = [\"Apple\", \"apple\", \"Banana\", \"banana\"]\n\n# Sort case-insensitive first, then case-sensitive for ties\nsorted_strings = sorted(strings, key=lambda s: (s.lower(), s))\n# ['apple', 'Apple', 'banana', 'Banana']\n\n# Sort by length, then alphabetically\nwords = [\"cat\", \"elephant\", \"dog\", \"mouse\"]\nby_length = sorted(words, key=lambda s: (len(s), s))\n# ['cat', 'dog', 'mouse', 'elephant']\n\n# Sort with custom ordering (putting specific strings first)\npriority = {\"high\": 0, \"medium\": 1, \"low\": 2}\ntasks = [\"high task\", \"low task\", \"medium task\"]\nby_priority = sorted(tasks, key=lambda s: priority.get(s.split()[0], 999))\n# ['high task', 'medium task', 'low task']\n</code></pre>"},{"location":"resources/python-strings-cheatsheet/#regular-expressions-with-strings","title":"Regular Expressions with Strings","text":"<pre><code>import re\n\ntext = \"Contact us at info@example.com or support@example.org\"\n\n# Finding all matches\nemails = re.findall(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b', text)\n# ['info@example.com', 'support@example.org']\n\n# Search and replace\ncensored = re.sub(r'[A-Za-z0-9._%+-]+@', \"EMAIL@\", text)\n# \"Contact us at EMAIL@example.com or EMAIL@example.org\"\n\n# Splitting with regex\nwords = re.split(r'\\W+', \"Hello, world! How are you?\")\n# ['Hello', 'world', 'How', 'are', 'you', '']\n\n# Match objects for more information\npattern = re.compile(r'(\\w+)@(\\w+)\\.(\\w+)')\nfor match in pattern.finditer(text):\n    print(f\"Full match: {match.group(0)}\")\n    print(f\"Username: {match.group(1)}\")\n    print(f\"Domain: {match.group(2)}\")\n    print(f\"TLD: {match.group(3)}\")\n\n# Search with regex\nmatch = re.search(r'\\d+', \"Order #12345 received\")\nif match:\n    print(f\"Found number: {match.group()}\")  # \"Found number: 12345\"\n</code></pre>"},{"location":"resources/python-strings-cheatsheet/#unicode-and-encoding","title":"Unicode and Encoding","text":"<pre><code># Understanding Unicode vs. Encodings (like UTF-8)\n\"\"\"\nThe difference between Unicode and UTF-8:\n\n1. Unicode is a CHARACTER SET - a standard that maps characters to unique code points\n   - Unicode assigns each character a unique numerical value (code point)\n   - For example, 'A' has code point U+0041, '\u4e16' has code point U+4E16\n   - Unicode doesn't specify how these code points should be stored in memory or files\n   - It's just a conceptual mapping: Character \u2194 Code Point\n\n2. UTF-8 is an ENCODING - a method for storing Unicode code points as bytes\n   - UTF-8 defines the actual bit/byte sequences used to represent Unicode code points\n   - It's a variable-width encoding: common characters use fewer bytes\n   - ASCII characters (U+0000 to U+007F) use 1 byte in UTF-8\n   - Other characters use 2-4 bytes depending on their code point value\n   - It's just one of several ways to encode Unicode (others include UTF-16, UTF-32)\n\nIn Python 3:\n- Strings store Unicode code points internally (not bytes)\n- When you need to save or transmit text, you encode these code points to bytes\n- UTF-8 is the most common encoding, but others can be used depending on requirements\n\"\"\"\n\n# Unicode characters and code points\nchar = \"\u00e9\"\ncode_point = ord(char)  # 233 (the Unicode code point for \u00e9)\nback_to_char = chr(code_point)  # \"\u00e9\" (convert code point back to character)\n\n# The Unicode standard contains over 140,000 characters\nprint(ord(\"A\"))        # 65 (Basic Latin)\nprint(ord(\"\u03a9\"))        # 937 (Greek)\nprint(ord(\"\u4e16\"))        # 19990 (CJK Unified Ideographs)\nprint(ord(\"\ud83d\ude00\"))        # 128512 (Emoji)\n\n# UTF-8 encoding of these characters (variable length)\nprint(len(\"A\".encode(\"utf-8\")))   # 1 byte\nprint(len(\"\u03a9\".encode(\"utf-8\")))   # 2 bytes\nprint(len(\"\u4e16\".encode(\"utf-8\")))   # 3 bytes\nprint(len(\"\ud83d\ude00\".encode(\"utf-8\")))   # 4 bytes\n\n# UTF-8 encoding preserves ASCII compatibility\nascii_str = \"ABC123\"\nutf8_bytes = ascii_str.encode(\"utf-8\")\nprint(utf8_bytes)  # b'ABC123' - each ASCII character is 1 byte in UTF-8\n\n# For non-ASCII characters, UTF-8 uses multi-byte sequences\nmixed_str = \"Hello, \u4e16\u754c! \ud83d\udc4b\"\nutf8_bytes = mixed_str.encode(\"utf-8\")\nprint(utf8_bytes)  # b'Hello, \\xe4\\xb8\\x96\\xe7\\x95\\x8c! \\xf0\\x9f\\x91\\x8b'\nprint([len(c.encode(\"utf-8\")) for c in mixed_str])  # [1, 1, 1, 1, 1, 1, 3, 3, 1, 4]\n\n# Other Unicode encodings handle bytes differently\nprint(len(\"\ud83d\ude00\".encode(\"utf-8\")))   # 4 bytes in UTF-8\nprint(len(\"\ud83d\ude00\".encode(\"utf-16\")))  # 4 bytes in UTF-16 (2 surrogate pairs)\nprint(len(\"\ud83d\ude00\".encode(\"utf-32\")))  # 4 bytes in UTF-32 (fixed width)\n\n# String encoding: Converting Unicode strings to bytes\ntext = \"\u3053\u3093\u306b\u3061\u306f\"  # \"Hello\" in Japanese\n\n# .encode(encoding, errors) converts a string to a bytes object\n# Important: encode() ALWAYS returns a bytes object, not a string\nencoded = text.encode(\"utf-8\")  \nprint(encoded)  # b'\\xe3\\x81\\x93\\xe3\\x82\\x93\\xe3\\x81\\xab\\xe3\\x81\\xa1\\xe3\\x81\\xaf'\nprint(type(encoded))  # &lt;class 'bytes'&gt;\n\n# Why ASCII characters don't show as hex in bytes representation:\nascii_bytes = \"hello\".encode(\"utf-8\")\nprint(ascii_bytes)  # b'hello'\n\n# This is because Python displays bytes objects in a dual format:\n# 1. ASCII-printable bytes (32-126) are shown as their character representation\n# 2. Non-ASCII-printable bytes are shown as hex escape sequences (\\xNN)\n\n# Let's see this with a mixed string:\nmixed = \"hello\u4e16\u754c\"\nmixed_bytes = mixed.encode(\"utf-8\")\nprint(mixed_bytes)  # b'hello\\xe4\\xb8\\x96\\xe7\\x95\\x8c'\n# 'hello' shows as ASCII characters, but the Chinese characters show as hex\n\n# To see the actual integer values of each byte:\nprint([b for b in ascii_bytes])  # [104, 101, 108, 108, 111]\nprint([b for b in encoded])      # [227, 129, 147, 227, 130, 147, ...] \n\n# Both are just sequences of bytes (integers 0-255), they just display differently\n# You can confirm they're the same type:\nprint(type(ascii_bytes) == type(encoded))  # True\n\n# Key differences between str and bytes:\n# - str: sequence of Unicode characters (can contain any character)\n# - bytes: sequence of integers between 0-255 (represents raw data)\n\n# Common encodings\nutf8_bytes = text.encode(\"utf-8\")    # Variable-width encoding (1-4 bytes per character)\nutf16_bytes = text.encode(\"utf-16\")  # Usually 2 or 4 bytes per character, with BOM\n\n# ASCII vs. Unicode encodings:\n# ASCII is a 7-bit encoding that can only represent characters with code points 0-127\n# It's actually a subset of UTF-8, meaning any valid ASCII text is also valid UTF-8\nprint(\"ASCII encoding vs. other Unicode encodings:\")\n\n# For ASCII-only text, UTF-8 and ASCII encoding produce identical results\nascii_text = \"Hello, world!\"\nprint(ascii_text.encode(\"ascii\"))  # b'Hello, world!'\nprint(ascii_text.encode(\"utf-8\"))  # b'Hello, world!'\n\n# The difference appears with non-ASCII characters\nmixed_text = \"Hello, caf\u00e9!\"  # Contains the non-ASCII '\u00e9' character\n\n# UTF-8 can handle any Unicode character\nutf8_result = mixed_text.encode(\"utf-8\")  \nprint(utf8_result)  # b'Hello, caf\\xc3\\xa9!' ('\u00e9' takes 2 bytes in UTF-8)\n\n# ASCII can only handle ASCII characters (code points 0-127)\ntry:\n    ascii_result = mixed_text.encode(\"ascii\")  # Raises UnicodeEncodeError\nexcept UnicodeEncodeError as e:\n    print(f\"ASCII encoding error: {e}\")\n    # Handle encoding errors with the errors parameter\n    ascii_replace = mixed_text.encode(\"ascii\", errors=\"replace\")  # b'Hello, caf?!'\n    ascii_ignore = mixed_text.encode(\"ascii\", errors=\"ignore\")    # b'Hello, caf!'\n    ascii_xmlcharrefreplace = mixed_text.encode(\"ascii\", errors=\"xmlcharrefreplace\")  # b'Hello, caf&amp;#233;!'\n\n    print(f\"With replace: {ascii_replace}\")\n    print(f\"With ignore: {ascii_ignore}\")\n    print(f\"With xmlcharrefreplace: {ascii_xmlcharrefreplace}\")\n\n# Comparing sizes of different encodings\nsample = \"Hello, \u4e16\u754c\"  # \"Hello, World\" with Chinese characters\nprint(f\"Text length: {len(sample)} characters\")\nprint(f\"ASCII (with replace): {len(sample.encode('ascii', errors='replace'))} bytes\")\nprint(f\"UTF-8: {len(sample.encode('utf-8'))} bytes\")\nprint(f\"UTF-16: {len(sample.encode('utf-16'))} bytes\")\nprint(f\"UTF-32: {len(sample.encode('utf-32'))} bytes\")\n\n# UTF-8 advantages:\n# - Backward compatible with ASCII (ASCII chars use 1 byte)\n# - Variable width (saves space for common Latin characters)\n# - Self-synchronizing (can detect start of next character)\n# - No endianness issues (unlike UTF-16/UTF-32)\n\n# Decoding: Converting bytes back to strings\ndecoded = encoded.decode(\"utf-8\")  # \"\u3053\u3093\u306b\u3061\u306f\"\n\n# The decode method has the same error handling options\ntry:\n    corrupt_decoded = b'\\xe3\\x81\\x93\\xe3'.decode(\"utf-8\")  # Raises UnicodeDecodeError (incomplete byte sequence)\nexcept UnicodeDecodeError:\n    # Handle decoding errors with the errors parameter\n    replace_decoded = b'\\xe3\\x81\\x93\\xe3'.decode(\"utf-8\", errors=\"replace\")  # \"\u3053\ufffd\"\n    ignore_decoded = b'\\xe3\\x81\\x93\\xe3'.decode(\"utf-8\", errors=\"ignore\")    # \"\u3053\"\n\n# Practical examples\n\n# 1. File I/O with encoding\nwith open(\"file.txt\", \"w\", encoding=\"utf-8\") as f:\n    f.write(\"Hello, caf\u00e9!\")  # Saved as UTF-8 bytes\n\nwith open(\"file.txt\", \"r\", encoding=\"utf-8\") as f:\n    content = f.read()  # Decoded from UTF-8 bytes to string\n\n# 2. Web requests\nimport urllib.request\nresponse = urllib.request.urlopen(\"https://www.example.com\")\nhtml_bytes = response.read()  # Bytes\nencoding = response.headers.get_content_charset() or \"utf-8\"  # Get encoding from headers or default\nhtml_string = html_bytes.decode(encoding)  # Convert to string\n\n# 3. Database storage\n# When storing strings in databases, you need to ensure the database\n# uses the right encoding (usually UTF-8) and that your connection\n# is configured to use that encoding\n\n# Detecting encoding\n# Python doesn't have a built-in way to detect encoding reliably,\n# but the chardet library can make educated guesses:\ntry:\n    import chardet\n    unknown_bytes = b'\\xe3\\x81\\x93\\xe3\\x82\\x93\\xe3\\x81\\xab\\xe3\\x81\\xa1\\xe3\\x81\\xaf'\n    detected = chardet.detect(unknown_bytes)\n    print(f\"Detected encoding: {detected['encoding']}, confidence: {detected['confidence']}\")\n    # Might output: \"Detected encoding: utf-8, confidence: 0.87\"\nexcept ImportError:\n    pass  # chardet not installed\n\n# Normalizing Unicode (handling composed vs decomposed forms)\nfrom unicodedata import normalize\ncomposed = \"\u00e9\"  # Single character\ndecomposed = \"e\\u0301\"  # 'e' followed by combining acute accent\nnormalize(\"NFC\", decomposed) == composed  # True (normalizes to composed form)\nnormalize(\"NFD\", composed) == decomposed  # True (normalizes to decomposed form)\n</code></pre>"},{"location":"resources/python-strings-cheatsheet/#string-interpolation-in-different-contexts","title":"String Interpolation in Different Contexts","text":"<pre><code># In SQL queries (vulnerable to SQL injection - don't use in production!)\nusername = \"O'Reilly\"  # Note the apostrophe\n# Wrong: f\"SELECT * FROM users WHERE username = '{username}'\"  # SQL injection risk!\n\n# Safe parameterized query\nquery = \"SELECT * FROM users WHERE username = %s\"\n# Then pass username as a parameter to your database driver\n\n# In HTML (vulnerable to XSS - don't use in production!)\nuser_input = \"&lt;script&gt;alert('XSS')&lt;/script&gt;\"\n# Wrong: f\"&lt;div&gt;{user_input}&lt;/div&gt;\"  # XSS risk!\n\n# Safe HTML escaping\nimport html\nsafe_html = f\"&lt;div&gt;{html.escape(user_input)}&lt;/div&gt;\"\n# \"&lt;div&gt;&amp;lt;script&amp;gt;alert('XSS')&amp;lt;/script&amp;gt;&lt;/div&gt;\"\n\n# In shell commands (vulnerable to command injection - don't use in production!)\nfilename = \"file; rm -rf /\"\n# Wrong: os.system(f\"cat {filename}\")  # Command injection risk!\n\n# Safe command execution\nimport subprocess\nsubprocess.run([\"cat\", filename], check=True)  # Safe, no shell interpretation\n</code></pre>"},{"location":"resources/python-strings-cheatsheet/#common-string-patterns","title":"Common String Patterns","text":""},{"location":"resources/python-strings-cheatsheet/#string-parsing-and-extraction","title":"String Parsing and Extraction","text":"<pre><code># Extracting parts of a string using slicing\nemail = \"user@example.com\"\nusername = email[:email.find('@')]  # \"user\"\ndomain = email[email.find('@')+1:]  # \"example.com\"\n\n# Extracting with regular expressions\nimport re\nurl = \"https://www.example.com/path?param=value\"\npattern = r\"https?://(?:www\\.)?([^/]+)(.*)\"\nmatch = re.match(pattern, url)\nif match:\n    domain = match.group(1)  # \"example.com\"\n    path = match.group(2)    # \"/path?param=value\"\n\n# Parsing key-value pairs\nquery_string = \"name=John&amp;age=30&amp;city=New%20York\"\nparams = {}\nfor pair in query_string.split('&amp;'):\n    if '=' in pair:\n        key, value = pair.split('=', 1)\n        params[key] = value.replace('%20', ' ')\n# params = {'name': 'John', 'age': '30', 'city': 'New York'}\n</code></pre>"},{"location":"resources/python-strings-cheatsheet/#text-processing","title":"Text Processing","text":"<pre><code># Word count\ntext = \"This is a sample text. This text has some duplicate words.\"\nwords = text.lower().split()\nword_count = {}\nfor word in words:\n    # Remove punctuation from word\n    word = word.strip('.,!?:;()\"\\'')\n    if word:\n        word_count[word] = word_count.get(word, 0) + 1\n# {'this': 2, 'is': 1, 'a': 1, 'sample': 1, 'text': 2, 'has': 1, 'some': 1, 'duplicate': 1, 'words': 1}\n\n# Finding most common words\nsorted_words = sorted(word_count.items(), key=lambda x: x[1], reverse=True)\nmost_common = sorted_words[:3]  # [('this', 2), ('text', 2), ('is', 1)]\n\n# Sentence tokenization (basic)\ntext = \"Hello. How are you? I'm fine. Thanks.\"\nsentences = [s.strip() for s in re.split(r'[.!?]+', text) if s.strip()]\n# ['Hello', 'How are you', \"I'm fine\", 'Thanks']\n\n# More accurate sentence tokenization\nimport re\ndef split_into_sentences(text):\n    # Handle common abbreviations to avoid false splits\n    text = re.sub(r'([A-Z][a-z]{1,2})\\. ', r'\\1\u00a7 ', text)  # Mr. Dr. etc.\n    # Split on sentence ending punctuation\n    sentences = re.split(r'(?&lt;=[.!?])\\s+', text)\n    # Restore periods in abbreviations\n    return [s.replace('\u00a7', '.') for s in sentences]\n\n# Text wrapping to a specific width\nimport textwrap\nlong_text = \"This is a very long text that needs to be wrapped to fit within a specific width.\"\nwrapped = textwrap.fill(long_text, width=30)\n# This is a very long text that\n# needs to be wrapped to fit\n# within a specific width.\n</code></pre>"},{"location":"resources/python-strings-cheatsheet/#string-validation","title":"String Validation","text":"<pre><code># Email validation (simple)\ndef is_valid_email(email):\n    pattern = r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$'\n    return bool(re.match(pattern, email))\n\nis_valid_email(\"user@example.com\")  # True\nis_valid_email(\"invalid-email\")     # False\n\n# Phone number validation (US format)\ndef is_valid_us_phone(phone):\n    # Remove any non-digit characters\n    digits_only = re.sub(r'\\D', '', phone)\n    # Check if we have 10 digits, or 11 digits starting with 1\n    if len(digits_only) == 10:\n        return True\n    if len(digits_only) == 11 and digits_only[0] == '1':\n        return True\n    return False\n\nis_valid_us_phone(\"(123) 456-7890\")  # True\nis_valid_us_phone(\"123-456-7890\")    # True\nis_valid_us_phone(\"1-123-456-7890\")  # True\nis_valid_us_phone(\"12345\")          # False\n\n# Password strength checking\ndef check_password_strength(password):\n    \"\"\"Check password strength and return feedback.\"\"\"\n    issues = []\n    if len(password) &lt; 8:\n        issues.append(\"Password should be at least 8 characters\")\n    if not re.search(r'[A-Z]', password):\n        issues.append(\"Password should contain uppercase letters\")\n    if not re.search(r'[a-z]', password):\n        issues.append(\"Password should contain lowercase letters\")\n    if not re.search(r'\\d', password):\n        issues.append(\"Password should contain digits\")\n    if not re.search(r'[!@#$%^&amp;*(),.?\":{}|&lt;&gt;]', password):\n        issues.append(\"Password should contain special characters\")\n\n    if not issues:\n        return \"Strong password\", []\n    return \"Weak password\", issues\n</code></pre>"},{"location":"resources/python-strings-cheatsheet/#json-string-handling","title":"JSON String Handling","text":"<pre><code>import json\n\n# Converting Python objects to JSON strings\ndata = {\n    \"name\": \"Alice\",\n    \"age\": 30,\n    \"is_student\": False,\n    \"courses\": [\"Python\", \"Data Science\"],\n    \"grades\": {\"Python\": 95, \"Data Science\": 88}\n}\n\n# Basic conversion\njson_str = json.dumps(data)\n# '{\"name\": \"Alice\", \"age\": 30, \"is_student\": false, \"courses\": [\"Python\", \"Data Science\"], \"grades\": {\"Python\": 95, \"Data Science\": 88}}'\n\n# Pretty printing\npretty_json = json.dumps(data, indent=4, sort_keys=True)\n# {\n#     \"age\": 30,\n#     \"courses\": [\n#         \"Python\",\n#         \"Data Science\"\n#     ],\n#     \"grades\": {\n#         \"Data Science\": 88,\n#         \"Python\": 95\n#     },\n#     \"is_student\": false,\n#     \"name\": \"Alice\"\n# }\n\n# Parsing JSON strings\nparsed_data = json.loads(json_str)\nname = parsed_data[\"name\"]  # \"Alice\"\n\n# Handling JSON encoding for custom objects\nclass Person:\n    def __init__(self, name, age):\n        self.name = name\n        self.age = age\n\ndef person_to_dict(obj):\n    if isinstance(obj, Person):\n        return {\"name\": obj.name, \"age\": obj.age}\n    raise TypeError(f\"Object of type {type(obj)} is not JSON serializable\")\n\nperson = Person(\"Bob\", 25)\nperson_json = json.dumps(person, default=person_to_dict)\n# '{\"name\": \"Bob\", \"age\": 25}'\n</code></pre>"},{"location":"resources/python-strings-cheatsheet/#string-performance-considerations","title":"String Performance Considerations","text":"<pre><code># String concatenation in loops\n# Inefficient (creates many intermediate strings):\nresult = \"\"\nfor i in range(1000):\n    result += str(i)\n\n# Efficient (collects strings and joins once):\nparts = []\nfor i in range(1000):\n    parts.append(str(i))\nresult = \"\".join(parts)\n\n# String building with join is much faster than += for many concatenations\n# Benchmark example:\nimport timeit\n\ndef concat_with_plus():\n    result = \"\"\n    for i in range(1000):\n        result += str(i)\n    return result\n\ndef concat_with_join():\n    return \"\".join(str(i) for i in range(1000))\n\nplus_time = timeit.timeit(concat_with_plus, number=100)\njoin_time = timeit.timeit(concat_with_join, number=100)\nprint(f\"Plus: {plus_time:.6f}s, Join: {join_time:.6f}s, Ratio: {plus_time/join_time:.2f}x\")\n# Example output: \"Plus: 0.068213s, Join: 0.005749s, Ratio: 11.87x\"\n\n# String interning\n# Python automatically interns some strings to save memory\na = \"hello\"\nb = \"hello\"\na is b  # True (same object due to interning)\n\nc = \"hello world\"\nd = \"hello world\"\nc is d  # May be False, as longer strings might not be interned\n\n# String copying (strings are immutable, so 'copying' just creates a reference)\noriginal = \"hello\"\ncopy = original  # Just a reference, not a separate copy\ncopy is original  # True\n\n# String memory usage\nimport sys\ntext = \"hello\"\nmemory = sys.getsizeof(text)  # Size in bytes (implementation dependent)\n</code></pre>"},{"location":"resources/python-strings-cheatsheet/#appendix-common-string-operations-and-their-time-complexity","title":"Appendix: Common String Operations and Their Time Complexity","text":"Operation Example Time Complexity Notes Creation <code>s = \"hello\"</code> O(n) n is the length of the string Concatenation <code>s1 + s2</code> O(n + m) n and m are the lengths of the strings Repetition <code>s * n</code> O(n * m) n is the repetition count, m is the length of s Slicing <code>s[1:5]</code> O(k) k is the length of the slice Length <code>len(s)</code> O(1) Constant time operation Membership <code>c in s</code> O(n) n is the length of the string Indexing <code>s[i]</code> O(1) Constant time operation Iteration <code>for c in s</code> O(n) n is the length of the string String comparison <code>s1 == s2</code> O(n) n is the length of the shorter string <code>find()</code>, <code>index()</code> <code>s.find(\"x\")</code> O(n) n is the length of the string <code>count()</code> <code>s.count(\"x\")</code> O(n) n is the length of the string <code>split()</code> <code>s.split()</code> O(n) n is the length of the string <code>join()</code> <code>\",\".join(list)</code> O(n) n is the total length of all strings <code>strip()</code>, <code>lstrip()</code>, <code>rstrip()</code> <code>s.strip()</code> O(n) n is the length of the string Case conversion <code>s.upper()</code> O(n) n is the length of the string <code>replace()</code> <code>s.replace(\"a\", \"b\")</code> O(n) n is the length of the string"},{"location":"resources/python-strings-cheatsheet/#appendix-understanding-unicode-and-encodings","title":"Appendix: Understanding Unicode and Encodings","text":""},{"location":"resources/python-strings-cheatsheet/#the-difference-between-unicode-and-utf-8","title":"The Difference Between Unicode and UTF-8","text":"<p>Unicode and UTF-8 represent two different aspects of text handling in computing:</p>"},{"location":"resources/python-strings-cheatsheet/#unicode-a-character-set","title":"Unicode: A Character Set","text":"<p>Unicode is a standard that maps characters to unique code points:</p> <ul> <li>Unicode assigns each character a unique numerical value called a code point</li> <li>Code points are typically written as U+XXXX (e.g., U+0041 for 'A', U+4E16 for '\u4e16')</li> <li>Unicode currently defines over 140,000 characters covering virtually all modern writing systems</li> <li>Unicode doesn't specify how these code points should be stored in memory or files</li> <li>It's purely a conceptual mapping: Character \u2194 Code Point</li> </ul> <p>Examples of Unicode code points: - 'A' \u2192 U+0041 (65 in decimal) - '\u00f1' \u2192 U+00F1 (241 in decimal) - '\u20ac' \u2192 U+20AC (8364 in decimal) - '\u4e16' \u2192 U+4E16 (19990 in decimal) - '\ud83d\ude00' \u2192 U+1F600 (128512 in decimal)</p>"},{"location":"resources/python-strings-cheatsheet/#utf-8-an-encoding","title":"UTF-8: An Encoding","text":"<p>UTF-8 is an encoding - a method for storing Unicode code points as bytes:</p> <ul> <li>UTF-8 defines the actual bit/byte sequences used to represent Unicode code points</li> <li>It's a variable-width encoding: common characters use fewer bytes</li> <li>ASCII characters (U+0000 to U+007F) use 1 byte in UTF-8</li> <li>Other characters use 2-4 bytes depending on their code point value</li> <li>UTF-8 is just one of several ways to encode Unicode (others include UTF-16, UTF-32)</li> </ul> <p>Characteristics of UTF-8: - ASCII-compatible (ASCII characters are encoded the same way) - Variable-width (1-4 bytes per character) - Self-synchronizing (can detect character boundaries) - Compact for Western languages - Most common encoding on the web</p>"},{"location":"resources/python-strings-cheatsheet/#how-utf-8-encoding-works","title":"How UTF-8 Encoding Works","text":"<p>UTF-8 uses a variable number of bytes for different characters:</p> Unicode Range UTF-8 Byte Format Bytes Used U+0000 to U+007F 0xxxxxxx 1 byte U+0080 to U+07FF 110xxxxx 10xxxxxx 2 bytes U+0800 to U+FFFF 1110xxxx 10xxxxxx 10xxxxxx 3 bytes U+10000 to U+10FFFF 11110xxx 10xxxxxx 10xxxxxx 10xxxxxx 4 bytes <p>Examples of UTF-8 encoding: - 'A' (U+0041) \u2192 01000001 (1 byte) - '\u00f1' (U+00F1) \u2192 11000011 10110001 (2 bytes) - '\u20ac' (U+20AC) \u2192 11100010 10000010 10101100 (3 bytes) - '\ud83d\ude00' (U+1F600) \u2192 11110000 10011111 10011000 10000000 (4 bytes)</p>"},{"location":"resources/python-strings-cheatsheet/#comparison-with-other-unicode-encodings","title":"Comparison with Other Unicode Encodings","text":"Encoding Description Advantages Disadvantages UTF-8 Variable-width (1-4 bytes) ASCII-compatible, compact for Latin text, no endianness issues Less efficient for Asian scripts UTF-16 Variable-width (2 or 4 bytes) Compact for Asian scripts Not ASCII-compatible, endianness issues, surrogate pairs UTF-32 Fixed-width (4 bytes) Simple indexing, no surrogate pairs Memory inefficient, not ASCII-compatible, endianness issues"},{"location":"resources/python-strings-cheatsheet/#pythons-string-handling","title":"Python's String Handling","text":"<p>In Python 3: - Strings are sequences of Unicode code points - String literals in code are Unicode by default - When you need to save or transmit text, you encode these code points to bytes - When reading external data, you decode bytes back to Unicode strings</p> <pre><code># Unicode code points\nchar = \"\u4e16\"\ncode_point = ord(char)  # 19990\nback_to_char = chr(code_point)  # '\u4e16'\n\n# Encoding to bytes\nutf8_bytes = char.encode(\"utf-8\")   # b'\\xe4\\xb8\\x96' (3 bytes)\nutf16_bytes = char.encode(\"utf-16\") # b'\\xff\\xfe\\x16N' (4 bytes with BOM)\nutf32_bytes = char.encode(\"utf-32\") # b'\\xff\\xfe\\x00\\x00\\x16N\\x00\\x00' (8 bytes with BOM)\n\n# Decoding from bytes\nchar_again = utf8_bytes.decode(\"utf-8\")  # '\u4e16'\n</code></pre>"},{"location":"resources/python-strings-cheatsheet/#common-encoding-related-issues","title":"Common Encoding-Related Issues","text":"<ol> <li> <p>UnicodeEncodeError: Raised when a string contains characters that cannot be encoded in the target encoding    </p><pre><code>\"caf\u00e9\".encode(\"ascii\")  # UnicodeEncodeError: 'ascii' codec can't encode character '\\xe9'\n</code></pre><p></p> </li> <li> <p>UnicodeDecodeError: Raised when bytes cannot be decoded using the specified encoding    </p><pre><code>b'\\xff\\xfe'.decode(\"utf-8\")  # UnicodeDecodeError: invalid continuation byte\n</code></pre><p></p> </li> <li> <p>Mojibake: Garbled text resulting from decoding bytes with the wrong encoding    </p><pre><code>utf8_bytes = \"caf\u00e9\".encode(\"utf-8\")\nwrong_decode = utf8_bytes.decode(\"latin-1\")  # 'caf\u00c3\u00a9' (mojibake)\n</code></pre><p></p> </li> <li> <p>BOM (Byte Order Mark): Special marker at the start of a file indicating endianness    </p><pre><code># UTF-16 encodes a BOM by default\n\"abc\".encode(\"utf-16\")  # b'\\xff\\xfe' (BOM) + b'a\\x00b\\x00c\\x00'\n</code></pre><p></p> </li> </ol>"},{"location":"resources/python-strings-cheatsheet/#best-practices-for-handling-text-in-python","title":"Best Practices for Handling Text in Python","text":"<ol> <li>Use Unicode strings (default in Python 3) for text processing</li> <li>Be explicit about encodings when reading/writing files</li> <li>Use UTF-8 as your default encoding unless you have a specific reason not to</li> <li>Handle encoding errors with appropriate error strategies</li> <li>Normalize Unicode when comparing strings from different sources</li> </ol>"},{"location":"resources/python_cheatsheet/","title":"Python Cheatsheet","text":""},{"location":"resources/python_cheatsheet/#python-cheatsheet","title":"Python Cheatsheet","text":"<p>WIP</p> <p>Python utility functions to keep in your L1 cache for coding interviews.</p>"},{"location":"resources/python_cheatsheet/#strings","title":"Strings","text":""},{"location":"resources/python_cheatsheet/#stralnum","title":"str.alnum","text":"<p>\"mystring\".isalnum()  </p> <p>str.isalnum() returns True if all characters in the string are alphanumeric and the string is not empty. If the string contains spaces, special characters, or is empty, it will return False.</p> Valid Palindrome <pre><code> class Solution:\n     def isPalindrome(self, s: str) -&gt; bool:\n\n         start,end = 0, len(s)-1\n\n         while start &lt; end :\n             if not s[start].isalnum() : \n                 start+=1\n                 continue\n             if not s[end].isalnum() :\n                 end-=1\n                 continue\n\n             if not s[start].lower() == s[end].lower() :\n                 return False\n\n             start +=1\n             end-=1\n\n         return True\n</code></pre>"},{"location":"resources/python_cheatsheet/#ord","title":"ord","text":"<p>ord('c')</p> <p>The ord() function in Python is used to get the Unicode code point (an integer) of a given character. It is the inverse of the chr() function, which converts a Unicode code point back to a character.</p> Score of String <pre><code>class Solution:\n    def scoreOfString(self, s: str) -&gt; int:\n        # Initializing this to cancel out the effect of the first element.\n        prev=ord(s[0]) \n        res = 0 \n        for c in s :\n            curr = ord(c)\n            res+= abs(curr - prev)\n            prev = curr \n        return res \n</code></pre>"},{"location":"resources/python_cheatsheet/#bisect-left-with-custom-key","title":"Bisect left with custom key","text":"<p>https://docs.python.org/3/library/bisect.html</p> <pre><code># from solution for https://leetcode.com/problems/insert-interval/\nfrom bisect import bisect_left\nintervals = [[0,1], [4,5]]\nnew_interval = [3,4]\nintervals.insert(\n     bisect_left(intervals,new_interval[0],key=lambda x : x[0]),\n     new_interval\n)\n</code></pre> <p>In Python, the insert() method is used to add an element to a list at a specific, chosen index. This method modifies the list in-place, meaning it directly changes the original list rather than creating a new one.</p> <p><code>list.insert(index,element)</code></p>"},{"location":"resources/python_cheatsheet/#sort-by-ascending-with-descending-tie-breaker","title":"Sort by ascending with descending tie breaker","text":"<pre><code>\"\"\"\nhttps://leetcode.com/problems/remove-covered-intervals/description/\n\"\"\"\nclass Solution:\n    def removeCoveredIntervals(self, intervals: List[List[int]]) -&gt; int:\n        # Sort by start point.\n        # If two intervals share the same start point\n        # put the longer one to be the first.\n        intervals.sort(key = lambda x: (x[0], -x[1]))\n        count = 0\n\n        prev_end = 0\n        for _, end in intervals:\n            # if current interval is not covered\n            # by the previous one\n            if end &gt; prev_end:\n                count += 1    \n                prev_end = end\n\n        return count\n</code></pre>"},{"location":"resources/python_cheatsheet/#isinstance","title":"isinstance","text":"<p>Checks if the object is of a specified type.</p> <pre><code>#ref : https://www.w3schools.com/python/ref_func_isinstance.asp\n\ndef flatten_list(nested_list):\n    \"\"\"\n    Flatten a nested list of any depth into a 1D list.\n\n    Args:\n        nested_list: A list that may contain nested lists as elements\n\n    Returns:\n        A flattened 1D list containing all elements in the nested list\n    \"\"\"\n    res = []\n    for item in nested_list :\n        if isinstance(item, list) :\n            res.extend(flatten_list(item))\n        else :\n            res.append(item)\n    return res\n\ntest_case_1 = [[1, 2, [3, 4], 5], [6], [7, [8, [9, 10]]]]\ntest_case_2 = [[1, \"a\", [3.1415, [True, 0]], \"b\"], [None, [2, [3, \"text\"]]]]\ntest_case_3 = [[[[]]], [], [[], [[], [[]]]]]\n\nprint(flatten_list(test_case_1))  # [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\nprint(flatten_list(test_case_2))  # [1, 'a', 3.1415, True, 0, 'b', None, 2, 3, 'text']\nprint(flatten_list(test_case_3))  # [] \n</code></pre>"},{"location":"resources/python_cheatsheet/#removing-punctutation","title":"removing punctutation","text":"<pre><code>&gt;&gt;&gt; import string\n&gt;&gt;&gt; string.punctuation\n'!\"#$%&amp;\\'()*+,-./:;&lt;=&gt;?@[\\\\]^_`{|}~'\n</code></pre> <p>string also has : </p><pre><code>&gt;&gt;&gt; dir(string)\n['Formatter', 'Template', '_ChainMap', '__all__', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_re', '_sentinel_dict', '_string', 'ascii_letters', 'ascii_lowercase', 'ascii_uppercase', 'capwords', 'digits', 'hexdigits', 'octdigits', 'printable', 'punctuation', 'whitespace']\n#example\n&gt;&gt;&gt; string.ascii_letters\n'abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ'\n</code></pre><p></p> <p>How not to use translate. Does not work:</p> <pre><code>&gt;&gt;&gt; 'a'.translate({'a':'b'})\n'a' # expected b\n</code></pre> <p>Use with str.maketrans</p> <pre><code>&gt;&gt;&gt; str.maketrans('a','b')\n{97: 98}\n&gt;&gt;&gt; 'a'.translate({97:98})\n'b'\n</code></pre> <p>or in one shot : </p><pre><code>'a'.translate(str.maketrans('a','b'))\n</code></pre><p></p> <p>You can also remove stuff by combining str.makestrans and translate. str.maketrans takes a third argument for removing characters. </p><pre><code>&gt;&gt;&gt; str.maketrans('','','a')\n{97: None}\n</code></pre><p></p> <p>Cant replace strings with this approach. </p> <pre><code>&gt;&gt;&gt; str.maketrans('replacethis','withthat','andremovethis')\nTraceback (most recent call last):\n  File \"&lt;stdin&gt;\", line 1, in &lt;module&gt;\nValueError: the first two maketrans arguments must have equal length\n</code></pre> <p>Its for replacing characters with their mappings:</p> <pre><code>&gt;&gt;&gt; str.maketrans('replacethis','replacethat','removethis')\n{114: None, 101: None, 112: 112, 108: 108, 97: 97, 99: 99, 116: None, 104: None, 105: None, 115: None, 109: None, 111: None, 118: None}\n&gt;&gt;&gt; str.maketrans('r','r')\n{114: 114}\n&gt;&gt;&gt; str.maketrans('r','r','r')\n{114: None}\n# The last argument for removal, overwrites and thus iverrides Mapping arguments.\n&gt;&gt;&gt; 'r'.translate(str.maketrans('r','r','r'))\n''\n</code></pre> <p>So finally this is how to do it : </p><pre><code>s = \"Hello, World! Python is amazing.\"\n\n# Create translation table\nimport string\ntranslator = str.maketrans('', '', string.punctuation)\n\n# Remove punctuation\nclean_text = s.translate(translator)\nprint(clean_text)\n</code></pre><p></p> <p>But one big problem : It removes apostrophe from 'shouldn't', 'how's' </p> <p>You could just exclude apostrophe from string.punctutaion and use translate + str.maketrans.</p> <p>Or use a regex. The following is from Claude :</p> <pre><code>import re\n\ns = \"This is a test string, with punctuation. This shouldn't fail...!\"\n\n# re.sub(pattern,with_whatever, input_string)\ntext = re.sub(r'[^\\w\\d\\s\\']+', '', s)\nprint(text)\n</code></pre> <p>In this code:</p> <ol> <li> <p><code>re.sub()</code> is being used to substitute matches of a pattern with a replacement string (empty string in this case).</p> </li> <li> <p>The regex pattern <code>r'[^\\w\\d\\s\\']+</code> breaks down as:</p> </li> <li><code>[^...]</code>: A negated character class (matches any character NOT in the brackets)</li> <li><code>\\w</code>: Word characters (letters, digits, underscores)</li> <li><code>\\d</code>: Digits (0-9)</li> <li><code>\\s</code>: Whitespace characters (spaces, tabs, newlines)</li> <li><code>\\'</code>: The apostrophe character (')</li> <li> <p><code>+</code>: One or more occurrences of the preceding character class</p> </li> <li> <p>So this pattern will match one or more characters that are NOT:</p> </li> <li>Letters, digits, or underscores</li> <li>Digits (redundant, as digits are already covered by \\w)</li> <li>Whitespace characters</li> <li> <p>Apostrophes</p> </li> <li> <p>When these matched characters are replaced with an empty string, you get a string with:</p> </li> <li>All punctuation marks removed EXCEPT for apostrophes</li> <li>All letters, numbers, spaces, and apostrophes preserved</li> </ol> <p>The output would be: </p><pre><code>This is a test string with punctuation This shouldn't fail\n</code></pre><p></p> <p>This approach is useful when you want to preserve contractions and possessives in English text (like \"shouldn't\", \"John's\", etc.) while removing other punctuation marks. It's a selective punctuation removal that's more language-aware than simply removing all punctuation.</p> <p>See also : https://stackoverflow.com/questions/59877761/how-to-strip-string-from-punctuation-except-apostrophes-for-nlp</p>"},{"location":"resources/python_cheatsheet/#raw-strings","title":"Raw Strings","text":"<p>The <code>r</code> before the string in <code>r'[^\\w\\s]'</code> is a prefix that denotes a \"raw string\" in Python. It's particularly useful when working with regular expressions. Here's what it does:</p>"},{"location":"resources/python_cheatsheet/#raw-string-prefix-r","title":"Raw String Prefix (<code>r</code>)","text":"<p>When you place an <code>r</code> before a string literal in Python, it tells the interpreter to treat backslashes (<code>\\</code>) as literal characters rather than escape characters.</p>"},{"location":"resources/python_cheatsheet/#without-the-r-prefix","title":"Without the <code>r</code> prefix:","text":"<pre><code>pattern = '\\w'  # Python interprets \\w as a special escape sequence\n</code></pre> In a normal string, backslashes have special meaning. For example, <code>\\n</code> represents a newline, <code>\\t</code> represents a tab, etc."},{"location":"resources/python_cheatsheet/#with-the-r-prefix","title":"With the <code>r</code> prefix:","text":"<pre><code>pattern = r'\\w'  # Python treats \\w literally as backslash + w\n</code></pre> In a raw string, backslashes are treated as literal backslashes, not as the start of an escape sequence."},{"location":"resources/python_cheatsheet/#why-its-important-for-regex","title":"Why it's important for regex:","text":"<p>Regular expressions use backslashes extensively to denote special character classes: - <code>\\w</code> matches word characters - <code>\\d</code> matches digits - <code>\\s</code> matches whitespace</p> <p>Without the <code>r</code> prefix, you would need to escape each backslash in your regex pattern: </p><pre><code># Without raw string - need to double escape\npattern = '\\\\w\\\\s'  # To match a word character followed by whitespace\n</code></pre><p></p> <p>With the <code>r</code> prefix, you can write regex patterns more naturally: </p><pre><code># With raw string - cleaner syntax\npattern = r'\\w\\s'  # Same pattern, more readable\n</code></pre><p></p> <p>Using raw strings makes regular expressions much more readable and less prone to errors, as you don't need to worry about Python's string escape processing interfering with regex syntax.</p> <p>Additional Resources :</p> <p>https://www.stratascratch.com/blog/python-string-methods-here-is-how-to-master-them/</p>"},{"location":"resources/uv_cheatsheet/","title":"uv Cheatsheet","text":""},{"location":"resources/uv_cheatsheet/#uv-cheatsheet","title":"uv  Cheatsheet","text":"<p>AI Assisted (Claude Sonnet 4)</p>"},{"location":"resources/uv_cheatsheet/#what-is-uv","title":"What is uv?","text":"<p>uv is a fast Python package manager and project manager written in Rust by Astral. It's designed as a drop-in replacement for pip, pip-tools, and virtualenv, offering significantly faster performance and better dependency resolution.</p> <p>Key benefits:</p> <ul> <li>10-100x faster than pip for most operations</li> <li>Unified tool replacing pip, pip-tools, virtualenv, and more</li> <li>Better dependency resolution with conflict detection</li> <li>Cross-platform support (Windows, macOS, Linux)</li> <li>Compatible with existing Python packaging standards</li> </ul>"},{"location":"resources/uv_cheatsheet/#installation","title":"Installation","text":""},{"location":"resources/uv_cheatsheet/#install-uv","title":"Install uv","text":"<pre><code># Using the official installer (recommended)\ncurl -LsSf https://astral.sh/uv/install.sh | sh\n\n# Using pip\npip install uv\n\n# Using Homebrew (macOS)\nbrew install uv\n\n# Using winget (Windows)\nwinget install --id=astral-sh.uv -e\n</code></pre>"},{"location":"resources/uv_cheatsheet/#verify-installation","title":"Verify Installation","text":"<pre><code>uv --version\n</code></pre>"},{"location":"resources/uv_cheatsheet/#project-management","title":"Project Management","text":""},{"location":"resources/uv_cheatsheet/#creating-a-new-project","title":"Creating a New Project","text":"<pre><code># Create a new Python project\nuv init my-project\ncd my-project\n\n# Create with specific Python version\nuv init --python 3.11 my-project\n\n# Create in current directory\nuv init .\n</code></pre> <p>What this does: Creates a new directory with a <code>pyproject.toml</code> file, a basic project structure, and initializes a virtual environment.</p>"},{"location":"resources/uv_cheatsheet/#project-structure","title":"Project Structure","text":"<p>After <code>uv init</code>, you'll see:</p> <pre><code>my-project/\n\u251c\u2500\u2500 pyproject.toml      # Project configuration\n\u251c\u2500\u2500 src/\n\u2502   \u2514\u2500\u2500 my_project/\n\u2502       \u2514\u2500\u2500 __init__.py\n\u251c\u2500\u2500 tests/\n\u2502   \u2514\u2500\u2500 __init__.py\n\u2514\u2500\u2500 README.md\n</code></pre>"},{"location":"resources/uv_cheatsheet/#virtual-environment-management","title":"Virtual Environment Management","text":""},{"location":"resources/uv_cheatsheet/#why-virtual-environments","title":"Why Virtual Environments?","text":"<p>Virtual environments isolate your project's dependencies from your system Python and other projects, preventing version conflicts.</p>"},{"location":"resources/uv_cheatsheet/#creating-virtual-environments","title":"Creating Virtual Environments","text":"<pre><code># Create virtual environment (uv automatically manages this)\nuv venv\n\n# Create with specific Python version\nuv venv --python 3.11\n\n# Create with custom name\nuv venv my-env\n\n# Create in specific location\nuv venv /path/to/my-env\n</code></pre>"},{"location":"resources/uv_cheatsheet/#activating-virtual-environments","title":"Activating Virtual Environments","text":"<pre><code># On Unix/macOS\nsource .venv/bin/activate\n\n# On Windows\n.venv\\Scripts\\activate\n\n# Or use uv run (automatically uses project's venv)\nuv run python script.py\n</code></pre>"},{"location":"resources/uv_cheatsheet/#package-installation","title":"Package Installation","text":""},{"location":"resources/uv_cheatsheet/#basic-installation","title":"Basic Installation","text":"<pre><code># Install a package\nuv add requests\n\n# Install multiple packages\nuv add requests numpy pandas\n\n# Install with version constraints\nuv add \"requests&gt;=2.25.0\"\nuv add \"django&gt;=4.0,&lt;5.0\"\n</code></pre> <p>What happens: uv resolves dependencies, updates <code>pyproject.toml</code>, and creates/updates <code>uv.lock</code> for reproducible installs.</p>"},{"location":"resources/uv_cheatsheet/#development-dependencies","title":"Development Dependencies","text":"<pre><code># Install development dependencies\nuv add --dev pytest black flake8\n\n# Install optional dependencies\nuv add --optional test pytest coverage\n</code></pre>"},{"location":"resources/uv_cheatsheet/#installing-from-different-sources","title":"Installing from Different Sources","text":"<pre><code># Install from PyPI (default)\nuv add requests\n\n# Install from Git repository\nuv add git+https://github.com/user/repo.git\n\n# Install from local path\nuv add ./local-package\n\n# Install from URL\nuv add https://files.pythonhosted.org/packages/.../package.whl\n</code></pre>"},{"location":"resources/uv_cheatsheet/#package-management","title":"Package Management","text":""},{"location":"resources/uv_cheatsheet/#listing-packages","title":"Listing Packages","text":"<pre><code># List installed packages\nuv pip list\n\n# Show dependency tree\nuv pip tree\n\n# Show package information\nuv pip show requests\n</code></pre>"},{"location":"resources/uv_cheatsheet/#updating-packages","title":"Updating Packages","text":"<pre><code># Update all packages\nuv lock --upgrade\n\n# Update specific package\nuv add requests --upgrade\n\n# Update to latest compatible version\nuv add \"requests&gt;=2.25.0\" --upgrade\n</code></pre>"},{"location":"resources/uv_cheatsheet/#removing-packages","title":"Removing Packages","text":"<pre><code># Remove a package\nuv remove requests\n\n# Remove multiple packages\nuv remove requests numpy pandas\n\n# Remove development dependencies\nuv remove --dev pytest\n</code></pre>"},{"location":"resources/uv_cheatsheet/#lock-files-and-reproducibility","title":"Lock Files and Reproducibility","text":""},{"location":"resources/uv_cheatsheet/#understanding-lock-files","title":"Understanding Lock Files","text":"<p>The <code>uv.lock</code> file contains exact versions of all dependencies and their dependencies. This ensures reproducible installs across different environments.</p> <pre><code># Generate/update lock file\nuv lock\n\n# Install from lock file\nuv sync\n\n# Install without updating lock file\nuv sync --frozen\n</code></pre>"},{"location":"resources/uv_cheatsheet/#synchronizing-environments","title":"Synchronizing Environments","text":"<pre><code># Sync environment with pyproject.toml\nuv sync\n\n# Sync only production dependencies\nuv sync --no-dev\n\n# Sync specific groups\nuv sync --group test\n</code></pre>"},{"location":"resources/uv_cheatsheet/#global-tool-management-pipx-replacement","title":"Global Tool Management (pipx Replacement)","text":""},{"location":"resources/uv_cheatsheet/#understanding-pipx-vs-uv-tools","title":"Understanding pipx vs uv Tools","text":"<p>pipx is a tool that installs CLI (command-line) applications in their own isolated environments while making them globally accessible. Think of it as an \"App Store for Python CLI tools\" where each app is self-contained but available everywhere.</p> <p>The Problem pipx Solves: </p><pre><code># \u274c Traditional global installation causes conflicts\npip install black    # black needs requests==2.25.0\npip install ruff     # ruff needs requests==2.30.0\n# Conflict! Dependencies clash in global environment\n\n# \u2705 pipx solution: each tool gets its own environment\npipx install black   # black + its dependencies in environment #1\npipx install ruff    # ruff + its dependencies in environment #2\n# No conflicts, both work perfectly!\n</code></pre><p></p>"},{"location":"resources/uv_cheatsheet/#pipx-vs-uv-tool-commands","title":"pipx vs uv Tool Commands","text":"Task pipx Command uv Equivalent Notes Install tool globally <code>pipx install black</code> <code>uv tool install black</code> Tool available system-wide Run without installing <code>pipx run cowsay hello</code> <code>uvx cowsay hello</code> Temporary execution List installed tools <code>pipx list</code> <code>uv tool list</code> Show all global tools Upgrade tool <code>pipx upgrade black</code> <code>uv tool upgrade black</code> Update to latest version Upgrade all tools <code>pipx upgrade-all</code> <code>uv tool upgrade --all</code> Update everything Uninstall tool <code>pipx uninstall black</code> <code>uv tool uninstall black</code> Remove tool Run specific version <code>pipx run --spec black==22.0.0 black</code> <code>uvx black@22.0.0</code> Use particular version"},{"location":"resources/uv_cheatsheet/#installing-global-development-tools","title":"Installing Global Development Tools","text":"<pre><code># Traditional pipx approach\npipx install black          # Code formatter\npipx install ruff           # Linter  \npipx install mypy           # Type checker\npipx install pytest        # Testing framework\npipx install pre-commit     # Git hooks\n\n# uv approach (much faster!)\nuv tool install black ruff mypy pytest pre-commit\n\n# Now available globally from any directory:\nblack my_file.py\nruff check .\nmypy src/\n</code></pre>"},{"location":"resources/uv_cheatsheet/#running-tools-without-installation","title":"Running Tools Without Installation","text":"<pre><code># Try tools without permanent installation\nuvx cowsay \"Hello World!\"           # Fun utility\nuvx httpie http GET httpbin.org/json # HTTP client\nuvx rich --help                     # Rich text demo\nuvx speedtest-cli                   # Internet speed test\n\n# Run with specific versions\nuvx black@23.1.0 my_file.py\nuvx ruff@0.1.0 check .\n\n# Run with additional dependencies\nuvx --with pandas python -c \"import pandas; print('Works!')\"\n</code></pre>"},{"location":"resources/uv_cheatsheet/#advanced-tool-management","title":"Advanced Tool Management","text":"<pre><code># Install with specific Python version\nuv tool install --python 3.11 black\n\n# Install with extras\nuv tool install 'jupyter[lab]'\n\n# Install from Git repository\nuv tool install git+https://github.com/user/tool.git\n\n# Install from local path\nuv tool install ./my-local-tool\n</code></pre>"},{"location":"resources/uv_cheatsheet/#tool-environment-isolation","title":"Tool Environment Isolation","text":"<p>When you install tools with uv, each gets its own isolated environment:</p> <pre><code>~/.local/share/uv/tools/\n\u251c\u2500\u2500 black/           # Black's isolated environment\n\u251c\u2500\u2500 ruff/            # Ruff's isolated environment  \n\u251c\u2500\u2500 pytest/         # Pytest's isolated environment\n\u2514\u2500\u2500 mypy/            # Mypy's isolated environment\n</code></pre> <p>Benefits: - No dependency conflicts between tools - Clean system Python remains untouched - Independent updates for each tool - Easy removal without leftover dependencies</p>"},{"location":"resources/uv_cheatsheet/#running-python-code","title":"Running Python Code","text":""},{"location":"resources/uv_cheatsheet/#direct-execution","title":"Direct Execution","text":"<pre><code># Run Python script with project dependencies\nuv run python script.py\n\n# Run module\nuv run -m pytest\n\n# Run with arguments\nuv run python script.py --verbose\n\n# Run interactive Python\nuv run python\n</code></pre>"},{"location":"resources/uv_cheatsheet/#running-tools","title":"Running Tools","text":"<pre><code># Run tools without installing globally\nuv run --with black black .\nuv run --with flake8 flake8 src/\n\n# Run with specific Python version\nuv run --python 3.11 python script.py\n</code></pre>"},{"location":"resources/uv_cheatsheet/#working-with-requirements-files","title":"Working with Requirements Files","text":""},{"location":"resources/uv_cheatsheet/#converting-from-pip","title":"Converting from pip","text":"<pre><code># Install from requirements.txt\nuv pip install -r requirements.txt\n\n# Generate requirements.txt from current environment\nuv pip freeze &gt; requirements.txt\n\n# Convert requirements.txt to pyproject.toml\nuv add -r requirements.txt\n</code></pre>"},{"location":"resources/uv_cheatsheet/#exporting-dependencies","title":"Exporting Dependencies","text":"<pre><code># Export to requirements.txt format\nuv export --format requirements-txt &gt; requirements.txt\n\n# Export only production dependencies\nuv export --no-dev --format requirements-txt &gt; requirements.txt\n</code></pre>"},{"location":"resources/uv_cheatsheet/#python-version-management","title":"Python Version Management","text":""},{"location":"resources/uv_cheatsheet/#installing-python-versions","title":"Installing Python Versions","text":"<pre><code># List available Python versions\nuv python list\n\n# Install specific Python version\nuv python install 3.11\n\n# Install multiple versions\nuv python install 3.10 3.11 3.12\n\n# Use specific Python for project\nuv python pin 3.11\n</code></pre>"},{"location":"resources/uv_cheatsheet/#finding-python-installations","title":"Finding Python Installations","text":"<pre><code># List installed Python versions\nuv python list --only-installed\n\n# Find Python executable\nuv python find 3.11\n</code></pre>"},{"location":"resources/uv_cheatsheet/#configuration","title":"Configuration","text":""},{"location":"resources/uv_cheatsheet/#global-configuration","title":"Global Configuration","text":"<pre><code># Set global configuration\nuv config set global.index-url https://pypi.org/simple/\n\n# View configuration\nuv config list\n\n# Edit configuration file\nuv config edit\n</code></pre>"},{"location":"resources/uv_cheatsheet/#project-configuration","title":"Project Configuration","text":"<p>Edit <code>pyproject.toml</code>:</p> <pre><code>[tool.uv]\n# Custom index URL\nindex-url = \"https://pypi.org/simple/\"\n\n# Additional index URLs\nextra-index-url = [\"https://private-pypi.company.com/simple/\"]\n\n# Exclude packages from updates\nconstraint-dependencies = [\"numpy==1.24.0\"]\n</code></pre>"},{"location":"resources/uv_cheatsheet/#advanced-features","title":"Advanced Features","text":""},{"location":"resources/uv_cheatsheet/#dependency-groups","title":"Dependency Groups","text":"<p>Define optional dependency groups in <code>pyproject.toml</code>:</p> <pre><code>[dependency-groups]\ntest = [\"pytest&gt;=6.0\", \"coverage&gt;=5.0\"]\ndocs = [\"sphinx&gt;=4.0\", \"sphinx-rtd-theme\"]\ndev = [\"black\", \"flake8\", \"mypy\"]\n</code></pre> <p>Install specific groups:</p> <pre><code>uv sync --group test\nuv sync --group docs --group dev\n</code></pre>"},{"location":"resources/uv_cheatsheet/#workspaces","title":"Workspaces","text":"<p>For multi-package projects, define workspace in <code>pyproject.toml</code>:</p> <pre><code>[tool.uv.workspace]\nmembers = [\"packages/*\"]\n</code></pre>"},{"location":"resources/uv_cheatsheet/#scripts","title":"Scripts","text":"<p>Define custom scripts in <code>pyproject.toml</code>:</p> <pre><code>[project.scripts]\nmy-script = \"my_package.cli:main\"\n\n[tool.uv.scripts]\ntest = \"pytest tests/\"\nlint = \"flake8 src/\"\nformat = \"black src/\"\n</code></pre> <p>Run scripts:</p> <pre><code>uv run test\nuv run lint\nuv run format\n</code></pre>"},{"location":"resources/uv_cheatsheet/#common-workflows","title":"Common Workflows","text":""},{"location":"resources/uv_cheatsheet/#starting-a-new-project","title":"Starting a New Project","text":"<pre><code># 1. Create project\nuv init my-project\ncd my-project\n\n# 2. Add dependencies\nuv add requests click\n\n# 3. Add development dependencies\nuv add --dev pytest black flake8\n\n# 4. Create your code\n# ... write your Python code ...\n\n# 5. Run tests\nuv run pytest\n</code></pre>"},{"location":"resources/uv_cheatsheet/#working-on-existing-project","title":"Working on Existing Project","text":"<pre><code># 1. Clone repository\ngit clone https://github.com/user/project.git\ncd project\n\n# 2. Sync dependencies\nuv sync\n\n# 3. Run the project\nuv run python main.py\n</code></pre>"},{"location":"resources/uv_cheatsheet/#migrating-from-pip","title":"Migrating from pip","text":"<pre><code># 1. Install uv\npip install uv\n\n# 2. Create virtual environment\nuv venv\n\n# 3. Install existing requirements\nuv pip install -r requirements.txt\n\n# 4. Convert to pyproject.toml (optional)\nuv add $(cat requirements.txt | tr '\\n' ' ')\n</code></pre>"},{"location":"resources/uv_cheatsheet/#performance-tips","title":"Performance Tips","text":""},{"location":"resources/uv_cheatsheet/#caching","title":"Caching","text":"<p>uv automatically caches packages and builds. Clear cache if needed:</p> <pre><code># Clear all caches\nuv cache clean\n\n# Clear specific package cache\nuv cache clean requests\n</code></pre>"},{"location":"resources/uv_cheatsheet/#parallel-installation","title":"Parallel Installation","text":"<p>uv installs packages in parallel by default. Configure concurrency:</p> <pre><code># Limit concurrent downloads\nuv pip install --max-concurrent-downloads 4\n</code></pre>"},{"location":"resources/uv_cheatsheet/#troubleshooting","title":"Troubleshooting","text":""},{"location":"resources/uv_cheatsheet/#common-issues","title":"Common Issues","text":"<p>Lock file conflicts: </p><pre><code># Reset lock file\nrm uv.lock\nuv lock\n</code></pre><p></p> <p>Python version issues: </p><pre><code># Check available Python versions\nuv python list\n\n# Install required Python version\nuv python install 3.11\n</code></pre><p></p> <p>Dependency conflicts: </p><pre><code># Show conflict resolution\nuv add package-name --verbose\n\n# Override with specific version\nuv add \"package-name==1.0.0\"\n</code></pre><p></p>"},{"location":"resources/uv_cheatsheet/#getting-help","title":"Getting Help","text":"<pre><code># General help\nuv --help\n\n# Command-specific help\nuv add --help\nuv run --help\n\n# Version information\nuv version\n</code></pre>"},{"location":"resources/uv_cheatsheet/#quick-reference","title":"Quick Reference","text":"Task Command Create project <code>uv init my-project</code> Add package <code>uv add requests</code> Remove package <code>uv remove requests</code> Install all deps <code>uv sync</code> Run script <code>uv run python script.py</code> Update deps <code>uv lock --upgrade</code> List packages <code>uv pip list</code> Create venv <code>uv venv</code> Install Python <code>uv python install 3.11</code> Export deps <code>uv export &gt; requirements.txt</code> Install global tool <code>uv tool install black</code> Run tool temporarily <code>uvx cowsay hello</code>"},{"location":"resources/uv_cheatsheet/#best-practices","title":"Best Practices","text":"<ol> <li>Always use lock files for reproducible builds</li> <li>Pin Python versions in production projects</li> <li>Use dependency groups to organize optional dependencies</li> <li>Keep pyproject.toml clean by using <code>uv add</code> instead of manual editing</li> <li>Use <code>uv run</code> instead of activating virtual environments manually</li> <li>Regularly update dependencies with <code>uv lock --upgrade</code></li> <li>Use workspaces for multi-package projects</li> <li>Cache Python installations for faster project setup</li> </ol>"},{"location":"resources/uv_cheatsheet/#migration-guide","title":"Migration Guide","text":""},{"location":"resources/uv_cheatsheet/#from-pip-virtualenv","title":"From pip + virtualenv","text":"Old Way New Way <code>python -m venv .venv</code> <code>uv venv</code> <code>pip install requests</code> <code>uv add requests</code> <code>pip install -r requirements.txt</code> <code>uv sync</code> <code>pip freeze &gt; requirements.txt</code> <code>uv export &gt; requirements.txt</code> <code>pip list</code> <code>uv pip list</code>"},{"location":"resources/uv_cheatsheet/#from-poetry","title":"From Poetry","text":"Poetry uv <code>poetry init</code> <code>uv init</code> <code>poetry add requests</code> <code>uv add requests</code> <code>poetry install</code> <code>uv sync</code> <code>poetry run python script.py</code> <code>uv run python script.py</code> <code>poetry shell</code> <code>uv run python</code> (or activate manually)"},{"location":"resources/uv_cheatsheet/#from-pipx","title":"From pipx","text":"pipx uv <code>pipx install black</code> <code>uv tool install black</code> <code>pipx run cowsay hello</code> <code>uvx cowsay hello</code> <code>pipx list</code> <code>uv tool list</code> <code>pipx upgrade black</code> <code>uv tool upgrade black</code> <code>pipx upgrade-all</code> <code>uv tool upgrade --all</code> <code>pipx uninstall black</code> <code>uv tool uninstall black</code> <p>official uv documentation</p>"},{"location":"blog/archive/2025/","title":"2025","text":""},{"location":"blog/archive/2025/#2025","title":"2025","text":""},{"location":"blog/archive/2023/","title":"2023","text":""},{"location":"blog/archive/2023/#2023","title":"2023","text":""},{"location":"blog/category/c/","title":"C","text":""},{"location":"blog/category/c/#c","title":"C","text":""},{"location":"blog/category/basics/","title":"Basics","text":""},{"location":"blog/category/basics/#basics","title":"Basics","text":""},{"location":"blog/category/encodings/","title":"Encodings","text":""},{"location":"blog/category/encodings/#encodings","title":"Encodings","text":""},{"location":"blog/category/data_structures/","title":"Data_Structures","text":""},{"location":"blog/category/data_structures/#data_structures","title":"Data_Structures","text":""},{"location":"blog/category/aws/","title":"AWS","text":""},{"location":"blog/category/aws/#aws","title":"AWS","text":""},{"location":"blog/category/iam/","title":"IAM","text":""},{"location":"blog/category/iam/#iam","title":"IAM","text":""},{"location":"blog/category/etl/","title":"ETL","text":""},{"location":"blog/category/etl/#etl","title":"ETL","text":""},{"location":"blog/category/serverless/","title":"Serverless","text":""},{"location":"blog/category/serverless/#serverless","title":"Serverless","text":""},{"location":"blog/category/algorithms/","title":"Algorithms","text":""},{"location":"blog/category/algorithms/#algorithms","title":"Algorithms","text":""}]}