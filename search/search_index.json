{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>Welcome to patibanb.me</p>"},{"location":"About/","title":"Index","text":"<p>Hello! I'm P. Bhanuprasad , an Engineer with 10 years of experience building robust data pipelines, scalable analytics platforms, and cloud-based solutions. My expertise spans across data engineering, ETL development, big data technologies, and cloud infrastructure.</p> <p>Over the past decade, I have worked with diverse teams to design and implement end-to-end data solutions, optimize data workflows, and enable data-driven decision making. I am passionate about leveraging technology to solve complex business problems and deliver actionable insights.</p> <p>I enjoy collaborating with cross-functional teams, building useful things, solving pesky problems and staying up-to-date with the latest advancements in data engineering, analytics and AI.</p> <p>Feel free to connect with me on LinkedIn or explore this website!</p>"},{"location":"Algorithms/","title":"Algorithms","text":"<p>These are my notes on Algorithms. This is very much a work in progress. My plan is to put down everything in a rough way first, and keep refining it over time.</p>"},{"location":"Algorithms/#philosophy","title":"Philosophy","text":"<p>Broadly, I classify any algorithm or data structure into recursive or iterative. Of course, this is not a strict demarcation, but only in terms of the usual implementations or concpetualization.</p> <p>As for data structures, there are two fundamental types in terms of how they are stored : contiguous storage (arrays, dynamic arrays) and linked storage (linked lists, trees, graphs) </p>"},{"location":"Algorithms/#mental-models","title":"Mental models","text":"<p>To effectively use a concept, it helps greatly to have the right mental model which allows you to use it in a consistent manner.</p> <p>For example, understanding and thinking of binary search in terms of bisect left and bisect right makes it easy to actually use it in a consistent manner.</p> <p>Another example, for array partitioning, just thinking of and  defining the loop invariant correctly is key. </p> <p>My effort is always to develop the correct understanding and mental model for any concept.</p>"},{"location":"Algorithms/#call-a-thing-by-its-right-name","title":"Call a thing by its right name","text":"<p>Just knowing the name of a thing is very powerful.  Know the name of the problem and know the name of the algorithm.</p>"},{"location":"Algorithms/External_Sorting/","title":"External Sorting","text":""},{"location":"Algorithms/External_Sorting/#problem-definition","title":"Problem Definition","text":"<p>Given limited RAM, how do you sort a file that is too large to fit into memory?</p>"},{"location":"Algorithms/External_Sorting/#solution","title":"Solution","text":"<p>Suppose you have an X GB file and only 2 GB of RAM, where X &gt; 2.</p> <p>Steps:</p> <ol> <li>Divide the file into chunks of size equal to available RAM (e.g., X // 2 GB chunks if RAM is 2 GB).</li> <li>For each chunk:<ul> <li>Read the chunk into memory.</li> <li>Sort it.</li> <li>Write it back to disk as a \"part file\".</li> </ul> </li> <li>Merge the sorted part files:<ul> <li>This is now a k-way merge problem.</li> <li>You can either:<ul> <li>Run a single k-way merge, or</li> <li>Use multiple passes of two-way merges to ultimately merge all part files into a single sorted file.</li> </ul> </li> </ul> </li> </ol>"},{"location":"Algorithms/External_Sorting/#additional-references","title":"Additional References","text":"<ol> <li>Wikipedia: External Sorting</li> </ol>"},{"location":"Algorithms/properties_of_comparison_based_sorting_algorithms/","title":"Properties of Comparison-Based Sorting Algorithms","text":""},{"location":"Algorithms/properties_of_comparison_based_sorting_algorithms/#quicksort","title":"Quicksort","text":"<ul> <li>Stability: Not stable  </li> <li>Adaptivity: Not adaptive  </li> <li>In-Place: Yes  </li> <li>Time Complexity: O(N log N) average case  </li> <li>Worst Case: O(N\u00b2) (e.g., sorted input, duplicate keys, poor pivot selection)  </li> </ul> <p>Notes: - Pivot selection and partitioning schemes significantly affect complexity.</p> <p>Pivot Selection: </p> <ul> <li>Fixed at high or low index (worst case)</li> <li>Random (good enough in practice)</li> <li>Median-of-three, etc.</li> </ul> <p>Partitioning: </p> <ul> <li>Two-way (e.g., Hoare partitioning with weak condition)</li> <li>Three-way (e.g., Dijkstra)</li> <li>Dual-pivot partitioning</li> </ul> <p>Quicksort is a highly researched algorithm with many variations and derivatives\u2014it's more like a family of algorithms.</p> <p>Important: </p> <ul> <li>The most common partitioning scheme found online (Lomuto partition) performs more swaps than Hoare partition with a weak condition.</li> <li>Lomuto degrades to O(N\u00b2) when all elements are the same.</li> <li>Hoare partition with a strict condition also degrades to O(N\u00b2) for duplicate keys.</li> <li>Lomuto was popularized by Bentley in \"Programming Pearls\" because he found Hoare unintuitive.  </li> <li>Advantage: Lomuto uses two forward iterators, so it can be used on singly linked lists.</li> </ul>"},{"location":"Algorithms/properties_of_comparison_based_sorting_algorithms/#mergesort","title":"Mergesort","text":"<ul> <li>Stability: Stable  </li> <li>Adaptivity: Not adaptive  </li> <li>In-Place: No (needs extra space)  </li> <li>Time Complexity: O(N log N) worst case  </li> </ul> <p>Can be combined with insertion sort for small subproblem sizes.</p>"},{"location":"Algorithms/properties_of_comparison_based_sorting_algorithms/#bubble-sort","title":"Bubble Sort","text":"<ul> <li>Stability: Stable  </li> <li>Adaptivity: Can be made adaptive (add a flag to check if a swap occurred; if not, exit early)  </li> <li>In-Place: Yes  </li> <li>Time Complexity: O(N\u00b2)  </li> </ul> <p>Optimized Bubble Sort Example:</p> <pre><code>def optimized_bubble_sort(arr):\n    n = len(arr)\n    for i in range(n - 1):\n        swapped = False  # Flag for no swaps in a pass\n        for j in range(n - i - 1):\n            if arr[j] &gt; arr[j + 1]:\n                arr[j], arr[j + 1] = arr[j + 1], arr[j]\n                swapped = True  # Swap occurred\n        if not swapped:\n            return  # Already sorted, no more passes needed\n    return arr\n</code></pre>"},{"location":"Algorithms/properties_of_comparison_based_sorting_algorithms/#insertion-sort","title":"Insertion Sort","text":"<ul> <li>Stability: Stable  </li> <li>Adaptivity: Adaptive  </li> <li>In-Place: Yes  </li> <li>Time Complexity: O(N\u00b2)  </li> </ul> <p>Example:</p> <pre><code>def insertion_sort(arr):\n    for i in range(1, len(arr)):\n        key = arr[i]\n        j = i - 1\n        while j &gt;= 0 and key &lt; arr[j]:\n            arr[j + 1] = arr[j]\n            j -= 1\n        arr[j + 1] = key\n</code></pre> <ul> <li>Similar to optimized bubble sort, except:</li> <li>In bubble sort, the maximum element is bubbled into the unsorted section.</li> <li>In insertion sort, the minimum element is inserted into the sorted section.</li> <li>Online: Each new element can be put in its sorted place as it is received.</li> <li>Shell Sort: A faster variation of insertion sort (named after D.L. Shell), using insertion sort on periodic subarrays.</li> </ul> <p>See also: - Stack Overflow: Online vs Offline Sorting Algorithms - CS StackExchange: Fastest Online Sorting Algorithm</p>"},{"location":"Algorithms/properties_of_comparison_based_sorting_algorithms/#selection-sort","title":"Selection Sort","text":"<ul> <li>Stability: Not stable  </li> <li>Adaptivity: Not adaptive  </li> <li>In-Place: Yes  </li> <li>Time Complexity: O(N\u00b2)  </li> </ul> <p>Note: - Only advantage over insertion sort: performs fewer swaps.</p>"},{"location":"Algorithms/properties_of_comparison_based_sorting_algorithms/#additional-resources","title":"Additional Resources","text":"<ol> <li>Toptal: Sorting Algorithms</li> <li>YouTube: Sorting Algorithms Playlist</li> </ol>"},{"location":"Algorithms/Array_Linearization/Mapping_1d_to_2d_Arrays/","title":"Mapping 1D Array to 2D Array","text":"<p>Consider a 1D array that represents a 2D array:</p> <pre><code>[ 00, 01, 02, 11, 12, 13 ]  # Row-major order\n  0   1   2   3   4   5\n</code></pre> <p>This could represent the following 2D array: <pre><code>[\n  [00, 01, 02],\n  [11, 12, 13]\n]\n</code></pre></p>"},{"location":"Algorithms/Array_Linearization/Mapping_1d_to_2d_Arrays/#row-major-order","title":"Row-Major Order","text":"<p>To convert a 1D array index back to 2D array coordinates in row-major order, use these formulas:</p> \\[ \\text{row_index} = \\left\\lfloor\\frac{\\text{index}}{\\text{number_of_cols}}\\right\\rfloor \\] \\[ \\text{col_index} = \\text{index} \\bmod \\text{number_of_cols} \\] <p>Explanation:</p> <ul> <li>Integer division (\u230aindex/cols\u230b) gives the row number because each row contains <code>number_of_cols</code> elements</li> <li>Remainder (modulo) gives the position within that row</li> </ul> <p>For example, given index <code>4</code> with <code>number_of_cols = 3</code>:</p> <ul> <li>row_index = \u230a4/3\u230b = 1</li> <li>col_index = 4 mod 3 = 1</li> <li>Therefore, index 4 maps to position (1,1)</li> </ul>"},{"location":"Algorithms/Array_Linearization/Mapping_1d_to_2d_Arrays/#column-major-order","title":"Column-Major Order","text":"<p>For a 1D array in column-major order: <pre><code>[ 00, 11, 01, 12, 02, 13 ]  # Column-major order\n  0   1   2   3   4   5\n</code></pre></p> <p>The formulas are:</p> \\[ \\text{row_index} = \\text{index} \\bmod \\text{number_of_rows} \\] \\[ \\text{col_index} = \\left\\lfloor\\frac{\\text{index}}{\\text{number_of_rows}}\\right\\rfloor \\] <p>Explanation:</p> <ul> <li>Remainder (modulo) gives position within the current column</li> <li>Integer division gives which column we're in</li> </ul> <p>For example, given index <code>3</code> with <code>number_of_rows = 2</code>:</p> <ul> <li>row_index = 3 mod 2 = 1</li> <li>col_index = \u230a3/2\u230b = 1</li> <li>Therefore, index 3 maps to position (1,1)</li> </ul>"},{"location":"Algorithms/Array_Linearization/Mapping_1d_to_2d_Arrays/#practice","title":"Practice","text":"Search in a 2D Matrix <pre><code>         \"\"\"\n         Solution for :  https://leetcode.com/problems/search-a-2d-matrix/description/\n\n\n         If you look at the examples, the data in the 2d matrix is sorted in row-major order.\n\n         You can use a 1D index to binary search (bisect left) over the range of the entire array.\n\n         The range of the 1D index will be :\n         lo =0 , hi = num_rows*num_cols\n\n         To get the value at any 1D index :\n         matrix[1d_index//num_cols][1d_index%num_cols]\n         \"\"\"\n         class Solution:\n             @staticmethod\n             def get_val_at_sn(sn: int, matrix : List[List[int]]) -&gt; int :\n                 \"\"\"Given a 1D index into 2D Matrix, Return the corresponding Value.\"\"\"\n                 num_rows,num_cols = len(matrix), len(matrix[0])\n\n                 return matrix[sn//num_cols][sn%num_cols ]\n\n             def searchMatrix(self, matrix: List[List[int]], target: int) -&gt; bool:\n                 \"\"\"\n                 00 - 0 01 -1 02 -2\n                 10 - 3 11 -4 12 -5\n                 21 - 6  22 -7 23 -8 \n\n                 3 - 3 \n                 r = 1 , c = 0\n\n                 r = sn // num_cols , c = sn % num_columns\n                 \"\"\"\n\n                 num_rows,num_cols = len(matrix), len(matrix[0])\n                 lo,hi = 0 , num_rows*num_cols\n\n                 while lo &lt; hi :\n                     mid = (lo + hi) // 2\n                     val = self.get_val_at_sn(mid,matrix)\n                     if val &lt; target :\n                         lo = mid + 1\n                     else :\n                         hi = mid\n\n                 if lo ==  num_rows*num_cols :\n                     return False\n\n                 if self.get_val_at_sn(lo,matrix) == target :\n                     return True\n\n                 return False\n</code></pre> Reshape Matrix <pre><code>         \"\"\"\n         Solution for: https://leetcode.com/problems/reshape-the-matrix/\n\n         Allocate the target array.\n         Use 1D indexing to read and write values from the source matrix to the target matrix.\n         \"\"\"\n         class Solution:\n             @staticmethod\n             def read_val(linear_index, matrix) : \n                 num_cols = len(matrix[0])\n                 return matrix[linear_index//num_cols][linear_index%num_cols]\n\n             @staticmethod\n             def write_val(linear_index, matrix,val) :\n                 \"\"\"\n                 write val at linear index into matrix\n                 \"\"\"\n                 num_cols = len(matrix[0])\n                 matrix[linear_index//num_cols][linear_index%num_cols] = val \n\n             def matrixReshape(self, mat: List[List[int]], r: int, c: int) -&gt; List[List[int]]:\n\n                 #Quote :\"If the reshape operation with given parameters is possible and legal, output the new reshaped matrix; Otherwise, output the original matrix.\"\n                 if not len(mat)*len(mat[0]) == r*c :\n                     return mat\n\n                 # Allocate target Matrix \n                 target = [ [0]*c for _ in range(r) ]\n\n                 #read from source matrix and write into target matrix\n                 for i in range(len(mat) * len(mat[0])) :\n                     val = self.read_val(i,mat)\n                     self.write_val(i,target,val)\n\n                 return target\n</code></pre>"},{"location":"Algorithms/Array_Linearization/Mapping_2d_to_1d_Arrays/","title":"Mapping 2D Array to 1D Array","text":"<p>Consider a 2D array:</p> <pre><code>[\n  [00, 01, 02],\n  [11, 12, 13]\n]\n</code></pre> <p>To convert this 2D array to a 1D array using row-major order, you list the elements row by row:</p> <pre><code>[ 00  ,01 , 02,   11,   12 ,  13 ]\n  0     1   2      3     4    5 \n</code></pre>"},{"location":"Algorithms/Array_Linearization/Mapping_2d_to_1d_Arrays/#row-major-order","title":"Row Major Order","text":"<p>For example, the element <code>12</code> is at position <code>(1, 2)</code> (row index 1, column index 2). Its corresponding index in the 1D array is <code>4</code>.</p> <p>The general formula for mapping a 2D array element at position \\((\\text{row_index},\\ \\text{col_index})\\) to a 1D array index is:</p> \\[ \\text{index} = \\text{row_index} \\times \\text{number_of_cols} + \\text{col_index} \\] <p>Explanation:</p> <ul> <li>\\(\\text{row_index} \\times \\text{number_of_cols}\\): Counts all elements in the rows before the current row.</li> <li>\\(\\text{col_index}\\): Gives the position within the current row.</li> </ul> <p>So, to find the 1D index, count all elements in previous rows, then add the column index of the current element.</p>"},{"location":"Algorithms/Array_Linearization/Mapping_2d_to_1d_Arrays/#column-major-order","title":"Column Major Order","text":"<p>To convert a 2D array to a 1D array using column-major order, you list the elements column by column:</p> <pre><code>[ 00, 11,   01,  12,  02,  13]\n  0   1     2    3    4    5\n</code></pre> <p>For example, the element <code>12</code> is at position <code>(1, 2)</code> (row index 1, column index 2). Its corresponding index in the 1D array is <code>3</code>.</p> <p>The general formula for mapping a 2D array element at position \\((\\text{row_index},\\ \\text{col_index})\\) to a 1D array index in column-major order is:</p> \\[ \\text{index} = \\text{col_index} \\times \\text{number_of_rows} + \\text{row_index} \\] <p>Explanation:</p> <ul> <li>\\(\\text{col_index} \\times \\text{number_of_rows}\\): Counts all elements in the columns before the current column.</li> <li>\\(\\text{row_index}\\): Gives the position within the current column.</li> </ul> <p>So, to find the 1D index, count all elements in previous columns, then add the row index of the current element.</p>"},{"location":"Algorithms/Array_Linearization/Mapping_2d_to_1d_Arrays/#addition-resources","title":"Addition Resources","text":"<ol> <li>Wikipedia - Row Major vs Column Major</li> <li>Performance Analysis : Row Major vs Column Major</li> <li>Why Use Parquet</li> <li>Reddit Discussion on Columnar DBs</li> </ol>"},{"location":"Algorithms/Array_Partitioning/Djikstra/","title":"Djikstra's Three Way Partition","text":"<p>Coming Soon</p>"},{"location":"Algorithms/Array_Partitioning/Hoare/","title":"Hoare Partition","text":""},{"location":"Algorithms/Array_Partitioning/Hoare/#hoare-partition-with-strict-condition","title":"Hoare partition with strict condition","text":"<p>Let us consider the simplest version of the problem.</p> <p>Given an array of integers, rearrange the elements such that the left part contains elements less than or equal to a pivot value and the right part contains values greater than the pivot value.</p> <p>Consider this example array and a pivot value of 4   :</p> <p>Example</p> <p>[ 1 , 9, 3, 5, 7, 4, 8  ]</p> <p>If this array were already partitioned s.t. the left partition has only elements less than or equal to the pivot. These would hold true : <pre><code>Any element in left partition &lt;= pivot\nAny element in right partition &gt; pivot\n</code></pre> If we take left and right pointers  while verifying that arr[left] &lt;= pivot and arr[right] &gt; pivot, left and right pointers would move past each other before evaluating to False for the first time. Left pointer would end up at the BEGINNING of the right partition and the right pointer would end up at the END of the left partition.</p> <p>Pointers</p> <pre><code>[ 1,   3,     4,   5,   7,   8,   9 ]\n  L/T  L/T   L/T *L/F*                            # L = Left pointer, T = arr[left]&lt;= pivot is True , F = Its False\n            *R/F* R/T   R/T     R/T    R/T        # R = right pointer, T = arr[right] &gt; pivot is True, F = Its False   \n</code></pre> <p>If left evaluates to True while its less than right, it would mean that this element is in the wrong partition. This also implies that there must be an element in the right partition which belongs to the left partition. </p> <p>Pointers</p> <pre><code>[  1 ,   9,   3,   5,   7,   4,   8  ]         \n  L/T **L/F**                                    # L = Left pointer, T = arr[left]&lt;= pivot is True, F = Its False\n                          **R/F**   R/T          # R = right pointer, T = arr[right] &gt; pivot is True, F = Its False                \n</code></pre> <p>We can simply swap these and keep moving until left and right pass each other.</p> <p>So, the algorithm would be:</p> <p>Algorithm</p> <pre><code>#Initialize pivot = arr[0]\n\n#Invariant 1) [0,right] only contains elements less than or equal to pivot\n#Invariant 2) (right, len(arr) -1] only contains elements greater than pivot\n\n\nWhile left &lt;= right :\n      while left&lt;=right and (Invaraint 1) is True :\n           left +=1\n      while left&lt;=right and (Invariant 2) is True :\n           right -=1\n      if left &lt; right :\n          swap left and right\nreturn right\n</code></pre> <p>This implementation does not necessarily place the pivot in its sorted position.</p> <p>Or rather, the problem is not asking us to do this,</p> <p>This array is partitioned by 4 but 4 is not in its sorted position.</p> <p>Example</p> <pre><code>[ 1,   4,    3,  |5,   7,   8,   9 ]\n</code></pre> <p>We can put the pivot in its sorted position by swapping it with the right pointer after running the algorithm.  A good way to handle this is :</p> <ul> <li>Ensure that  the pivot at the lowest index.</li> <li>Run the partitioning algorithm on the rest of the array.</li> <li>Swap right with low.</li> </ul> <p>But again, this will not group multiple instances of pivot together if pivot is duplicated. That is another problem called three way partitioning.</p> <p>Here is the python implementation for both cases :</p> Code Without placing pivot in sorted postitionPlacing Pivot in its sorted position <pre><code>from typing import List\n\n#Given a list return partition index after partition on the pivot index value\ndef partition(nums: List[int],lo,hi) -&gt; int :\n    \"\"\"\n    Hoare partition with strong condition.\n    Picks the first element as the pivot value.\n    There is no guarantee that pivot will be in its right place in this implementation.\n    All we get is that the first half has elements less than or equal to pivot.\n    Pivot can be any value in the array. (Pivot Value can actually be any arbitrary value in this version).\n    \"\"\"\n    pivot = nums[lo]\n    left,right = lo+1 ,hi\n\n    while left &lt;= right :\n        while left&lt;= right and nums[left] &lt;=  pivot :\n            left+=1\n        while left &lt;=right and nums[right] &gt; pivot :\n            right-=1\n        if left &lt; right :\n            nums[left],nums[right] = nums[right],nums[left]\n\n    print(f\"Pivot Value: {pivot} , Partitioned list : {nums}, Partition Index = {right}\")\n    return right\n\nif __name__ == \"__main__\" :\n    nums = [2,4,5,1,4,8,9]\n    partition(nums, 0, len(nums)-1)\n    nums = [2,2,2,2,2,2]\n    partition(nums, 0, len(nums)-1)\n</code></pre> <pre><code>from typing import List\nimport random\n\n# Given a list return partition index after partition on the pivot index value\ndef partition(nums: List[int],lo,hi) -&gt; int :\n    \"\"\"\n    Hoare partition with strong condition.\n    Here we modififed the code slightly to:\n    Fix pivot value at index zero.\n    Partition by the pivot value.\n    Finally place the pivot  at the end of the left partition.\n    This implementation puts pivot in the sorted position as well after partitioning.\n    This version is  not entropy optimal . That is, it does not lead to balanced partitions when there are a lot of duplicates. \n\n    Why? Consider,  if this partition scheme is used in quickselect for finding the smallest element in an array of size n). \n    What happens when all elements are duplicates (disregarding random pivot selection):\n    [1,1,1,1,1,1]\n     l         r   Iter 1 : 6  comparisons\n     l       r     Iter 2 : 5  comparisons\n     l     r       Iter 3 : 4  comparisons\n     l   r         Iter 4 : 3  comparisons\n     l r           Iter 5 : 2  comparisons\n     lr            Iter 6 : 1  comparisons\n\n    Number of comparisons = SUM([1.....n]) = n(n+1)/2 = (n*2 + n ) /2\n    Therefore, Complexity is O(n^2) \n    \"\"\"\n\n    print(f\"Input List : {nums}\")\n    left, right = lo,hi\n\n    # RANDOM PIVOT SELECTION\n    # Always keep pivot at index lo\n    # pivot_index = random.randint(lo,hi)\n    # nums[lo],nums[pivot_index] =  nums[pivot_index],nums[lo]\n\n    #PARTITIONING\n    left+=1\n    pivot_val = nums[lo]\n\n    while left &lt;= right :\n\n        while left &lt;= right and nums[left] &lt;= pivot_val :\n            left+=1\n\n        while left &lt;= right and nums[right] &gt; pivot_val :\n            right-=1\n\n        if left &lt; right :\n            #swap\n            nums[left],nums[right] = nums[right],nums[left]\n\n    #Put pivot Value in sorted  position\n    nums[lo] , nums[right] =  nums[right] ,nums[lo]\n    print(f\"Pivot Value: {pivot_val}  ,Partitioned list : {nums}, Partition Index = {right}\")\n    return right\n\nif __name__ == \"__main__\" : \n    # nums= [1,2,3]\n    # partition(nums, 0, len(nums)-1)\n    # nums = [2,2,2,2]\n    # partition(nums, 0, len(nums)-1)\n    # nums = [2,4,5,1,4,8,9]\n    # partition(nums, 0, len(nums)-1)\n    # nums= [3,2]\n    # partition(nums, 0, len(nums)-1)\n    # nums= [3]\n    # partition(nums, 0, len(nums)-1)\n    nums = [5,2,1,1,1,1,1,1,1,1,1,5,5,-3,-2,-5]\n    partition(nums, 0, len(nums)-1)\n</code></pre>"},{"location":"Algorithms/Array_Partitioning/Hoare/#hoare-partition-with-weak-condition","title":"Hoare partition with weak condition","text":"<p>The problem with the partition scheme with a strict condition (where we always put pivot values  in one of the partitions) is that it produces unbalanced partitions when used in quickselect or quicksort.</p> <p>Consider if the above partition scheme were used in quickselect for finding the smallest element in an array with k=0 and fixed pivot selection at arr[0]. </p> <p>What happens when all elements are duplicates :</p> <p>Dry Run</p> <p>l and r indicate the search space for each iteration of quickselect loop/each recursive call to quickselect. Comparisons refer to those made by the partitioning subroutine.</p> <pre><code>    [1,1,1,1,1,1]\n     l         r   Iter 1 : 5  comparisons\n     l       r     Iter 2 : 4  comparisons\n     l     r       Iter 3 : 3  comparisons\n     l   r         Iter 4 : 2  comparisons\n     l r           Iter 5 : 1  comparisons\n     lr            Iter 6 : 0  comparisons\n</code></pre> <p>The Hoare partition, after performing r-l+1 comparisons, would return the rightmost position for each call recursive call to quickselect. r would reduce linearly until it meets l, thus giving a quadratic running time!</p> <p>Number of comparisons = SUM([1.....n]) = n(n+1)/2 = (n*2 + n ) /2 Therefore, Complexity is O(n^2)  in this case.</p> <p>To mitigate this, we could slightly modify the definition of our partitions :</p> <p><pre><code>Any element in left partition &lt;= pivot\nAny element in right partition &gt;= pivot\n</code></pre> In terms of Invariants :</p> <p>Invariants</p> <pre><code>Invariant 1) [low,right] only contains elements less than OR EQUAL to pivot\n\nInvariant 2) (right, high] only contains elements greater OR EQUAL to pivot\n</code></pre> <p>That is, pivot values are allowed to be in either partition.</p> <p>Our problem now becomes : </p> <p>Reframed Problem</p> <p>Given an array of integers, rearrange the elements such that the left part contains elements less than or equal to a pivot value and the right part contains values greater than or equal to the pivot value.</p> <p>The implementation is tricky when handling values equal to pivot. We stop scanning when left or right are equal to pivot in either partition. We swap and move the left and right pointers. After any swap, both partitions will increase by 1. Pivot values might end up in any partition. This will also be the case when, left and right are both pointing to pivot value i.e. both partitions increase by 1 and there is a redundant swap between two pivot values.</p> <p>Algorithm</p> <pre><code>While left &lt;= right :\n      while  left&lt;=right and left &lt; pivot :\n           left +=1\n      while left&lt;=right and right &gt; pivot:\n           right -=1\n      if left &lt;= right :\n          swap left and right\n          left+=1\n          right-=1\nreturn right\n</code></pre> <p>Let us see the behavior of quickselect with this scheme for the same case :</p> <p>Dry Run</p> <pre><code>    [1,1,1,1,1,1]\n     l         r   Iter 1 : 5  comparisons\n     l   r         Iter 2 : 3  comparisons; This iteration returns r=0 and ends the loop\n     rl            Iter 2 END \n</code></pre> <p>Now the hoare partitioning subroutine will reduce r by half in each iteration. And quickselect is called LOG(n) times in the worst case, when k = 0. The complexity is reduced to LOG(n)*LOG(n).</p> <p>Here is the implementation :</p> Code Hoare Partition with Weak Condition <pre><code>from typing import List\nimport random\n\n\ndef partition(arr, lo, hi):\n    \"\"\"\n    Entropy Optimal Hoare partition.\n    Produces balanced partitions when there are large number of duplicates.\n    [lo,right] contains elements less than or equal to pivot.\n    (right,hi]  contains elements greater than or equal to pivot.\n    Invariants for the while loop :\n    [lo] has pivot  \n    [lo, left)  &lt;= pivot    #Has values &lt;= pivot\n    (right, hi] &gt;= pivot\n    [pivot|--- &lt;=pivot-----|-----Undetermined-------|---&gt;=pivot----]\n                            left               right\n    After execution of while loop : \n    [pivot|----&lt;=pivot------|-----&gt;=pivot------]\n    lo                  right                  hi\n    [lo,right] &lt;= pivot\n    After Putting pivot in sorted position : \n    [----&lt;=pivot----|pivot|-----&gt;=pivot------]\n    lo               right                  hi\n    Finally, return right.\n    \"\"\"\n    #PIVOT SELECTION\n    #Pick a random pivot index and always keep pivot at index lo\n    #NB:  random.randint(0,0) is 0.\n    pivot_index = random.randint(lo,hi)\n    arr[lo],arr[pivot_index] =  arr[pivot_index],arr[lo]\n    #read pivot value\n    pivot = arr[lo]\n\n    #PARTITIONING\n    #partition [lo+1,hi] ; \n    #NB : when lo == hi , while loop will not be executed\n    left,right = lo+1, hi\n    while left&lt;=right:\n        #Move left ahead if arr[left] is strictly less than pivot value\n        while left &lt;= right and arr[left] &lt; pivot :\n            left+=1\n        #Move right to the left if it is strictly higher than pivot\n        while left &lt;= right  and arr[right] &gt; pivot :\n            right-=1\n        #Swap left and right and move pointers\n        #If both values are equal to pivot this will do a swap,move pointers and effectively leave pivot values where they are. \n        if left &lt;=  right :\n            arr[left], arr[right] = arr[right], arr[left]\n            right-=1\n            left+=1\n    #Put pivot in sorted position\n    arr[lo], arr[right] = arr[right], arr[lo]\n    print(f\"Pivot Value: {pivot}  ,Partitioned list : {arr}, Partition Index = {right}\")\n    return right\n\nif __name__ == \"__main__\" : \n    # nums= [1,2,3]\n    # partition(nums, 0, len(nums)-1)\n    # nums = [2,2,2,2]\n    # partition(nums, 0, len(nums)-1)\n    # nums = [2,4,5,1,4,8,9]\n    # partition(nums, 0, len(nums)-1)\n    # nums= [3,2]\n    # partition(nums, 0, len(nums)-1)\n    # nums= [3]\n    # partition(nums, 0, len(nums)-1)\n    nums = [5,2,1,1,1,1,1,1,1,1,1,5,5,-3,-2,-5]\n    partition(nums, 0, len(nums)-1)\n</code></pre>"},{"location":"Algorithms/Array_Partitioning/Hoare/#important-things-to-note","title":"Important things to Note:","text":"<ul> <li> <p>This algorithm is not stable. The relative order of elements will not be preserved. Naive partitioning using extra space is the only algorithm which preserves relative order of elements.</p> </li> <li> <p>When used in quicksort/quickselect, instead of fixed pivot selection either random pivot selection or selection algorithms like Median of Medians are used to pick the pivot.  </p> </li> </ul>"},{"location":"Algorithms/Array_Partitioning/Hoare/#additional-resources","title":"Additional Resources","text":"<ol> <li>ALGS4 - Sedgewick </li> </ol>"},{"location":"Algorithms/Array_Partitioning/In-Place_Deduplication/","title":"In-Place Deduplication","text":""},{"location":"Algorithms/Array_Partitioning/In-Place_Deduplication/#in-place-deduplication-of-sorted-array","title":"In-Place deduplication of Sorted Array","text":""},{"location":"Algorithms/Array_Partitioning/In-Place_Deduplication/#defining-the-problem","title":"Defining the Problem","text":"<p>The task is to partition a sorted array s.t the left partition contains only unique elements. The relative order of elements must be maintained. The function should return the length of the left partition. (Leetcode Link)</p> Example <pre><code>Example 1:\n\nInput: nums = [1,1,2]\nOutput: 2, nums = [1,2,_]\nExplanation: Your function should return k = 2, with the first two elements of nums being 1 and 2 respectively.\nIt does not matter what you leave beyond the returned k (hence they are underscores).\nExample 2:\n\nInput: nums = [0,0,1,1,1,2,2,3,3,4]\nOutput: 5, nums = [0,1,2,3,4,_,_,_,_,_]\nExplanation: Your function should return k = 5, with the first five elements of nums being 0, 1, 2, 3, and 4 respectively.\nIt does not matter what you leave beyond the returned k (hence they are underscores).\n</code></pre>"},{"location":"Algorithms/Array_Partitioning/In-Place_Deduplication/#algorithm","title":"Algorithm","text":"<p>Key Ideas</p> <ul> <li>[0,s] - Is the inclusive interval containing only non-duplicate elements</li> <li>(s,f) - Is the exclusive interval containing duplicates from [0,s]</li> </ul> <p>Initialize s=0 and f=0.</p> <p>s+1 is the write position for the next unique element. f scouts ahead for the next unique element.</p> <p>The key insight is to compare f with s. Since the array is sorted, all elements between s and f that equal arr[s] are duplicates. </p> <ul> <li> <p>s stays at the inclusive right boundary of the interval with no duplicates.</p> </li> <li> <p>To maintain this invariant, f moves forward to find the next unique element which should be placed at s+1. </p> </li> <li> <p>This means if s and f are equal, we move only f ahead to find the next unique element.</p> </li> <li> <p>Once a swap is made the interval with duplicates increases by 1 so s is incremented and Invariant 1 is maintained.</p> </li> </ul> <p>Notice that f is incremented in both cases. But s is only incremented if a swap is made.</p> <p>Algorithm Steps:</p> <ol> <li>Initialize s=0, f=0 (both pointing to first element)</li> <li> <p>For each position f:</p> <ul> <li> <p>If nums[f] \u2260 nums[s]:</p> <ul> <li>Write nums[f] to position s+1</li> <li>Increment s</li> </ul> </li> </ul> </li> <li> <p>Return s+1 as the length of valid partition</p> </li> </ol> <p>Initially,</p> <ul> <li>s,f = 0,0</li> <li>[0,0] - Has only 1 element. First Invariant is trivially True.</li> <li>(0,0) - Empty set. Second Invariant is trivially True.</li> </ul> <p>Maintenance</p> <p>In the loop we will maintain the invariants. Here is the dry run:</p> <p>DRY RUN</p> <pre><code>    [ 1  1  1  2  2  3  4  5]\n      sf                      # s and f are equal, only f moves forward  \n      s  f                    # ----\"----\n      s     f                 # ----\"----\n      s        f              # ----\"----\n    [ 1  2  1  1  2  3  4  5] # s and f are not equal ; f is at next unique element; Swap s+1 with f and move both pointers forward\n         s        f           # s and f are equal, only f moves forward\n         s           f        # s and f are not equal ;f is at next unique element; Swap s+1 with f and move both pointers forward\n    [ 1  3  1  1  2  2  4  5]\n            s           f     # s and f are not equal ;f is at next unique element; Swap s+1 with f and move both pointers forward\n    [ 1  3  4  1  2  2  1  5]\n               s           f  # s and f are not equal ;f is at next unique element; Swap s+1 with f and move both pointers forward \n    [ 1  3  4  5  2  2  1  1]\n                  s           f  # Final state after loop ends     \n\n    Notice that (s,f) always contains duplicates of elements from [0,s]\n</code></pre> <p>Code</p> <pre><code>class Solution:\n    def removeDuplicates(self, nums: List[int]) -&gt; int:\n        s =0\n        for f in range(len(nums)) :\n            if nums[s] != nums[f] :\n                nums[s+1] , nums[f] = nums[f],nums[s+1]\n                s+=1\n        #[0,s] is the interval with no duplicates\n        k = s+1 # k is the number of elements in the first interval\n        return k \n</code></pre>"},{"location":"Algorithms/Array_Partitioning/In-Place_Deduplication/#complexity","title":"Complexity","text":"<ul> <li><code>O(n) time complexity</code></li> <li><code>O(1) Space Complexity</code></li> </ul>"},{"location":"Algorithms/Array_Partitioning/In-Place_Deduplication/#in-place-deduplication-with-unique-elements-appearing-at-most-twice","title":"In-Place deduplication with Unique Elements appearing at most Twice","text":""},{"location":"Algorithms/Array_Partitioning/In-Place_Deduplication/#defining-the-problem_1","title":"Defining the Problem","text":"<p>Partition a sorted array s.t. the left partition contains at most two duplicates for each value. The relative order should be maintained. </p> <p>More formally, if there are k elements after removing the duplicates, then the first k elements of nums should hold the final result. It does not matter what you leave beyond the first k elements.</p> <p>This is similar to the first problem with a relaxed constraint that allows one duplicate in the left partition.(Leetcode Link)</p> Example <pre><code>Example 1:\n\nInput: nums = [1,1,1,2,2,3]\nOutput: 5, nums = [1,1,2,2,3,_]\nExplanation: Your function should return k = 5, with the first five elements of nums being 1, 1, 2, 2 and 3 respectively.\nIt does not matter what you leave beyond the returned k (hence they are underscores).\nExample 2:\n\nInput: nums = [0,0,1,1,1,1,2,3,3]\nOutput: 7, nums = [0,0,1,1,2,3,3,_,_]\nExplanation: Your function should return k = 7, with the first seven elements of nums being 0, 0, 1, 1, 2, 3 and 3 respectively.\nIt does not matter what you leave beyond the returned k (hence they are underscores).\n</code></pre>"},{"location":"Algorithms/Array_Partitioning/In-Place_Deduplication/#algorithm_1","title":"Algorithm","text":"<p>This problem is all about defining the invariant correctly. Let's dig into the solution.</p> <p>Key Ideas</p> <ul> <li>[0,s] - Is the inclusive interval containing no more than two duplicate</li> <li>(s,f) - Is the exclusive interval containing duplicates from [0,s]</li> </ul> <p>Initialize s=1 and f=2.</p> <p>s+1 is the write position for the next valid element. f scouts for the next valid element which can be placed at s+1.</p> <p>The key insight is to look two positions back to identify when we've seen more than two of the same element. Since the array is sorted, we can identify duplicates by comparing with the element at position s-1. We know that if arr[f] == arr[s-1] -&gt; arr[f] == arr[s] as well.</p> <p>If it's same as s-1, we've seen this element more than twice, therefore only f moves forward and s stays where it is.</p> <p>If it is different, then it is the next element which belongs at position s+1. Swap s+1 with f and move both s and f forward.</p> <p>Algorithm Steps:</p> <ol> <li>Handle edge case of array with length \u2264 1</li> <li>Initialize s=1, accepting first two elements by default</li> <li>For each position f starting from 2:</li> <li>If nums[f] \u2260 nums[s-1]:<ul> <li>Write nums[f] to position s+1</li> <li>Increment s</li> </ul> </li> <li>Return s+1 as the length of valid partition</li> </ol> <p>Warning</p> <p>#s,f= 1,1 # Cannot Initialize like this. f has to be s+1 initially for swapping to work correctly. If f=s, you will end up putting s ahead of f initially if f !=s-1, since we are swapping s+1 with f. Besides if (s,f) is an exclusive interval, f should be initialized to s+1.</p> <p>Initially,</p> <ul> <li>s,f = 1,2</li> <li>[0,1] - Has only two elements. First Invariant is Trivially True.</li> <li>(1,2) - Empty set. Second Invariant is Trivially True.</li> </ul> <p>Maintenance</p> <p>In the loop we will maintain the invariants. Here is the dry run: </p> <p>DRY RUN</p> <pre><code>[1,1,1,2,2,3] # everything before s including s is good\n   s f        # (s,f) empty,  f = s-1 . Only f moves ahead, After f moves ahead : (s,f) will contain the excess 1. 1 is a duplicate wrt to elements in [0,1]. This works because array is sorted. We know that if arr[f] == arr[s-1] -&gt; arr[f] == arr[s] as well..\n   s   f      # If we move f ahead now, it will contain 2 which is not a duplicate,swap s+1 with f and move both ahead to maintain both invariants \n[1,1,2,1,2,3] # swapped s+1 with f and s+=1, f+=1\n     s   f    # f!=s-1 : swap s+1 with f\n[1,1,2,2,1,3] #swapped s+1 with f and s+=1,f+=1\n       s   f  # f!=s-1  : swap f with s+1 \n[1,1,2,2,3,1] #swapped s+1 with f and s+=1,f+=1\n         s   f\n</code></pre> <p>Code</p> <pre><code>class Solution:\n    def removeDuplicates(self, nums: List[int]) -&gt; int:\n        #Edge case - nums has only one element\n        if len(nums) == 1 :\n            return 1\n\n        #Initialization: s=1 and f=2\n        s=1\n        for f  in range(2,len(nums)) :\n            #Maintenance\n            if nums[f] != nums[s-1] :\n                nums[s+1],nums[f] = nums[f],nums[s+1]\n                s+=1\n        return s+1\n</code></pre>"},{"location":"Algorithms/Array_Partitioning/In-Place_Deduplication/#complexity_1","title":"Complexity","text":"<ul> <li><code>O(n) time complexity</code></li> <li><code>O(1) Space Complexity</code></li> </ul>"},{"location":"Algorithms/Array_Partitioning/In-Place_Deduplication/#comparison-with-lomutos-partition","title":"Comparison with Lomuto's Partition","text":"<p>The deduplication algorithm looks awfully similar to Lomuto, but there are subtle and crucial differences.While both algorithms use two-pointer techniques, their invariants reveal fascinating differences in how they handle elements:</p>"},{"location":"Algorithms/Array_Partitioning/In-Place_Deduplication/#interval-types-and-their-significance","title":"Interval Types and Their Significance","text":"<p>A key distinction lies in how these algorithms define their intervals:</p> <p>Lomuto's Partition: Uses half-open intervals</p> <ul> <li><code>[lo,s)</code> \u2264 pivot: Excludes s</li> <li><code>[s,f)</code> &gt; pivot: Excludes f</li> <li>This makes sense because s acts as a boundary between two partitions</li> <li>Next valid element (\u2264 pivot) is written at position s</li> </ul> <p>Deduplication: Uses closed interval for primary invariant</p> <ul> <li><code>[0,s]</code>: Includes s</li> <li>This is necessary because s marks the last valid unique element</li> <li>The next unique element will be placed at s+1</li> </ul> <p>Deduplication needs s to be part of the valid region since it represents the last confirmed unique element</p>"},{"location":"Algorithms/Array_Partitioning/In-Place_Deduplication/#deduplication-as-a-lomuto-variant","title":"Deduplication as a Lomuto Variant","text":"<p>The deduplication algorithm can be viewed as a specialized variant of Lomuto's partition where:</p> <ol> <li> <p>Pivot Selection: Instead of a fixed pivot, we use a \"moving pivot\" (nums[s]) that changes as we progress</p> </li> <li> <p>Partition Criteria: Rather than comparing with a fixed value:</p> <ul> <li>Lomuto: Elements \u2264 pivot go left</li> <li>Deduplication: Elements \u2260 current value go to next position</li> </ul> </li> <li> <p>Pointer Movement: </p> <ul> <li>Both use s as a boundary of the \"accepted\" elements</li> <li>Both increment f to scan through remaining elements</li> <li>Both swap when their condition is met</li> </ul> </li> </ol> <p>The key insight is that deduplication adapts Lomuto's two-pointer partitioning strategy but:</p> <ul> <li>Uses equality comparison instead of less-than</li> <li>Updates its \"pivot\" value after each partition</li> <li>Requires input to be sorted </li> </ul> <p>This connection explains why both algorithms share similar pointer movement patterns despite serving different purposes.</p>"},{"location":"Algorithms/Array_Partitioning/Introduction/","title":"Introduction","text":"<p>Array partitioning is the problem of rearranging the elements of an array according to some predicate. </p> <p>We are interested here, firstly, in array partitioning schemes usually applied in quicksort, particularly Hoare's and Lomuto's partitioning schemes. These are two way partitioning schemes, which is to say that they have a binary predicate which could place elements in either left or right partitions based on comparison with a pivot value. There are different ways of implementing two way paritioning based on how you define the predicate. We will see this in some depth while understanding Hoare Partitioning.</p> <p>If we instead define a ternary predicate which puts an element in one of three partitions : left , middle or right based on comparison with a pivot value, it is called three way partitioning. All the duplicates are grouped into a middle partition.  Three way partitioning is also called the Dutch National Flag Problem, because the flag has three color bands. There are again different algorithms for doing this, one of which is Djikstra's, who was the one to name this as the Dutch National Flag problem. On the same pattern, the two way partitioning problem is sometimes called the Polish National Flag problem, because that flag has two color bands.</p> <p>Tip</p> <p>Once you understand Partitioning you get two other Algorithms for free : Quicksort and Quickselect.</p> <p>Paritioning Schemes are the actual meat of the Quicksort (and Quickselect)  algorithms. What makes Quicksort so interesting is that it can be implemented in slightly different ways with very different performance characteritics using any of these partitioning schemes. This is why Quicksort is not really a single algorithm but a family of algorithms and is a research area by itself in Computer Science.</p>"},{"location":"Algorithms/Array_Partitioning/Lomuto/","title":"Lomuto Partition","text":""},{"location":"Algorithms/Array_Partitioning/Lomuto/#defining-the-problem","title":"Defining the problem","text":"<p>Let us consider the simplest case, that of an array of integers.</p> <p>Given an array of integers, rearrange the elements such that the left part contains elements less than or equal to a pivot value and the right part contains values greater than the pivot value.</p>"},{"location":"Algorithms/Array_Partitioning/Lomuto/#the-algorithm","title":"The Algorithm","text":"<p>The Lomuto partition uses two forward iterators to maintains two invariants :</p> <p>[lo,s) contains only elements less than or equal to the pivot.  # Left closed Right open interval</p> <p>[s,f) contains elements larger than the pivot.      # Left closed right open interval</p> <p>Usually, pivot is fixed at hi. </p> <p>Initial conditions are :</p> <p>s, f = 0,0</p> <p>Both invariants are trivially true initially.</p> <p>During execution :</p> <p>Pointers</p> <pre><code>[---&lt;=pivot----|---&gt;pivot------|---Unexamined---|pivot] \nlo              s               f                    hi \n</code></pre> <p>s and f can be thought of as slow and fast pointers with s at the write position and f at the read position.</p> <p>The algorithm is </p> Code <pre><code>def partition(arr,lo,hi) :\n    s = lo\n    pivot = arr[hi]\n    for f in range(lo,hi+1) : \n         if arr[f] &lt;= pivot : \n              arr[s],arr[f] = arr[f],arr[s] #swap s and f\n              s+=1\n    return s-1      \n</code></pre> <p>Since [s,f] is a half closed interval, it is empty if  s and f are equal. If there is a gap, this interval is gauranteed to only contain elements greater than pivot, since a gap INCREASES as a result of f pointing to a value less than or equal to the pivot and s pointing to a value greater than pivot,  in the first place.</p> Dry Run <pre><code>General case :\n     [ 1,  4,  7,  3,  1,  4 ]\n0      sf                      # Initial condition, pivot = 4, arr[f] &lt;= 4 is True,  s will be swapped with f and both will move ahead\n1          sf                  # arr[f] &lt;= 4 , swap s with f and move both ahead\n2              sf              # arr[f] &lt;=4 is False, s stays where it is and only f moves ahead\n3              s  f            # arr[f] &lt;=4 is True, swap s with f and move both ahead\n4    [ 1,  4,  3,  7,  1,  4 ]\n                   s   f       # arr[f] &lt;=4 is True, swap s with f and move both ahead\n5    [ 1,  4,  3,  1,  7,  4 ] # arr[f] &lt;=4 is True, swap s with f and move both ahead\n                       s   f\n6    [ 1,  4,  3,  1,  4,  7 ] # arr[f] &lt;=4 is True, swap s with f and move both ahead\n                           s   # For loop ends\nReturn s -1\n\nOnly one element :\n  [4]\n0  sf      #arr[f] &lt;= f is True, swap s with f and move both ahead\n1    sf\nReturn s-1\n\nSorted array \n  [1,2,3]\n0  sf           \n1    sf\n2      sf\n3        s\n\nreturn s-1\n\nReverse sorted array :\n  [ 3, 2, 1 ]\n0   sf               #Initial conditions, arr[s] &lt;= 1 is False, only f will move ahead\n1   s  f             #arr[s] &lt;= 1 is False, only f will move ahead          \n2   s     f          #arr[s] &lt;= 1 is True, s and f will be swapped and both will move ahead \n3 [ 1, 2, 3 ]\n       s     f       #for loop ends\nreturn s -1 \n\nReverse sorted array with duplicate keys:\n[3, 2 ,1, 1]\n s     f\n[1, 2 ,3, 1]\n    s     f\n[1, 1 ,3, 2]\n       s     f   # for loop ends, \nreturn s-1 \n\nDuplicate keys :\n[1 , 1, 1, 1]\n             sf\nreturn s-1\n</code></pre>"},{"location":"Algorithms/Array_Partitioning/Lomuto/#variant-implementation","title":"Variant Implementation","text":"<p>We could redefine our problem statement slightly : Given an array of integers, rearrange the elements such that the left part contains elements less  a pivot value and the right part contains values greater than or equal to the pivot value.</p> <p>[lo,s) only contains elements strictly less than pivot [s,f)  contains elements equal to or greater than pivot.</p> <p>Initial conditions are :</p> <p>s, f = 0,0</p> <p>Both invariants are trivially true initially.</p> <p>During execution :</p> <p>Pointers</p> <pre><code>[---&lt;pivot----|---&gt;=pivot------|---Unexamined---|pivot] \nlo             s                f                    hi \n</code></pre> <p>The algorithm is </p> Code <pre><code>def partition(arr,lo,hi) :\n    s = lo\n    pivot = arr[hi]\n    # The range might as well be range(lo,hi+1). It would make no difference except one extra redundant comparsion.\n    for f in range(lo,hi) : \n         if arr[f] &lt; pivot : \n              arr[s],arr[f] = arr[f],arr[s] #swap s and f\n              s+=1\n    arr[s],arr[hi] = arr[hi],arr[s]\n    return s      \n</code></pre> <p>When for loop exits, s is positioned at the sorted position of pivot. We swap pivot with s and return s.</p> Dry Run <pre><code>[ 1,  4,  7,  3,  1,  4 ]\n      s       f             # arr[f] &lt; pivot is True, s and f will be swapped and both will move ahead.\n[ 1,  3,  7,  4,  1,  4 ]    \n          s       f\n[ 1,  3,  1,  4,  7,  4 ]\n              s       f\n[ 1,  3,  1,  4,  7,  4 ]\n                  s       f  # for loop ends\n[ 1,  3,  1,  4,  4,  7 ]\n                  s       f  # FInally Swap s with f\nReturn s \n\n\nReverse sorted array :\n  [ 3, 2, 1 ]\n    s        f \n  [ 1, 2, 3 ]\n    s        f\nreturn s\n\nReverse sorted array with duplicate keys:\n[3, 2 ,1, 1]\n s          f\nreturn s \n\nDuplicate keys :\n1  1  1  1 \ns          f #swap s[0] with s[-1]\nreturn s \n</code></pre> <p>Remember</p> <p>The two implementations behave differently when it comes to duplicate keys :</p> <p>The first implementation puts one instance of the pivot value in the LAST sorted position. The second implementation puts one instance of the pivot value in the FIRST sorted position.</p>"},{"location":"Algorithms/Array_Partitioning/Lomuto/#important-things-to-note","title":"Important things to note.","text":"<p>Tip</p> <p>With quickselect or quicksort, using Hoare partition with a weak condition is preferable to Lomuto.</p> <p>Lomuto partition has the same problem as Hoare partition with a strict condition when it somes to duplicate keys. It degenrates to O(n^2). Also, performs more swaps.</p> <p>Tip</p> <p>Lomuto is semi-stable. Some 'easy' interview style problems can be solved only if you know Lomuto and this property of the algorithm.</p> <p>Also, since it uses two forward iterators, it can be applied to singly linked lists.</p>"},{"location":"Algorithms/Array_Partitioning/Lomuto/#additional-resources","title":"Additional Resources","text":"<ol> <li>Sorting Lecture Notes</li> <li>Lomuto Partition Scheme</li> <li>Partition Notes by Stepanov</li> <li>Lomuto's Comeback</li> <li>Array Partition Blog</li> <li>Quick Sort Notes</li> </ol>"},{"location":"Algorithms/Backtracking/","title":"Backtracking","text":"<p>Backtracking seems confusing and difficult to learn, because there are so many different ways of doing the same thing.</p> <p>Here, we solve the same problem in many different ways only to see the possibilities.</p> <p>There is no staisfying and clear definition of backtracking I have found yet. I like to define it as an exhaustive search technique where you make a choice, explore that choice and then undo the choice (backtrack) to make another choice. </p> <p>This act of making a choice and undoing it is backtracking.</p> <p>It looks very much like a depth first tree traversal (or graph) traversal with state management.</p>"},{"location":"Algorithms/Backtracking/#problem-generate-n-bit-binary-numbers","title":"Problem: Generate N-bit Binary Numbers","text":"<p>Given an integer n, generate all possible binary numbers of length n and return them as a list of strings.</p>"},{"location":"Algorithms/Backtracking/#examples","title":"Examples:","text":"<pre><code>Input: n = 2\nOutput: [\"00\", \"01\", \"10\", \"11\"]\n\nInput: n = 3\nOutput: [\"000\", \"001\", \"010\", \"011\", \"100\", \"101\", \"110\", \"111\"]\n</code></pre>"},{"location":"Algorithms/Backtracking/#constraints","title":"Constraints:","text":"<ul> <li>1 \u2264 n \u2264 16</li> <li>The result list should be in lexicographically sorted order</li> <li>Each binary number in the output should be exactly n digits long (pad with leading zeros if necessary)</li> </ul>"},{"location":"Algorithms/Backtracking/#implementation-approaches","title":"Implementation Approaches","text":"<p>Approach 0: Pure Recursion</p> <p>Uses recursive decomposition without backtracking. Each recursive call returns its own result list. <pre><code>\"\"\"\nRecursive Decomposition- There is no need to maintain a path in this implementation. Therefore, no backtracking is required.\n\"\"\"\ndef generate(n) : \n    \"\"\"\n    Eg : For n = 3 , We have 3 spots each of which can be filled with 0 or 1.\n\n    -    -   -\n    1/0\n         1/0  \n             1/0\n\n    1 1 1\n    1 1 0\n    1 0 1\n    1 0 0\n    0 1 1\n    0 1 0\n    0 0 1\n    0 0 0 \n    \"\"\"\n    if n == 0 : \n        return [\"\"]\n    # Recursively generate the binary numbers for n-1 bits\n    # generate(n-1) returns a list of binary numbers of length n-1\n    # We will take each of these binary numbers and prepend '1' and '0' to each of them\n    l = [ \"1\" + _ for _ in  generate(n-1) ] \n    r = [ \"0\" + _ for _ in  generate(n-1) ]\n\n    print(f\"n = {n} , l = {l} , r = {r}\")\n    #Concatenate the two lists\n    return l + r  \n\nprint(generate(3))\n</code></pre></p> <p>This approach: - Uses pure recursive decomposition - Each call builds its own result independently - No need for global state or backtracking - More intuitive but less memory efficient - Time: O(2^n), Space: O(2^n) due to string copies</p> <p>Approach 1: Pre-allocated Array</p> <p>Uses a fixed-size array with direct index assignments. Most memory efficient for known size problems. <pre><code>\"\"\"\nBacktracking Implementation Using Pre-allocated Array Strategy\nKey Features:\n- Uses fixed-size array instead of growing/shrinking structure\n- No push/pop operations needed\n- Memory efficient as array size is known upfront\n- Implicit undoing of choices through overwriting\n\"\"\"\ndef generate(n) : \n    \"\"\"\n    Implementation Notes:\n    1. Pre-allocated array of size 'n' with None values\n    2. Each position directly overwritten with 0 or 1\n    3. No need for explicit undo operations\n    4. More efficient than stack-based approaches for known size problems\n\n    Memory Layout (n=3):\n    Initial: [None, None, None]\n    During:  ['0'/'1', '0'/'1', '0'/'1']\n\n    Time Complexity: O(2^n) - must generate all binary numbers\n    Space Complexity: O(n) - fixed size array, no additional growth\n    \"\"\"\n\n    # Pre-allocate array of size n - more efficient than growing/shrinking a list\n    path = [None] * n\n    res = []\n\n    #choose  0 or 1 at idx\n    def backtrack(idx) : \n        if idx == len(path): \n            res.append(\"\".join(path[:]))\n            return \n        # Direct assignment to index - no push/pop needed\n        path[idx] = '0'\n        backtrack(idx+1)\n        # No explicit undo needed - next assignment overwrites\n\n        path[idx] = '1'\n        backtrack(idx+1)\n        # No explicit undo needed at end - array will be reused\n\n    backtrack(0)\n    return res\n\nprint(generate(4))\n</code></pre></p> <p>Approach 2: Stack-based with Dual Recursion</p> <p>Uses explicit push/pop operations with two separate recursive calls for '0' and '1'. <pre><code>\"\"\"\nGenerate n-bit binary numbers using backtracking.\nImplementation Strategy: Stack-based Backtracking \n- Uses list as stack for building binary numbers\n- For each position, we make TWO recursive calls because:\n  1. First recursive call explores path after choosing '0'\n  2. Second recursive call explores path after choosing '1'\n- This creates a binary tree where each node represents a digit position\n  and has exactly two children (one for '0' and one for '1')\n\"\"\"\ndef generate(n) : \n    \"\"\"\n    Decision Tree Shows Two Branches at Each Level:\n    Root\n    \u251c\u2500\u2500 0 (First recursive call)\n    \u2502   \u251c\u2500\u2500 00\n    \u2502   \u2514\u2500\u2500 01\n    \u2514\u2500\u2500 1 (Second recursive call)\n        \u251c\u2500\u2500 10\n        \u2514\u2500\u2500 11\n\n    Two Recursive Calls Pattern:\n    1. First Call (with '0'): Explores all possibilities starting with '0'\n    2. Second Call (with '1'): Explores all possibilities starting with '1'\n\n    This ensures we generate ALL possible combinations systematically\n    \"\"\"\n\n    path = [] #We will treat this as a stack in this implementation\n    res = []\n\n    #choose  0 or 1 at idx\n    def backtrack(idx) : \n        if idx == n: \n            res.append(\"\".join(path[:]))\n            return \n\n        # First Recursive Branch: Try '0' at current position\n        path.append('0')\n        backtrack(idx+1)  # Explore all possibilities after choosing '0'\n        path.pop()        # Backtrack by removing '0'\n\n        # Second Recursive Branch: Try '1' at current position\n        path.append('1')\n        backtrack(idx+1)  # Explore all possibilities after choosing '1'\n        path.pop()        # Backtrack by removing '1'\n\n    backtrack(0)\n    return res\n\nprint(generate(3))\n</code></pre></p> <p>Approach 3: For-loop Choices</p> <p>Uses iteration over choices, making it more extensible for problems with multiple choices. <pre><code>\"\"\"\nGenerate n-bit binary numbers using backtracking with for-loop iteration.\nImplementation Strategy: For-loop Based Choice Selection\n- Uses pre-allocated array like version 2\n- Iterates over choices using for-loop instead of explicit recursive calls\n- More generalizable pattern that can handle multiple choices beyond just binary\n- Common pattern in backtracking problems where we iterate over available choices\n\"\"\"\ndef generate(n) : \n    \"\"\"\n    Key Differences from Other Versions:\n    1. Uses for-loop to iterate over choices ['0','1']\n    2. Single recursive call instead of multiple explicit ones\n    3. More extensible - easy to add more choices if needed\n    4. Common pattern seen in other backtracking problems like:\n       - Generating permutations (choices are remaining numbers)\n       - N-Queens (choices are valid positions)\n       - Sudoku (choices are valid digits)\n\n    Memory Layout:\n    - Pre-allocated array: [None] * n\n    - Choices handled by for-loop: for choice in ['0','1']\n    - Implicit undo through overwriting\n    \"\"\"\n\n    path = [None] * n\n    res = []\n\n    #choose  0 or 1 at idx\n    def backtrack(idx) : \n        if idx == len(path): \n            res.append(\"\".join(path[:]))\n            return \n        for choice in ['0','1'] : \n            path[idx] = choice\n            backtrack(idx+1)\n            #Here we dont care if path[idx] overwritten,\n            #Undo of choice is implicit\n\n    backtrack(0)\n    return res\n\nprint(generate(3))\n</code></pre></p> <p>Approach 4: Dynamic Stack with For-loop</p> <p>Combines dynamic stack operations with for-loop choice iteration for clearer state management. <pre><code>\"\"\"\nBacktracking with Dynamic Stack &amp; For-loop Choices\nKey Features:\n- Combines dynamic stack operations (like version 3) with for-loop choice iteration (like version 4)\n- Explicit undo operations through pop() (unlike version 2 &amp; 4's implicit overwrites)\n- More verbose but clearer state management\n- Shows complete choice-explore-undo cycle explicitly\n\"\"\"\ndef generate(n) : \n    \"\"\"\n    Implementation Characteristics:\n    1. Dynamic Growth: path grows/shrinks as we make/undo choices\n    2. Explicit State Management:\n       - Choice: path.append(choice)\n       - Explore: backtrack(idx+1)\n       - Undo: path.pop()\n    3. Uses for-loop for choices (more flexible than dual recursion)\n    4. Memory Usage: O(n) but with dynamic resizing overhead\n\n    Choice-Explore-Undo Pattern:\n    For each position:\n        For each choice (0,1):\n            1. Make choice (append)\n            2. Explore further (recurse)\n            3. Undo choice (pop)\n    \"\"\"\n\n    path = []  # We will treat this like a stack in this implementation\n    res = []\n\n    #choose  0 or 1 at idx\n    def backtrack(idx) : \n        if idx == n: \n            res.append(\"\".join(path[:]))\n            return \n        for choice in ['0','1'] : \n            # State Change: Add choice to current path\n            path.append(choice)\n            # Explore: Recurse with this choice in place\n            backtrack(idx+1)\n            # State Restoration: Remove choice before next iteration\n            path.pop()  # Explicit undo - different from array overwrite versions\n\n    backtrack(0)\n    return res\n\nprint(generate(3))\n</code></pre></p>"},{"location":"Algorithms/Backtracking/#comparison-of-approaches","title":"Comparison of Approaches","text":"Approach Memory Usage State Management Extensibility Code Clarity Pure Recursion O(2^n) None (functional) Limited Excellent Pre-allocated Array Most efficient (O(n) fixed) Implicit (overwrites) Limited Good Stack with Dual Recursion O(n) with resizing Explicit (push/pop) Limited Very Good For-loop Choices O(n) fixed Implicit (overwrites) Excellent Good Dynamic Stack with For-loop O(n) with resizing Most explicit Excellent Excellent <p>Key Takeaways</p> <ol> <li>Pure Recursion: Intuitive but least memory efficient. No bactracking involved.</li> <li>Pre-allocated Array: Best for memory efficiency when size is known</li> <li>Stack with Dual Recursion: Most intuitive for binary choices</li> <li>For-loop Choices: Most extensible for varying number of choices</li> <li>Dynamic Stack with For-loop: Best balance of clarity and flexibility</li> </ol>"},{"location":"Algorithms/Basic_Recursion/","title":"Basic Recursion","text":"<p>Recursion can be tricky to understand, especially when it comes to the order of operations. Here, we'll try to understand what happens when you \"do\" something before the recursive call (preorder position) or after the recursive call (postorder position). </p> <p>Debug this in your IDE and see what is happening on the stack and how function calls are stacked and unwound.</p>"},{"location":"Algorithms/Basic_Recursion/#understanding-recursive-positions","title":"Understanding Recursive Positions","text":"<ol> <li>Preorder Position: Operations performed before the recursive call</li> <li>Postorder Position: Operations performed after the recursive call</li> <li>Base Case: Condition to stop recursion</li> </ol>"},{"location":"Algorithms/Basic_Recursion/#implementation-approaches","title":"Implementation Approaches","text":"<p>Print i to 5 forwards and backwards</p> <p>Input is the starting point .Use preorder position to print in ascending order and postorder position to print in reverse order. Base Case is when i exceeds 5. <pre><code>\"\"\"\nPrint Numbers from i to 5.\n\"\"\"\n\ndef print_nums(i) :\n    if i &gt; 5 :\n        return\n\n    #preorder position\n    print(i)\n    #recursive call\n    print_nums(i+1)\n\n\"\"\"\nPrint Numbers from i to 5 in reverse order\n\"\"\"\n\ndef print_nums_backwards(i) :\n    if i &gt; 5 :\n        return\n\n    #recursive call\n    print_nums_backwards(i+1)\n    #postorder position\n    print(i)\n\n\nprint_nums(1)\nprint_nums_backwards(1)\n</code></pre> In preorder position, numbers are printed along with stack build up. In postorder, stack builds up before any print statement is executed and numbers are printed as the stack unwinds.</p> <p>Print numbers forward and backward in the range 1 to n.</p> <p>n is the input parameter to the function. Base case is when n becomes less than 1 i.e. equal to zero. Here, we have to print in postorder position to get the output in ascending order. We use preorder position to print in descending order. This is becuase we are taking the upper bound as input. Whereas, in the previous example, we took the lower bound as input.  <pre><code>\"\"\"\nPrint Numbers from 1 to n .\n\"\"\"\n\ndef print_nums_1_to_n(n) :\n    if n == 0 :\n        return\n    print_nums_1_to_n(n-1)\n    print(n)\n\n#print  1 to n\nprint_nums_1_to_n(5)\n\n\n\"\"\"\nPrint Numbers from n to 1 .\n\"\"\"\n\ndef print_nums_n_to_1(n) :\n    #base cases\n    if n == 0 :\n        return\n\n    print(n)\n    print_nums_n_to_1(n-1)\n\n\nprint_nums_n_to_1(5)\n</code></pre> The takeaway is that preorder position matches the input order and postorder position matches the reverse of the input order.</p> <p>Approach 3: Print Both Ways</p> <p>Uses both preorder and postorder positions to print numbers in both orders. <pre><code>\"\"\"\nPrint Numbers n to 1 and then 1 to n\n\"\"\" \ndef print_nums(n) :\n    if n == 0 :\n        print(\"Base Case Hit.\")\n        return\n    #preorder position\n    print(f\"Preorder : {n}\")\n    #recursive call\n    print_nums(n-1)\n    #postorder position\n    print(f\"Postorder : {n}\")\n\n\nprint_nums(5)\n</code></pre></p>"},{"location":"Algorithms/Basic_Recursion/#comparison-of-approaches","title":"Comparison of Approaches","text":"Position When Operation Happens Stack State Output Order Preorder Before recursive call During stack building Same as Inout Order Postorder After recursive call During stack unwinding Reverse of Input Order Both Before and after Both phases Both orders <p>Key Takeaways</p> <ol> <li>Operation Placement: The position of operations relative to the recursive call determines the order of execution</li> <li>Stack Behavior:<ul> <li>Stack builds up: Follows input order (n \u2192 n-1 \u2192 n-2 \u2192 ... \u2192 1)</li> <li>Stack unwinds: Reverse of input order (1 \u2192 2 \u2192 ... \u2192 n-1 \u2192 n)</li> </ul> </li> <li>Memory Usage: All approaches use O(n) stack space</li> <li>Choose Based On:<ul> <li>Same as input order \u2192 Use preorder position</li> <li>Reverse of input order \u2192 Use postorder position</li> <li>Both orders \u2192 Use both positions</li> </ul> </li> </ol>"},{"location":"Algorithms/Binary_search/Introduction/","title":"Introduction","text":"<p>Note</p> <p>This article is heavily based on this awesome video by mcoding. This is perhaps the best resource on the internet to understand binary search.</p>"},{"location":"Algorithms/Binary_search/Introduction/#bisect-left","title":"Bisect Left","text":"<p>Binary search (also called bisection search) is an algorithm to find an element quickly in a sorted array.</p> <p>Example</p> <p>Suppose we are trying to find 7 in this array : <code>[2,3,3,4,6,7,8,9]</code></p> <ol> <li> <p>Is the element you want to find in the left half or the right half? \u2192 Right half</p> <p>Our search space is now only the right half.  <pre><code>[        6,7,8,9 ] # remaining 4\n</code></pre> 2. Repeat the process <pre><code>[        6,7     ] # remaining 2\n[          7     ] # remaining 1\n</code></pre> 3. Finally there is just one element left. That's our element.</p> </li> </ol> <p>At each step you cut out half of the search space. How can the idea be so simple, but the implementation be so difficult?. It's not - you just have to think about it in the right way.</p> <p>The First Decision: What Do You Want to Return? The object itself, or the index where it is at?</p> <ul> <li>The index itself is a \"more useful piece of information\". You can always just grab the object at that index later.</li> </ul> <p>What do we do when the object we are looking for is not there?</p> <ul> <li>Return -1? Raise exception? </li> <li>What if there are multiple valid indices to choose from? </li> </ul> <p>With every arbitrary choice we make, we would have to remember the implementation details for each choice.</p> <p>Implementation Complexity</p> <p>A better way to do it, is to rephrase the question, so that there is always exactly one answer.If I were going to add another element to the array, say 7, where should I put it so that it is the first 7?</p> <ul> <li>If there are no 7s in the array, this answer would be the index where you would put 7 to maintain the sorted property of the array. </li> <li>If there is at least one 7, the answer would be the index of the first 7.</li> <li>The index would be zero if the array is empty, which is not actually a valid index, but is the right answer to our Question!</li> <li>It could also be <code>(n-1) + 1</code> if the element is absent from the array and greater than all elements in a non-empty array. This is also not a valid index. But, again,it is the right answer to our Question.</li> </ul> <p>Correct Implementation Strategy</p> <p>The array being sorted is not actually the property being used in binary search. The property is that everything greater than 7 is to the right of 7 and everything less than 7 is to the left of 7.</p> <p>Here, I disturbed the order of some elements in a sorted array: <code>[2, 3, 5, 4, 6, 7, 9, 8]</code></p> <p>The array is no longer in sorted order, but notice the steps are exactly the same. What we need is that everything less than 7 is to the left of 7 and everything greater than 7 should be to the right. </p> <p>Important Insight</p> <p>In fact, the numbers can be replaced with the important information: Is this number less than 7?</p> <p>Pointers</p> <pre><code>[2, 3, 5, 4, 6, 7, 9, 8]\n T  T  T  T  T  F  F  F  #&lt;7?\n</code></pre> <p>Notice how any Trues are on the left and any Falses are on the right.</p> <p>Now that we are looking at the relevant data, where do we insert 7 so that it is the first 7?</p> <p>Notice that the answer we are looking for is the index of the first False value or the end of the array if there are no false values. This is the key insight, we need to keep in mind that makes the implementation extremely easy to remember :</p> <p>Tip</p> <p>Squint Your Eyes and Find the First False Value</p> <p>Ok, let's write the implementation:</p> <p>Bisect Left</p> <pre><code>def bisect(arr, x):\n    lo = 0\n    hi = len(arr)     \n    while lo &lt; hi:\n        mid = (lo + hi) // 2\n        if arr[mid] &lt; x:  # mid is a True value\n            lo = mid + 1\n        else:            # mid is a False Value\n            hi = mid\n    return lo\n</code></pre> <p>The first two lines inside the function, initialize lo and hi.</p> <p>Tip</p> <p>lo and hi are lower and upper bounds on where this First FALSE index is. </p> <p>If our answer is zero, it would mean that first value is False or array is empty. If it is len(arr), it would mean that all the values are True and we would have to put the new element at the end of the array.</p> <p>The idea of the while loop (line 4), is that we continuously decrease the upper bound and increase the upper bound until they are equal. lo and hi will meet at the unique index we are looking for. So, we could return lo or high, once the while loop runs, it would make no difference.</p> <p>Next, we calculate the midpoint in each iteration of the while loop. Python can represent arbitrarily large integers, so you do not have to worry about overflow. But, if you were using another language, you might have to worry about overflow. In that case, you would represent mid as <code>lo+(hi-lo)//2</code> : lo plus half the distance between hi and lo. If you do it this way, it will never overflow.</p> <p>Comparing mid with x</p> <p>Next comes the part which is often written incorrectly. We must compare whatever is at mid with our search term <code>x</code>. Remember, we are looking for the first False value where True of False is determined by the question : Is this element strictly Less than <code>x</code>:  <pre><code>if arr[mid] &lt; x :\n    lo = mid+1   # mid is a True Value\nelse :\n    hi =mid      # mid is a False Value\n</code></pre> If <code>mid</code> is strictly less than <code>x</code>, it is a <code>True</code> Value. Since the Value at <code>mid</code> is <code>True</code>, the earliest a <code>False</code> Value could occur is the next index which is <code>mid+1</code>. Therefore, our lower bound becomes <code>mid+1</code></p> <p>What if <code>mid</code> is a <code>False</code> Value? In this case the FIRST <code>False</code> Value could not be any later than mid. But it could be mid or earlier than mid. There could be other <code>False</code> values before midm. So our upper bound becomes mid.</p> <p>When the while loop is done, lo and hi meet at the index we are looking for.</p> <p>How do we know that this implementation always gives the right answer?  - We know that lo is always &lt;=  answer  - We also know that answer is always &lt;= hi  - In each iteration the difference between hi and lo always decreases by at least one.</p> <p>Thereifre, lo and hi always meet at the answer.</p> <p>But how do we know that in each iteration lo and hi come closer to each other by at least one? If we enter the True clause on line no. 6 <code>lo = mid+1</code> ensures that lo always increases by at least one.</p> <p>If we enter the False clause on lines 8,9 we are setting hi to mid. So, hopefully we are lowering hi. But how do we know mid is actually strictly less than hi?</p> <p>Here we are actually using the property that integer division rounds down. Inside the while loop: while lo &lt; hi, lo is strictly less than hi. So lo + hi is strictly less than hi + hi.</p> <p>That means for calculating mid: mid = (lo + hi) // 2</p> <p>When we divide (lo + hi) by 2 and round down we will get something strictly less than (hi + hi) // 2 i.e. hi.</p>"},{"location":"Algorithms/Binary_search/Introduction/#checking-the-return-value-of-bisect-left","title":"Checking the return value of Bisect Left","text":"<p>Since bisect left can return an out of bounds index and not necessarily the index of a matching value, you have to handle this when using it, depending on what you are trying to do.  </p> <p>Checking if x exists using bisect left</p> <pre><code>def search(arr:list, x) -&gt;int :\n    idx = bisect_left(arr,x)\n    if i != len(arr) and arr[i] == x:\n        return i\n    raise ValueError(f\"{x} not found!\")\n</code></pre>"},{"location":"Algorithms/Binary_search/Introduction/#bisect-right","title":"Bisect Right","text":"<p>But wait. Your ask: instead of returning a 7 so that it is the first seven, could I not return the index, so that it would be the last 7?</p> <p>You could. You could follow the same analysis and get a very similar algorithm.</p> <p>To find where it should go to be the first 7, the relevant piece of information is: \"Is this (current element) strictly less than 7\"</p> <p>To find where it should go to be the last element, the relevant piece of information i: \"Is this (current element) less than or equal to  7\"</p> <p>Bisect Left vs Bisect Right</p> <pre><code>[2, 3, 5, 4, 6, 7, 7, 7, 8, 9]\n[T, T, T, T, T, F, F, F, F, F] #&lt;7  ; Bisect Left\n[T, T, T, T, T, T, T, T, F, F] #&lt;=7 ; Bisect Right\n</code></pre> <p>In either case, you are looking for the first False value. You just need to decide what you mean by True or False.</p> <p>Bisect Right</p> <pre><code>def bisect_right(arr, x):\n    lo = 0\n    hi = len(arr)     \n    while lo &lt; hi:\n        mid = (lo + hi) // 2\n        if arr[mid] &lt;= x:  # mid is a True value\n            lo = mid + 1\n        else:            # mid is a False Value\n            hi = mid\n    return lo\n</code></pre>"},{"location":"Algorithms/Binary_search/Introduction/#checking-the-return-value-of-bisect-right","title":"Checking the return value of Bisect Right","text":"<p>Bisect right can return values from 0 to len(arr). If it returns zero it means that either arr is empty or all elements in the array are greater than x. Any other value could mean x exists at i-1. A return value of len(arr) means x is greater than or equal to all elements in the array.</p> <p>Checking if x exists using bisect right</p> <pre><code>def search(arr:list, x) -&gt;int :\n    idx = bisect_right(arr,x)\n    if i &gt; 0 and arr[i-1] == x:\n        return i\n    raise ValueError(f\"{x} not found!\")\n</code></pre>"},{"location":"Algorithms/Binary_search/Introduction/#summary","title":"Summary","text":"<p>Key Takeaways</p> <ul> <li>Binary search is fundamentally about finding boundaries in bisected data</li> <li>Reframe the problem as \"find the first False value\" </li> <li>The difference between <code>bisect_left</code> and <code>bisect_right</code> is just changing <code>&lt;</code> to <code>\u2264</code></li> <li>Always think about what \"True\" and \"False\" mean in your specific context</li> </ul>"},{"location":"Algorithms/Binary_search/Median_of_two_sorted_arrays/","title":"Median of two sorted arrays","text":"<p>This is one of those problems which I like to call an \"Algorithm in itself\". Although categorized under Binary Search, its not just a straightforward application of it.</p> <p>Description</p> <p>Leetcode Link </p> <pre><code>Given two sorted arrays nums1 and nums2 of size m and n respectively, return the median of the two sorted arrays.\n\nThe overall run time complexity should be O(log (m+n)).\n\n\n\nExample 1:\n\nInput: nums1 = [1,3], nums2 = [2]\nOutput: 2.00000\nExplanation: merged array = [1,2,3] and median is 2.\nExample 2:\n\nInput: nums1 = [1,2], nums2 = [3,4]\nOutput: 2.50000\nExplanation: merged array = [1,2,3,4] and median is (2 + 3) / 2 = 2.5.\n\n\nConstraints:\n\nnums1.length == m\nnums2.length == n\n0 &lt;= m &lt;= 1000\n0 &lt;= n &lt;= 1000\n1 &lt;= m + n &lt;= 2000\n-106 &lt;= nums1[i], nums2[i] &lt;= 106\n</code></pre> <p>Let us figure out how binary search can be applied to solve this problem..</p>"},{"location":"Algorithms/Binary_search/Median_of_two_sorted_arrays/#what-is-the-data","title":"What is the data?","text":"<p>nums1 = [1,2], nums2 = [3,4]</p> <p>Two sorted arrays.</p>"},{"location":"Algorithms/Binary_search/Median_of_two_sorted_arrays/#what-is-being-asked","title":"What is being asked?","text":"<p>To find the median in logarithmic time.</p>"},{"location":"Algorithms/Binary_search/Median_of_two_sorted_arrays/#thoughts","title":"Thoughts","text":"<p>What is a median</p> <p>Consider : <pre><code>[1 2 |3 4]  #right partition is inclusive \n</code></pre> If we partition an even lengthed sorted array such that the left and right partitions are of equal size.</p> <p>The median is ( max(left partition) + min(right partition)  ) / 2</p> <p>If the array is odd lengthed : <pre><code>[1 2 3 4 5]\n     *\n</code></pre> We can consider the middle element to belong to both partitions. In which case <pre><code>[1 2 3 4 5]\n     *\n</code></pre></p> <p>The middle element IS the median i.e. index len(arr)//2</p> <p>Provided A and B are both sorted, We can always find partitions on A and B such that elements to the left of both partitions are less than elements to their right and both partitions combined divide the combined arrays in half. It will be clear what is meant from the examples below (Consider left out of bounds position to be -inf and right out of bounds position to be +inf). </p> <p>Example 1 : Valid Partitions</p> <pre><code>Example 1: \nB [ 1, 2, |6, 7]  # right pertition is inclusive\nA  [  5, |10  ]\n\n1,2,5  &lt; 6,7,10\n</code></pre> <p>Example 2 : Valid Partitions</p> <pre><code>Example 2 :\nB [ |5, 6, 7, 8, 9 ] # consider left out of bounds position to be -inf and right out of bounds position to be +inf\n\nA [ 1, 2, 3, 4| ]\n\n1,2,3,4 &lt; 5,6,7,8,9\n</code></pre> <p>Note that if you pick a partition on array A, there is only one corresponding partition on array B because we are looking for the set of partitions which divide the combined arrays in two halves.</p> <p>We can tell a set of partitions COULD BE a valid by verifying that the last element of the left partition of one array is less than or equal to the first element of the other arrays right partition. We can check the inverse to confirm that the partitions are in fact not valid. </p> <p>We can in fact use binary search to find the correct partitions. All we need is to know which way should we move one of the partitions. We will go into this in more detail. </p> <p>The idea is if we find the correct partitions, we can calculate the median by looking at the elements clustered around both partitions. Let's see how..</p> <p>The first question is how should we define the partitions?</p> <p>partition_a + partition_b = (m+n+1) //2 OR partition_a + partition_b = (m+n) // 2 </p> <p>let len(A) =m and len(B) = n </p> <p>FOR ODD LENGTHED ARRAY :</p> <pre><code>B  [1]   ; #IFF partition_b = (m+n +1 )//2 - partition_a  = 2//2 -0 = 1 But *IFF partition_b =  (m+n)//2 - partition_a =  1//2 -0 = 0-0 = 0 \n   *0 #1  \nA  [ ]   ; partition_a = (0+ 0)//2 =0 \n    0\n</code></pre> <p>If we say that partition_x + partition_y = (m+n)//2 then median is in the right partition in case of odd lengthed array.</p> <p>If we say that partition_x + partition_y = (m+n+1)//2 then median is in the left partition in case of odd lenghted array.</p> <p>It makes no difference for arrays where m+n is even. <pre><code>B [1 2]   ; #IFF partition_b = (m+n+1)//2 - partition_a  = 5//2 - 1 = 2- 1 = 1 and *IFF partition_b = (m+n)//2 - partition_a = 4//2 -1 = 1\n    #*1\nA [1 2]   ; partition_a = (0 + 2) // 2 = 1\n</code></pre></p> <p>Lets pick one convention : </p> <p>partition_a + partition_b = (m+n) //2</p> <p>Now for the binary search:</p> <p>It makes sense to binary search on the smaller array. Lets call this array A and the partition on it,  partition_a.</p> <p>Binary Search Logic</p> <p>In each iteration of the binary search, we consider mid as partition_a. For each partition_a we determine partition_b (as per our definition) and ask is left_b &gt; partition_a where left_b is the last element in the left partition of array B. </p> <ul> <li>If this is TRUE, then partition_a cannot be the pivot. Now the question is where do we move partition_a: to the left or to the right? We know that left_b is too big right now, so it needs to be made SMALLER i.e. Moved left. For doing this, by definition of our partitions, we NEED TO MOVE partition_a to the right i.e. partition_a need to be made larger. This is like increasing our lower bound.</li> <li>If this is FALSE, it means that the current set of partitions COULD BE VALID. Therefore, we set hi = mid. That is, decrease our upper bound.</li> </ul> <p>Dry Run</p> <pre><code>INIT:         \nB [ 1  3  4  5   6  ]\nA  [ 2  6  7   8   ] \n     lo          hi\n\nITER 1:\nA [1  3  4  5  6  ]\n         *            #left_b is 3.\nB  [ 2 6  7  8 ]  \n     lo   *            #3 &gt; 7 is F; this could be the right partition_a ; hi = mid\n         F/hi\n\nITER 2 :\nA [1  3  4  5  6  ]\n            *\nB  [ 2  6 7  8 ]\n   lo   *           #4&gt; 6 is F  ; this could be partition_a hi = mid            \n       F/hi\n\nITER 3 :\nA [1  3  4  5  6  ]\n               *\nB  [ 2    6  7  8 ]\n     *              #5&gt;2 T; This cannot be the right partition_a ;\n     T/lo hi        #lo = mid+1  ; Now lo == hi; lo and hi have converged and  loop exits in next iteration\n\nITER 5 :\nB [2   6   7   8]  # lo = hi = 1 and while loop exits\n       *\n       F/lo,hi\n</code></pre> <p>tl;dr</p> <p>Binary search will converge on the first false value. <pre><code>   [1    3    4     |5    6  ]\n     [2     |6    7    8]\n      T     F    F    F\n</code></pre></p> <p>In our binary search we need a function move_right which simply returns left_b &gt; right_a</p> <p>Thats it. Once , we have the right partition, calculate median:</p> <ul> <li> <p>If (m+n) is odd : return min(right_a, right_b)</p> </li> <li> <p>If (m+n) is even : return (max(left_a, left_b) + min(right_a, right_b)) /2 </p> </li> </ul>"},{"location":"Algorithms/Binary_search/Median_of_two_sorted_arrays/#code","title":"Code","text":"Code <pre><code>class Solution:\n    def findMedianSortedArrays(self, nums1: List[int], nums2: List[int]) -&gt; float:\n        # A is always the smaller array\n        A, B = nums1, nums2 \n        if len(A) &gt; len(B) :\n            A , B = B, A\n\n        #measure their lengths    \n        m,n = len(A), len(B)\n\n        def find_partition_b(partition_a) : \n            return (m+n)//2 - partition_a\n\n        def get_left(partition,array) :\n            return float('-inf') if (partition-1 )&lt;0 else array[partition-1]\n\n        def get_right(partition,array) :\n            return float('inf') if (partition)&gt;= len(array) else array[partition]\n\n        def move_right(partition_a) : \n            partition_b = find_partition_b(partition_a)\n            left_b = get_left(partition_b,B)\n            right_a = get_right(partition_a,A)\n            return left_b &gt; right_a\n\n        #binary search to find the correct partition_a\n        lo, hi  = 0 , len(A)\n        while lo &lt; hi :\n            partition_a = (lo +hi) // 2\n            if move_right(partition_a) :\n                lo = partition_a+1\n            else :\n                hi = partition_a\n\n        #lo and hi have converged on the correct partition_a \n        partition_a,partition_b   = lo, find_partition_b(lo)\n        right_a = get_right(partition_a,A)\n        right_b = get_right(partition_b,B)\n\n        #combined array length is odd\n        if (m+n) % 2 == 1 :\n            return min(right_a,right_b)\n\n        #combined array length is even    \n        left_a = get_left(partition_a,A)\n        left_b = get_left(partition_b,B)\n        return ( max(left_a,left_b) + min(right_a,right_b) ) / 2        \n</code></pre>"},{"location":"Algorithms/K_Way_Merge/","title":"The k-way Merge problem","text":""},{"location":"Algorithms/K_Way_Merge/#merging-k-sorted-arrays","title":"Merging k sorted arrays","text":"<p>We can think of the k-way merge problem as a generalization of the 2-way merge problem. That is given, k sorted containers, return a merged sorted container.</p> <p>In the 2-way merge problem , we had a binary choice when picking the next value to write to the merged container.  Now we have a k-way choice among k or less elements at each step. We basically need a good way to pick the smallest value from among all possible choices. This is what a heap does. Since we are considering containers sorted in a non decreasing order, a Min Heap is what we need to dynamically pick the smallest value at each step.</p> <p>The crux of the algorithm is :</p> <p>Algorithm</p> <pre><code>initialize a Min heap of all possible choices. \nWhile there is something in the heap : \n   Pop the heap top and write it to the merged container.\n   Get the element next to popped element in the container to which it belongs (If it exists). Put it in the heap.\n</code></pre> <p>Thats it.</p> <p>The code should actually simpler than handling the 2-way merge because there are no if else conditions to handle unequal lengthed arrays.</p> Code K Way Merge on Arrays <pre><code>\"\"\"\nThe same problem but with a different signature :\nhttps://www.geeksforgeeks.org/problems/merge-k-sorted-arrays/1\n\"\"\"\n#User function Template for python3\nfrom typing import List\nfrom heapq import heapify, heappop, heappush\nclass Solution:\n    #Function to merge k sorted arrays.\n    def mergeKArrays(self, arr : List[List[int]]) -&gt; List[int]:\n        \"\"\"\n        Given k sorted arrays, return a single merged array.\n        Input :\n         arr : a list of integer arrays\n        \"\"\"\n        # return merged list\n        len_res = 0\n\n        #Priority Queue- min heap\n        pq = []\n        #Assume that arrays can be of unequal length\n        #Initialize min heap\n        for array in arr :\n            len_res += len(array)\n            #Store the current index and a reference to the array for getting the next value in the array\n            pq.append((array[0] , 0 , array))\n\n        #heapify pq\n        heapify(pq)\n\n        #allocate result array\n        res = [None] * len_res\n\n        #write pointer\n        write_ptr = 0 \n        #Pick the lowest value from the current choices and write it to res\n        while pq :\n            curr_val, curr_ptr, curr_arr = heappop(pq)\n            res[write_ptr] = curr_val\n            if curr_ptr &lt; len(curr_arr) -1  : \n                heappush(pq, (curr_arr[curr_ptr+1] , curr_ptr+1,curr_arr ))\n            write_ptr+=1\n        return res\n\nif __name__ == \"__main__\" :\n    print( Solution().mergeKArrays([[1,2,3] , [5,6] , [9,9,10]]) )\n</code></pre>"},{"location":"Algorithms/K_Way_Merge/#merging-k-sorted-linked-lists","title":"Merging k sorted linked lists","text":"<p>The same thing for linked lists. </p> Code K Way Merge on Arrays <pre><code>from heapq import heapify,heappush,heappop\n#Definition for singly-linked list.\nclass ListNode:\n    def __init__(self, val=0, next=None):\n        self.val = val\n        self.next = next\nclass Solution:\n    def mergeKLists(self, lists: List[Optional[ListNode]]) -&gt; Optional[ListNode]:\n        pq=[]\n        for idx, head in enumerate(lists) : \n            if not head : continue\n            #Put idx between the value and the node object  to avoid comparing objects in case of a tie \n            #Which will lead to this error :'&lt;' not supported between instances of 'ListNode' and 'ListNode'\n            heappush(pq,(head.val,idx, head)) \n\n        #Allocate Write container and write pointer\n        dummy = ListNode(0)\n        write_ptr = dummy\n\n        while pq :\n            #Pick the lowest valued node from available choices.\n            _,idx,curr_node = heappop(pq)\n\n            #write to write container and move write pointer ahead\n            write_ptr.next = curr_node\n            write_ptr = write_ptr.next\n\n            #For curr_node, put its next node in the heap  \n            nxt_node = curr_node.next \n            if nxt_node : heappush(pq,(nxt_node.val, idx, nxt_node ))\n        return dummy.next\n</code></pre>"},{"location":"Algorithms/K_Way_Merge/#practice","title":"Practice","text":"Kth Smallest Element in a Sorted Matrix <pre><code>\"\"\"\nSolution for https://leetcode.com/problems/kth-smallest-element-in-a-sorted-matrix/\n\nThis can be solved using k-way merge. Instead of putting the next smaller value in a resultant merged array, \nsimply put it in a max heap of size k.\n\nExample :\n[\n[1,5,9],\n       *\n[10,11,13],\n    *\n[12,13,15]\n *\n]\n\nsay , k = 4\nCan we use A Max heap to maintain the k largest numbers?\n[10,9,5,1]\n\nWhen size becomes greater than k pop from the max heap:\n\n&gt;[11,10,9,5,1]\n&gt;size is greater than 5 \n&gt;pop from max heap\n[10,9,5,1]\n\nMemory required is : max heap of size k to  keep track of k smallest numbers\nA min heap of size len(matrix) (equal to the number of rows in matrix) to pick the next smallest number\n\nAlgorithm :\n\n1) Initialize a min heap to pick next smallest number from all rows of matrix\n2) While min heap is not empty:\n    -  Pop from min heap and put in a max heap of size k\n    -  Put element next to popped element into min heap  \n3) Return the top of max heap.\n\n\"\"\"\nfrom heapq import heapify, heappush , heappop\nclass Solution:\n    def kthSmallest(self, matrix: List[List[int]], k: int) -&gt; int:\n        pq = []\n        for row in matrix : \n            e = (row[0], 0, row)  # tuple of Curr_Value, curr_index, row reference\n            pq.append(e)\n        heapify(pq)\n\n        max_hp = []\n        while pq :\n            #pick next smallest number\n            val, idx, row = pq.pop()\n            #Simulate max heap by negating values\n            heappush(max_hp,-val)\n            if len(max_hp) &gt; k :\n                heappop(max_hp)\n            if idx &lt; len(row) -1 :\n                heappush(pq,(row[idx+1],idx+1,row))\n        # Retun top of max heap\n        return -heappop(max_hp) \n</code></pre> Find K Pairs with Smallest Sums <pre><code>\"\"\"\nSolution for : https://leetcode.com/problems/find-k-pairs-with-smallest-sums/\n\nThe problem statement is confusing. Key informations is that all possible pairs are formed by \ntaking one number from first array and combining it with a second number from the second array.\n        [1,7,11]\n        [2,4,6]\n [ (1,2) , (1,4) (1,6) ,(7,2) , (7,4) , (7,6) , (11,1), (11,2) , (11,7) ]\n\nThe problem is asking us , if we sort the above list of tuples by their sum, what would be the first k tuples.\n\nThoughts :\n\nDo we have to enumerate all pairs? Maybe. If we do that, we can maintain a max heap fo size k. \nIn the max heap, keep tuples ( (sum of two elements), (el1, el2))\n\nWe can do this :\n\nfrom heapq import heappop, heappush, heapify \nclass Solution:\n    def kSmallestPairs(self, nums1: List[int], nums2: List[int], k: int) -&gt; List[List[int]]:\n        res = []\n        ## Quadratic Time Complexity.\n        for n1 in nums1 :\n            for n2 in nums2 :\n                heappush(res,(-1*(n1+n2),(n1,n2)))\n                if len(res) &gt; k :\n                    heappop(res)\n\n        ans = []\n        while res : \n            ans.append(heappop(res)[1])\n        return ans\n\nBut this has a  quadratic run time and will give TLE.\n\nCan we do better ? \nWhy yes we can!\n\n\nConsider :\n[1,1,2], nums2 = [1,2,3], k = 3\n\nIsn't this how we are enumerating pairs?\n1,1 -&gt; 1,2 -&gt; 1,3\n*\n1,1 -&gt; 1,2 -&gt; 1,3\n*\n2,1 -&gt; 2,2 -&gt; 2,3\n* \n\nThese look an awful lot like three linked lists. \nWe know how to get the next element for each linked list. \nTo get the n smallest pairs we just need to run a k-way merge n times. \n(Lets call the number of pairs required n to avoid confusion with the concept of k-way merge.\nIn k-way merge, k refers to the number of linked lists we are merging )\n\nLets implement the solution. \n\"\"\"\n\nfrom heapq import heappop,heappush\n\nclass Solution:\n    def kSmallestPairs(self, nums1: List[int], nums2: List[int], k: int) -&gt; List[List[int]]:\n        n = k # rename k to n to avoid confusion with out mental model\n        pq = []\n        #Init pq \n        #Each linked list Node is a tuple of two elements.\n        #In each tuple the first element is fixed and from the first array. We take second element from the second array.\n        #We will use the index of the first element as tie breaker\n        #Use index of second element to get the next element in the same linked list\n\n        idx_2 = 0 \n        for idx_1, val in enumerate(nums1) :\n            heappush(pq,( ( nums1[idx_1] + nums2[0] ,idx_1, idx_2) \n            ))#  (sum of two elements, index of first element, index of second element )   \n\n        res = []\n\n        # Pick the next smallest tuple k times\n        while pq and n :\n            n-=1\n            _,idx_1,idx_2 = heappop(pq) # idx_2 is the index of element2 in the tuple\n            res.append((nums1[idx_1],nums2[idx_2])\n            )\n            #Put the element next to popped element in the min heap\n            if idx_2 &lt; len(nums2)-1 :\n                heappush(pq,( ( nums1[idx_1] + nums2[idx_2+1] ,idx_1, idx_2+1) )\n                 ) #  (sum of two elements, index of first element, index of second element) \n        return res\n</code></pre>"},{"location":"Algorithms/Math/Division_Algorithm/","title":"The Division Algorithm","text":"<p>LibreTexts- Mathematics</p> <p>Given any integers \\(a\\) and \\(b\\), where \\(a &gt; 0\\), there exist integers \\(q\\) and \\(r\\) such that</p> <p>\\(b = aq + r\\)</p> <p>where \\(0 \\leq r &lt; a\\). Furthermore, \\(q\\) and \\(r\\) are uniquely determined by \\(a\\) and \\(b\\).</p> <p>The integers \\(b\\), \\(a\\), \\(q\\), and \\(r\\) are called the dividend, divisor, quotient, and remainder, respectively. Notice that \\(b\\) is a multiple of \\(a\\) if and only if \\(r = 0\\).</p> <ul> <li>b - Dividend</li> <li>a - Divisor</li> <li>q - Quotient</li> <li>r - Remainnder </li> </ul>"},{"location":"Algorithms/Math/Division_Algorithm/#in-python","title":"In Python","text":"<p>divmod(dividend,divisor) -&gt; (quotient, remainder)</p> <p>divmod(b,a) -&gt; (b//a,b%a)</p> <p>Example</p> <pre><code>&gt;&gt;&gt;divmod(1,10)\n&gt;&gt;&gt;(0, 1)\n</code></pre> <p>Example</p> <pre><code>&gt;&gt;&gt; divmod(4,2)\n(2, 0)\n&gt;&gt;&gt; 4 // 2\n2\n&gt;&gt;&gt; 4 % 2\n0\n</code></pre>"},{"location":"Algorithms/Math/Division_Algorithm/#python-rounds-negative-nymbers-away-from-zero","title":"Python Rounds Negative Nymbers away from zero","text":"<p>In python :</p> <p>15 / -4  = - 3.75</p> <p>But,</p> <p>14// - 4 =  - 4</p> <p>To round towards zero, do floating point division and convert to int.</p> <pre><code>Do floating point division then convert to an int. No extra modules needed.\n\nPython 3:\n\n&gt;&gt;&gt; int(-1 / 2)\n0\n&gt;&gt;&gt; int(-3 / 2)\n-1\n&gt;&gt;&gt; int(1 / 2)\n0\n&gt;&gt;&gt; int(3 / 2)\n1\n\nPython 2:\n\n&gt;&gt;&gt; int(float(-1) / 2)\n0\n&gt;&gt;&gt; int(float(-3) / 2)\n-1\n&gt;&gt;&gt; int(float(1) / 2)\n0\n&gt;&gt;&gt; int(float(3) / 2)\n1\n</code></pre> Example - Evaluate RPN <pre><code>\"\"\"\nWhat is the data :\nRPN expression as a list of tokens\ntokens = [\"2\",\"1\",\"+\",\"3\",\"*\"]\n\n\nFor each opearator +,-,*,/ there will be two pops.\nfirst pop is the rhs and second pop is the lhs \n\nPitfall :\nIn python integer division truncates away from zero\n&gt;&gt;&gt; -4//3\n-2             ## Truncates away from zero for negative numbers\n&gt;&gt;&gt; 4//3    \n1              ## Truncates towards zero for positive numbers\n\nTo handle this :\n&gt;&gt;&gt; -4/3               ## Do floating point division first\n-1.3333333333333333\n&gt;&gt;&gt; int(-4/3)          ## Then cast to int and result will always truncate towards zero.\n-1\n\"\"\"\nclass Solution:\n    def evalRPN(self, tokens: List[str]) -&gt; int:\n        evaluate  = {\n            '+' : lambda x,y : x+y ,\n            '-' : lambda x,y : x-y,\n            '/' : lambda x,y : int(x/y),\n            '*' : lambda x,y : x*y \n        }\n        stack = []\n        for token in tokens : \n            if token in evaluate :\n                rhs = stack.pop()\n                lhs = stack.pop()\n                stack.append(evaluate[token](lhs,rhs))\n            else :\n                stack.append(int(token))\n        return stack[-1]\n</code></pre>"},{"location":"Algorithms/Math/Division_Algorithm/#additional-resources","title":"Additional Resources :","text":"<p>LibreTexts- Mathematics</p>"},{"location":"Algorithms/Math/Appendix/Number_Theoretic_Notation/","title":"Number Theoretic Notation","text":"<p>Refresh Page if math notation is not rendered</p> <p>From here</p> <pre><code>Lemma. If m | pq and (m,p) = 1, then m | q.\n</code></pre> <p>Vertical Bar is the divides symbol.</p> <p>m | pq means that m divides the product pq, meaning there exists some integer k such that pq = km.</p> <p>This is called the gcd notation :</p> <p><code>(m,p) = 1</code></p> <p>Here it is used to express that m and p are co-prime or relatively prime. Meaning that their GCD is 1 and they share no common factors other than 1.</p> <p>Translation</p> <pre><code>If m divides pq and m and p are co-prime, then m must divide q.\n</code></pre> <p>Isn't it simple or at least approachable, once you understand what the notation means? </p> <p>So, I worked with Claude 4.0 to generate the following guide. I think it did a pretty good job. </p> <p>Following Content is AI Generated</p>"},{"location":"Algorithms/Math/Appendix/Number_Theoretic_Notation/#basic-number-systems-and-sets","title":"Basic Number Systems and Sets","text":"<p>Number theory begins with understanding different types of numbers and their standard mathematical notation.</p>"},{"location":"Algorithms/Math/Appendix/Number_Theoretic_Notation/#standard-number-sets","title":"Standard Number Sets","text":"<p>The foundation of number theory rests on several key number systems, each with specific notation:</p> <p>Natural Numbers: \\(\\mathbb{N}\\) represents the set of natural numbers. The exact definition varies by context:</p> <ul> <li>\\(\\mathbb{N} = {1, 2, 3, 4, \\ldots}\\) (positive integers only)</li> <li>\\(\\mathbb{N}_0 = {0, 1, 2, 3, \\ldots}\\) (including zero)</li> <li>\\(\\mathbb{N}^* = {1, 2, 3, 4, \\ldots}\\) (explicitly positive)</li> </ul> <p>Integers: \\(\\mathbb{Z} = {\\ldots, -2, -1, 0, 1, 2, \\ldots}\\) includes all positive and negative whole numbers.</p> <p>Rational Numbers: \\(\\mathbb{Q} = {\\frac{a}{b} : a, b \\in \\mathbb{Z}, b \\neq 0}\\) represents all fractions.</p> <p>Real Numbers: \\(\\mathbb{R}\\) encompasses all rational and irrational numbers.</p> <p>Complex Numbers: \\(\\mathbb{C} = {a + bi : a, b \\in \\mathbb{R}, i^2 = -1}\\) extends the reals.</p>"},{"location":"Algorithms/Math/Appendix/Number_Theoretic_Notation/#modular-arithmetic-sets","title":"Modular Arithmetic Sets","text":"<p>Integers modulo n: The set \\(\\mathbb{Z}/n\\mathbb{Z} = {0, 1, 2, \\ldots, n-1}\\) represents equivalence classes under modular arithmetic. Alternative notation \\(\\mathbb{Z}_n\\) exists but can create confusion with p-adic integers.</p> <p>Units modulo n: \\((\\mathbb{Z}/n\\mathbb{Z})^*\\) denotes integers relatively prime to \\(n\\) in the range \\({1, 2, \\ldots, n-1}\\).</p>"},{"location":"Algorithms/Math/Appendix/Number_Theoretic_Notation/#divisibility-and-basic-relations","title":"Divisibility and Basic Relations","text":"<p>Understanding when one integer divides another forms a cornerstone of number theory.</p>"},{"location":"Algorithms/Math/Appendix/Number_Theoretic_Notation/#divisibility-notation","title":"Divisibility Notation","text":"<p>The expression \\(a \\mid b\\) means \"a divides b\" or \"a is a divisor of b.\" This indicates that \\(b = ka\\) for some integer \\(k\\).</p> <p>Examples:</p> <ul> <li>\\(3 \\mid 12\\) since \\(12 = 4 \\times 3\\)</li> <li>\\(5 \\mid 35\\) since \\(35 = 7 \\times 5\\)</li> </ul> <p>The negation \\(a \\nmid b\\) means \"a does not divide b.\"</p>"},{"location":"Algorithms/Math/Appendix/Number_Theoretic_Notation/#greatest-common-divisor-gcd","title":"Greatest Common Divisor (GCD)","text":"<p>The greatest common divisor has several notational conventions. The most unambiguous is \\(\\gcd(a,b)\\), though traditional mathematics sometimes uses \\((a,b)\\).</p> <p>Key Properties:</p> <ul> <li>\\(\\gcd(a,b) = \\gcd(b, a \\bmod b)\\) (Euclidean algorithm)</li> <li>\\(\\gcd(a,b) \\cdot \\text{lcm}(a,b) = ab\\)</li> </ul> <p>Examples:</p> <ul> <li>\\(\\gcd(12, 18) = 6\\)</li> <li>\\(\\gcd(17, 19) = 1\\) (relatively prime)</li> </ul>"},{"location":"Algorithms/Math/Appendix/Number_Theoretic_Notation/#least-common-multiple-lcm","title":"Least Common Multiple (LCM)","text":"<p>The least common multiple uses \\(\\text{lcm}(a,b)\\) or traditional bracket notation \\([a,b]\\).</p> <p>Examples:</p> <ul> <li>\\(\\text{lcm}(12, 18) = 36\\)</li> <li>\\(\\text{lcm}(4, 6) = 12\\)</li> </ul>"},{"location":"Algorithms/Math/Appendix/Number_Theoretic_Notation/#prime-numbers-and-factorization","title":"Prime Numbers and Factorization","text":"<p>Prime numbers serve as the building blocks of all integers through unique factorization.</p>"},{"location":"Algorithms/Math/Appendix/Number_Theoretic_Notation/#prime-notation-and-factorization","title":"Prime Notation and Factorization","text":"<p>Individual primes are typically denoted by lowercase \\(p\\), with subscripts for distinct primes: \\(p_1, p_2, \\ldots, p_k\\).</p> <p>Fundamental Theorem of Arithmetic: Every integer \\(n &gt; 1\\) has a unique prime factorization:</p> <p>\\[n = p_1^{e_1} p_2^{e_2} \\cdots p_k^{e_k}\\]</p> <p>where \\(p_1 &lt; p_2 &lt; \\cdots &lt; p_k\\) are distinct primes and \\(e_i \\geq 1\\).</p> <p>Examples:</p> <ul> <li>\\(12 = 2^2 \\cdot 3^1\\)</li> <li>\\(100 = 2^2 \\cdot 5^2\\)</li> </ul>"},{"location":"Algorithms/Math/Appendix/Number_Theoretic_Notation/#prime-related-functions","title":"Prime-Related Functions","text":"<p>Prime counting function: \\(\\pi(x)\\) counts primes up to \\(x\\)</p> <ul> <li>\\(\\pi(10) = 4\\) (primes: 2, 3, 5, 7)</li> </ul> <p>Number of prime factors:</p> <ul> <li>\\(\\Omega(n)\\) counts prime factors with multiplicity</li> <li>\\(\\omega(n)\\) counts distinct prime factors</li> </ul> <p>Examples:</p> <ul> <li>\\(\\Omega(12) = 3\\) since \\(12 = 2^2 \\cdot 3\\)</li> <li>\\(\\omega(12) = 2\\) since 12 has two distinct prime factors</li> </ul>"},{"location":"Algorithms/Math/Appendix/Number_Theoretic_Notation/#congruences-and-modular-arithmetic","title":"Congruences and Modular Arithmetic","text":"<p>Congruences provide a powerful framework for studying integer relationships.</p>"},{"location":"Algorithms/Math/Appendix/Number_Theoretic_Notation/#congruence-notation","title":"Congruence Notation","text":"<p>The fundamental congruence relation \\(a \\equiv b \\pmod{n}\\) means \\(n \\mid (a-b)\\).</p> <p>Standard form: \\(a \\equiv b \\pmod{n}\\) with parentheses around the modulus.</p> <p>Examples:</p> <ul> <li>\\(13 \\equiv 1 \\pmod{12}\\) since \\(12 \\mid (13-1)\\)</li> <li>\\(25 \\equiv 4 \\pmod{7}\\) since \\(7 \\mid (25-4)\\)</li> </ul> <p>Key properties:</p> <ul> <li>Reflexive: \\(a \\equiv a \\pmod{n}\\)</li> <li>Symmetric: If \\(a \\equiv b \\pmod{n}\\), then \\(b \\equiv a \\pmod{n}\\)</li> <li>Transitive: If \\(a \\equiv b \\pmod{n}\\) and \\(b \\equiv c \\pmod{n}\\), then \\(a \\equiv c \\pmod{n}\\)</li> </ul>"},{"location":"Algorithms/Math/Appendix/Number_Theoretic_Notation/#modular-arithmetic-operations","title":"Modular Arithmetic Operations","text":"<p>Operations preserve congruences:</p> <ul> <li>\\((a + b) \\bmod n\\)</li> <li>\\((a \\cdot b) \\bmod n\\)</li> <li>\\(a^k \\bmod n\\)</li> </ul> <p>Multiplicative inverse: \\(a^{-1} \\pmod{n}\\) exists when \\(\\gcd(a,n) = 1\\).</p> <p>Example: \\(3^{-1} \\equiv 4 \\pmod{11}\\) since \\(3 \\cdot 4 \\equiv 1 \\pmod{11}\\).</p>"},{"location":"Algorithms/Math/Appendix/Number_Theoretic_Notation/#eulers-totient-function","title":"Euler's Totient Function","text":"<p>Euler's totient function counts integers relatively prime to a given number.</p>"},{"location":"Algorithms/Math/Appendix/Number_Theoretic_Notation/#definition-and-notation","title":"Definition and Notation","text":"<p>\\(\\phi(n)\\) (or \\(\\varphi(n)\\)) equals the number of positive integers less than or equal to \\(n\\) that are relatively prime to \\(n\\).</p> <p>Formula for prime powers: If \\(p\\) is prime, then:</p> <ul> <li>\\(\\phi(p) = p - 1\\)</li> <li>\\(\\phi(p^k) = p^k - p^{k-1} = p^{k-1}(p-1)\\)</li> </ul> <p>General formula: For \\(n\\) with prime factorization \\(n = p_1^{e_1} p_2^{e_2} \\cdots p_k^{e_k}\\):</p> <p>\\[\\phi(n) = n \\prod_{i=1}^{k} \\left(1 - \\frac{1}{p_i}\\right)\\]</p> <p>Examples:</p> <ul> <li>\\(\\phi(9) = 6\\) (numbers 1, 2, 4, 5, 7, 8 are relatively prime to 9)</li> <li>\\(\\phi(12) = 4\\) (numbers 1, 5, 7, 11 are relatively prime to 12)</li> </ul>"},{"location":"Algorithms/Math/Appendix/Number_Theoretic_Notation/#eulers-theorem","title":"Euler's Theorem","text":"<p>For \\(\\gcd(a,n) = 1\\): \\(a^{\\phi(n)} \\equiv 1 \\pmod{n}\\)</p> <p>Fermat's Little Theorem (special case): </p> <p>For prime \\(p\\) and \\(\\gcd(a,p) = 1\\): \\(a^{p-1} \\equiv 1 \\pmod{p}\\)</p>"},{"location":"Algorithms/Math/Appendix/Number_Theoretic_Notation/#floor-and-ceiling-functions","title":"Floor and Ceiling Functions","text":"<p>These functions convert real numbers to integers with specific rounding behavior.</p>"},{"location":"Algorithms/Math/Appendix/Number_Theoretic_Notation/#floor-function","title":"Floor Function","text":"<p>\\(\\lfloor x \\rfloor\\) gives the greatest integer less than or equal to \\(x\\).</p> <p>Examples:</p> <ul> <li>\\(\\lfloor 3.7 \\rfloor = 3\\)</li> <li>\\(\\lfloor -2.3 \\rfloor = -3\\)</li> <li>\\(\\lfloor 5 \\rfloor = 5\\)</li> </ul>"},{"location":"Algorithms/Math/Appendix/Number_Theoretic_Notation/#ceiling-function","title":"Ceiling Function","text":"<p>\\(\\lceil x \\rceil\\) gives the smallest integer greater than or equal to \\(x\\).</p> <p>Examples:</p> <ul> <li>\\(\\lceil 3.2 \\rceil = 4\\)</li> <li>\\(\\lceil -2.3 \\rceil = -2\\)</li> <li>\\(\\lceil 5 \\rceil = 5\\)</li> </ul>"},{"location":"Algorithms/Math/Appendix/Number_Theoretic_Notation/#applications-in-number-theory","title":"Applications in Number Theory","text":"<p>Counting divisors: The number of multiples of \\(d\\) up to \\(n\\) is \\(\\lfloor n/d \\rfloor\\).</p> <p>Division algorithm: For integers \\(a\\) and \\(b &gt; 0\\): \\(a = bq + r\\) where \\(q = \\lfloor a/b \\rfloor\\) and \\(0 \\leq r &lt; b\\).</p>"},{"location":"Algorithms/Math/Appendix/Number_Theoretic_Notation/#summation-and-product-notation","title":"Summation and Product Notation","text":"<p>These operators handle collections of terms systematically.</p>"},{"location":"Algorithms/Math/Appendix/Number_Theoretic_Notation/#summation-over-divisors","title":"Summation Over Divisors","text":"<p>A fundamental pattern in number theory involves summing over all divisors of an integer.</p> <p>Notation: \\(\\sum_{d \\mid n} f(d)\\) means sum \\(f(d)\\) over all positive divisors \\(d\\) of \\(n\\).</p> <p>Examples:</p> <ul> <li>\\(\\sum_{d \\mid n} 1 = d(n)\\) (number of divisors)</li> <li>\\(\\sum_{d \\mid n} d = \\sigma(n)\\) (sum of divisors)</li> <li>\\(\\sum_{d \\mid n} \\phi(d) = n\\) (fundamental identity)</li> </ul>"},{"location":"Algorithms/Math/Appendix/Number_Theoretic_Notation/#summation-over-primes","title":"Summation Over Primes","text":"<p>Examples:</p> <ul> <li>\\(\\sum_{p \\leq x} \\frac{1}{p}\\) (sum of reciprocals of primes up to \\(x\\))</li> <li>\\(\\sum_{\\substack{p \\text{ prime} \\ p \\equiv 1 \\pmod{4}}} \\frac{1}{p^s}\\) (conditional summation)</li> </ul>"},{"location":"Algorithms/Math/Appendix/Number_Theoretic_Notation/#product-notation","title":"Product Notation","text":"<p>Euler product formula: </p> <p>\\[\\zeta(s) = \\sum_{n=1}^{\\infty} \\frac{1}{n^s} = \\prod_{p \\text{ prime}} \\frac{1}{1-p^{-s}}\\]</p> <p>Totient function product:</p> <p>\\[\\phi(n) = n \\prod_{\\substack{p \\text{ prime} \\ p \\mid n}} \\left(1 - \\frac{1}{p}\\right)\\]</p>"},{"location":"Algorithms/Math/Appendix/Number_Theoretic_Notation/#asymptotic-notation","title":"Asymptotic Notation","text":"<p>Asymptotic analysis describes the growth behavior of functions as their arguments approach infinity.</p>"},{"location":"Algorithms/Math/Appendix/Number_Theoretic_Notation/#big-o-family-bachmann-landau-notation","title":"Big O Family (Bachmann-Landau Notation)","text":"<p>Big O: \\(f(n) = O(g(n))\\) means \\(f\\) grows at most as fast as \\(g\\).</p> <ul> <li>Formally: \\(\\exists\\ c &gt; 0, n_0\\) such that \\(|f(n)| \\leq c|g(n)|\\) for all \\(n \\geq n_0\\)</li> </ul> <p>Big Omega: \\(f(n) = \\Omega(g(n))\\) means \\(f\\) grows at least as fast as \\(g\\).</p> <p>Big Theta: \\(f(n) = \\Theta(g(n))\\) means \\(f\\) and \\(g\\) have the same growth rate.</p> <p>Little o: \\(f(n) = o(g(n))\\) means \\(f\\) grows strictly slower than \\(g\\).</p> <p>Asymptotic equivalence: \\(f(n) \\sim g(n)\\) means \\(\\lim_{n \\to \\infty} \\frac{f(n)}{g(n)} = 1\\).</p>"},{"location":"Algorithms/Math/Appendix/Number_Theoretic_Notation/#applications-in-number-theory_1","title":"Applications in Number Theory","text":"<p>Prime Number Theorem: \\(\\pi(x) \\sim \\frac{x}{\\ln x}\\)</p> <p>Divisor function: \\(d(n) = O(n^{\\epsilon})\\) for any \\(\\epsilon &gt; 0\\)</p> <p>Average order of Euler's totient: \\(\\sum_{n \\leq x} \\phi(n) \\sim \\frac{3x^2}{\\pi^2}\\)</p>"},{"location":"Algorithms/Math/Appendix/Number_Theoretic_Notation/#advanced-number-theory-symbols","title":"Advanced Number Theory Symbols","text":"<p>Specialized symbols appear in deeper number theory topics.</p>"},{"location":"Algorithms/Math/Appendix/Number_Theoretic_Notation/#legendre-symbol","title":"Legendre Symbol","text":"<p>For odd prime \\(p\\) and integer \\(a\\), the Legendre symbol \\(\\left(\\frac{a}{p}\\right)\\) indicates quadratic residue status:</p> <ul> <li>\\(\\left(\\frac{a}{p}\\right) = 1\\) if \\(a\\) is a quadratic residue modulo \\(p\\)</li> <li>\\(\\left(\\frac{a}{p}\\right) = -1\\) if \\(a\\) is a quadratic non-residue modulo \\(p\\)  </li> <li>\\(\\left(\\frac{a}{p}\\right) = 0\\) if \\(p \\mid a\\)</li> </ul> <p>Example: \\(\\left(\\frac{2}{7}\\right) = 1\\) since \\(3^2 \\equiv 2 \\pmod{7}\\).</p>"},{"location":"Algorithms/Math/Appendix/Number_Theoretic_Notation/#jacobi-symbol","title":"Jacobi Symbol","text":"<p>The Jacobi symbol \\(\\left(\\frac{a}{n}\\right)\\) extends the Legendre symbol to odd composite integers \\(n\\).</p> <p>For \\(n = p_1^{e_1} p_2^{e_2} \\cdots p_k^{e_k}\\): \\[\\left(\\frac{a}{n}\\right) = \\prod_{i=1}^{k} \\left(\\frac{a}{p_i}\\right)^{e_i}\\]</p>"},{"location":"Algorithms/Math/Appendix/Number_Theoretic_Notation/#quadratic-reciprocity","title":"Quadratic Reciprocity","text":"<p>For distinct odd primes \\(p\\) and \\(q\\):</p> <p>\\[\\left(\\frac{p}{q}\\right)\\left(\\frac{q}{p}\\right) = (-1)^{\\frac{p-1}{2} \\cdot \\frac{q-1}{2}}\\]</p>"},{"location":"Algorithms/Math/Appendix/Number_Theoretic_Notation/#arithmetic-functions","title":"Arithmetic Functions","text":"<p>M\u00f6bius function: \\(\\mu(n)\\) equals:</p> <ul> <li>\\(1\\) if \\(n\\) is a square-free positive integer with even number of prime factors</li> <li>\\(-1\\) if \\(n\\) is a square-free positive integer with odd number of prime factors  </li> <li>\\(0\\) if \\(n\\) has a squared prime factor</li> </ul> <p>Von Mangoldt function: \\(\\Lambda(n)\\) equals \\(\\ln p\\) if \\(n = p^k\\) for prime \\(p\\), and \\(0\\) otherwise.</p> <p>Dirichlet characters: \\(\\chi(n)\\) are completely multiplicative functions modulo some integer.</p>"},{"location":"Algorithms/Math/Appendix/Number_Theoretic_Notation/#understanding-mathematical-expressions","title":"Understanding Mathematical Expressions","text":"<p>When reading number theory texts, you'll encounter mathematical expressions that combine multiple notations. Here are some common patterns:</p>"},{"location":"Algorithms/Math/Appendix/Number_Theoretic_Notation/#complex-expressions","title":"Complex Expressions","text":"<p>Multiplicative functions: An expression like \\(\\sum_{d \\mid n} \\mu(d) f(n/d)\\) means \"sum over all divisors \\(d\\) of \\(n\\), the product of the M\u00f6bius function \\(\\mu(d)\\) and \\(f(n/d)\\).\"</p> <p>Conditional sums: \\(\\sum_{\\substack{p \\leq x \\ p \\equiv 1 \\pmod{4}}} \\frac{1}{p}\\) means \"sum \\(\\frac{1}{p}\\) over all primes \\(p\\) up to \\(x\\) that are congruent to 1 modulo 4.\"</p> <p>Product formulas: \\(\\prod_{p \\mid n} \\left(1 + \\frac{1}{p}\\right)\\) means \"take the product over all primes \\(p\\) that divide \\(n\\) of the expression \\(\\left(1 + \\frac{1}{p}\\right)\\).\"</p>"},{"location":"Algorithms/Math/Appendix/Number_Theoretic_Notation/#reading-order-and-precedence","title":"Reading Order and Precedence","text":"<p>Mathematical expressions follow standard precedence rules:</p> <ol> <li>Exponents and function applications (like \\(\\phi(n)\\))</li> <li>Multiplication and division</li> <li>Addition and subtraction</li> <li>Relations (like \\(\\equiv\\), \\(\\mid\\), \\(&lt;\\))</li> </ol> <p>Example: \\(a^{\\phi(n)} \\equiv 1 \\pmod{n}\\) reads as \"(\\(a\\) raised to the power \\(\\phi(n)\\)) is congruent to 1 modulo \\(n\\).\"</p> <p>This comprehensive guide provides the foundational notation needed for understanding number theory concepts in academic literature and mathematical texts.</p>"},{"location":"Algorithms/Math/Appendix/Set_Theoretic_Notation/","title":"Set Theoretic Notation","text":"<p>AI Generated Content</p> <p>Refresh Page if math notation is not rendered</p> <p>A comprehensive reference for mathematical notation used in set theory, from basic concepts to advanced topics.</p>"},{"location":"Algorithms/Math/Appendix/Set_Theoretic_Notation/#basic-set-notation","title":"Basic Set Notation","text":"<p>Set theory provides the foundation for modern mathematics through its system of notation for collections and relationships.</p>"},{"location":"Algorithms/Math/Appendix/Set_Theoretic_Notation/#set-definition-and-membership","title":"Set Definition and Membership","text":"<p>Sets are collections of objects called elements or members. The fundamental relationship is membership.</p> <p>Membership: \\(x \\in A\\) means \"\\(x\\) is an element of set \\(A\\)\" or \"\\(x\\) belongs to \\(A\\).\"</p> <p>Non-membership: \\(x \\notin A\\) means \"\\(x\\) is not an element of set \\(A\\).\"</p> <p>Examples: - \\(3 \\in {1, 2, 3, 4}\\) (3 is in the set) - \\(5 \\notin {1, 2, 3, 4}\\) (5 is not in the set) - \\(\\pi \\in \\mathbb{R}\\) (\u03c0 is a real number)</p>"},{"location":"Algorithms/Math/Appendix/Set_Theoretic_Notation/#set-builder-notation","title":"Set Builder Notation","text":"<p>Sets can be defined by listing elements or by describing properties.</p> <p>Roster notation: \\(A = {1, 2, 3, 4}\\) lists all elements explicitly.</p> <p>Set-builder notation: \\(A = {x : P(x)}\\) or \\(A = {x \\mid P(x)}\\) means \"the set of all \\(x\\) such that property \\(P(x)\\) holds.\"</p> <p>Examples: - \\({x \\in \\mathbb{Z} : x^2 &lt; 10}\\) (integers whose square is less than 10) - \\({x \\in \\mathbb{R} \\mid x &gt; 0}\\) (positive real numbers) - \\({2n : n \\in \\mathbb{N}}\\) (even natural numbers)</p>"},{"location":"Algorithms/Math/Appendix/Set_Theoretic_Notation/#special-sets","title":"Special Sets","text":"<p>Empty set: \\(\\emptyset\\) or \\({}\\) contains no elements.</p> <p>Universal set: \\(U\\) or \\(\\Omega\\) represents the collection of all objects under consideration in a particular context.</p> <p>Singleton set: \\({a}\\) contains exactly one element \\(a\\).</p>"},{"location":"Algorithms/Math/Appendix/Set_Theoretic_Notation/#set-relationships","title":"Set Relationships","text":"<p>Understanding how sets relate to each other is fundamental to set theory.</p>"},{"location":"Algorithms/Math/Appendix/Set_Theoretic_Notation/#subset-relations","title":"Subset Relations","text":"<p>Subset: \\(A \\subseteq B\\) means \"\\(A\\) is a subset of \\(B\\)\" or \"every element of \\(A\\) is also in \\(B\\).\"</p> <ul> <li>Formally: \\(\\forall x (x \\in A \\rightarrow x \\in B)\\)</li> </ul> <p>Proper subset: \\(A \\subsetneq B\\) or \\(A \\subset B\\) means \"\\(A\\) is a proper subset of \\(B\\)\" (\\(A \\subseteq B\\) and \\(A \\neq B\\)).</p> <p>Superset: \\(B \\supseteq A\\) means \"\\(B\\) is a superset of \\(A\\)\" (equivalent to \\(A \\subseteq B\\)).</p> <p>Proper superset: \\(B \\supsetneq A\\) or \\(B \\supset A\\) means \"\\(B\\) is a proper superset of \\(A\\).\"</p> <p>Examples: - \\({1, 2} \\subseteq {1, 2, 3}\\) - \\(\\mathbb{N} \\subset \\mathbb{Z} \\subset \\mathbb{Q} \\subset \\mathbb{R}\\) - \\(\\emptyset \\subseteq A\\) for any set \\(A\\)</p>"},{"location":"Algorithms/Math/Appendix/Set_Theoretic_Notation/#set-equality","title":"Set Equality","text":"<p>Set equality: \\(A = B\\) means sets \\(A\\) and \\(B\\) have exactly the same elements.</p> <ul> <li>Equivalent to: \\(A \\subseteq B\\) and \\(B \\subseteq A\\)</li> </ul> <p>Set inequality: \\(A \\neq B\\) means sets \\(A\\) and \\(B\\) do not have the same elements.</p>"},{"location":"Algorithms/Math/Appendix/Set_Theoretic_Notation/#set-operations","title":"Set Operations","text":"<p>Set operations combine or modify sets to create new sets.</p>"},{"location":"Algorithms/Math/Appendix/Set_Theoretic_Notation/#basic-operations","title":"Basic Operations","text":"<p>Union: \\(A \\cup B\\) is the set of elements in \\(A\\) or \\(B\\) (or both).</p> <ul> <li>\\(A \\cup B = {x : x \\in A \\text{ or } x \\in B}\\)</li> </ul> <p>Intersection: \\(A \\cap B\\) is the set of elements in both \\(A\\) and \\(B\\).</p> <ul> <li>\\(A \\cap B = {x : x \\in A \\text{ and } x \\in B}\\)</li> </ul> <p>Difference: \\(A \\setminus B\\) or \\(A - B\\) is the set of elements in \\(A\\) but not in \\(B\\).</p> <ul> <li>\\(A \\setminus B = {x : x \\in A \\text{ and } x \\notin B}\\)</li> </ul> <p>Complement: \\(A^c\\) or \\(\\overline{A}\\) or \\(A'\\) is the set of elements not in \\(A\\) (relative to some universal set).</p> <ul> <li>\\(A^c = {x \\in U : x \\notin A}\\)</li> </ul> <p>Examples: - \\({1, 2} \\cup {2, 3} = {1, 2, 3}\\) - \\({1, 2} \\cap {2, 3} = {2}\\) - \\({1, 2, 3} \\setminus {2} = {1, 3}\\)</p>"},{"location":"Algorithms/Math/Appendix/Set_Theoretic_Notation/#disjoint-sets","title":"Disjoint Sets","text":"<p>Disjoint sets: \\(A\\) and \\(B\\) are disjoint if \\(A \\cap B = \\emptyset\\).</p> <p>Pairwise disjoint: A collection of sets is pairwise disjoint if every pair of distinct sets in the collection is disjoint.</p>"},{"location":"Algorithms/Math/Appendix/Set_Theoretic_Notation/#extended-operations","title":"Extended Operations","text":"<p>Symmetric difference: \\(A \\triangle B\\) or \\(A \\oplus B\\) contains elements in exactly one of \\(A\\) or \\(B\\).</p> <ul> <li>\\(A \\triangle B = (A \\setminus B) \\cup (B \\setminus A) = (A \\cup B) \\setminus (A \\cap B)\\)</li> </ul>"},{"location":"Algorithms/Math/Appendix/Set_Theoretic_Notation/#generalized-operations","title":"Generalized Operations","text":"<p>Set operations can be extended to collections of multiple sets.</p>"},{"location":"Algorithms/Math/Appendix/Set_Theoretic_Notation/#indexed-families","title":"Indexed Families","text":"<p>Indexed union: \\(\\bigcup_{i \\in I} A_i\\) is the union of all sets \\(A_i\\) where \\(i\\) ranges over index set \\(I\\).</p> <ul> <li>\\(\\bigcup_{i \\in I} A_i = {x : \\exists i \\in I, x \\in A_i}\\)</li> </ul> <p>Indexed intersection: \\(\\bigcap_{i \\in I} A_i\\) is the intersection of all sets \\(A_i\\) where \\(i\\) ranges over index set \\(I\\).</p> <ul> <li>\\(\\bigcap_{i \\in I} A_i = {x : \\forall i \\in I, x \\in A_i}\\)</li> </ul> <p>Common notations:</p> <ul> <li>\\(\\bigcup_{i=1}^{n} A_i = A_1 \\cup A_2 \\cup \\cdots \\cup A_n\\)</li> <li>\\(\\bigcap_{i=1}^{\\infty} A_i\\) (intersection of countably many sets)</li> <li>\\(\\bigcup_{x \\in X} A_x\\) (union indexed by elements of set \\(X\\))</li> </ul>"},{"location":"Algorithms/Math/Appendix/Set_Theoretic_Notation/#partition","title":"Partition","text":"<p>Partition: A partition of set \\(A\\) is a collection of non-empty, pairwise disjoint subsets whose union is \\(A\\).</p> <p>If \\(\\{A_i\\}_{i \\in I}\\) is a partition of \\(A\\), then:</p> <ul> <li>\\(A_i \\neq \\emptyset\\) for all \\(i \\in I\\)</li> <li>\\(A_i \\cap A_j = \\emptyset\\) for \\(i \\neq j\\)</li> <li>\\(\\bigcup_{i \\in I} A_i = A\\)</li> </ul>"},{"location":"Algorithms/Math/Appendix/Set_Theoretic_Notation/#power-sets-and-cardinality","title":"Power Sets and Cardinality","text":"<p>These concepts deal with the \"size\" and structure of sets.</p>"},{"location":"Algorithms/Math/Appendix/Set_Theoretic_Notation/#power-set","title":"Power Set","text":"<p>Power set: \\(\\mathcal{P}(A)\\) or \\(2^A\\) is the set of all subsets of \\(A\\).</p> <ul> <li>\\(\\mathcal{P}(A) = {X : X \\subseteq A}\\)</li> </ul> <p>Examples: - \\(\\mathcal{P}({1, 2}) = {\\emptyset, {1}, {2}, {1, 2}}\\) - \\(\\mathcal{P}(\\emptyset) = {\\emptyset}\\)</p>"},{"location":"Algorithms/Math/Appendix/Set_Theoretic_Notation/#cardinality","title":"Cardinality","text":"<p>Cardinality: \\(|A|\\) or \\(\\#A\\) or \\(\\text{card}(A)\\) represents the number of elements in set \\(A\\).</p> <p>Finite sets: \\(|A| = n\\) for some natural number \\(n\\).</p> <p>Infinite cardinalities:</p> <ul> <li>\\(|\\mathbb{N}| = \\aleph_0\\) (aleph-null, countable infinity)</li> <li>\\(|\\mathbb{R}| = 2^{\\aleph_0} = \\mathfrak{c}\\) (continuum, uncountable)</li> </ul> <p>Cardinality relationships:</p> <ul> <li>\\(|A| = |B|\\) means \\(A\\) and \\(B\\) have the same cardinality</li> <li>\\(|A| \\leq |B|\\) means there exists an injection from \\(A\\) to \\(B\\)</li> <li>\\(|A| &lt; |B|\\) means \\(|A| \\leq |B|\\) and \\(|A| \\neq |B|\\)</li> </ul>"},{"location":"Algorithms/Math/Appendix/Set_Theoretic_Notation/#relations-and-functions","title":"Relations and Functions","text":"<p>Relations and functions are special types of sets that describe connections between elements.</p>"},{"location":"Algorithms/Math/Appendix/Set_Theoretic_Notation/#relations","title":"Relations","text":"<p>Relation: A relation \\(R\\) from set \\(A\\) to set \\(B\\) is a subset of the Cartesian product \\(A \\times B\\).</p> <p>Notation: \\(aRb\\) or \\((a,b) \\in R\\) means \"\\(a\\) is related to \\(b\\) by relation \\(R\\).\"</p> <p>Types of relations on a set \\(A\\):</p> <ul> <li>Reflexive: \\(\\forall a \\in A, aRa\\)</li> <li>Symmetric: \\(\\forall a,b \\in A, aRb \\rightarrow bRa\\)</li> <li>Transitive: \\(\\forall a,b,c \\in A, (aRb \\land bRc) \\rightarrow aRc\\)</li> <li>Antisymmetric: \\(\\forall a,b \\in A, (aRb \\land bRa) \\rightarrow a = b\\)</li> </ul> <p>Equivalence relation: A relation that is reflexive, symmetric, and transitive.</p> <p>Partial order: A relation that is reflexive, antisymmetric, and transitive.</p>"},{"location":"Algorithms/Math/Appendix/Set_Theoretic_Notation/#functions","title":"Functions","text":"<p>Function: \\(f: A \\rightarrow B\\) is a relation where each element in \\(A\\) is related to exactly one element in \\(B\\).</p> <p>Function notation: \\(f(a) = b\\) means \"function \\(f\\) maps element \\(a\\) to element \\(b\\).\"</p> <p>Domain: \\(\\text{dom}(f)\\) is the set of all inputs to function \\(f\\).</p> <p>Codomain: The set \\(B\\) in \\(f: A \\rightarrow B\\).</p> <p>Range or Image: \\(\\text{range}(f) = {f(a) : a \\in A}\\) is the set of all actual outputs.</p> <p>Image of a set: \\(f[X] = {f(x) : x \\in X}\\) for \\(X \\subseteq A\\).</p> <p>Preimage: \\(f^{-1}[Y] = {x \\in A : f(x) \\in Y}\\) for \\(Y \\subseteq B\\).</p>"},{"location":"Algorithms/Math/Appendix/Set_Theoretic_Notation/#function-properties","title":"Function Properties","text":"<p>Injective (one-to-one): \\(\\forall a_1, a_2 \\in A, f(a_1) = f(a_2) \\rightarrow a_1 = a_2\\)</p> <p>Surjective (onto): \\(\\forall b \\in B, \\exists a \\in A, f(a) = b\\)</p> <p>Bijective: Both injective and surjective.</p> <p>Inverse function: If \\(f: A \\rightarrow B\\) is bijective, then \\(f^{-1}: B \\rightarrow A\\) exists.</p>"},{"location":"Algorithms/Math/Appendix/Set_Theoretic_Notation/#cartesian-products-and-tuples","title":"Cartesian Products and Tuples","text":"<p>These constructions create new sets from existing ones.</p>"},{"location":"Algorithms/Math/Appendix/Set_Theoretic_Notation/#cartesian-product","title":"Cartesian Product","text":"<p>Cartesian product: \\(A \\times B = {(a,b) : a \\in A \\text{ and } b \\in B}\\) is the set of all ordered pairs.</p> <p>Higher-order products:</p> <ul> <li>\\(A \\times B \\times C = {(a,b,c) : a \\in A, b \\in B, c \\in C}\\)</li> <li>\\(A^n = A \\times A \\times \\cdots \\times A\\) (\\(n\\) times)</li> </ul> <p>Examples: - \\({1,2} \\times {a,b} = {(1,a), (1,b), (2,a), (2,b)}\\) - \\(\\mathbb{R}^2 = \\mathbb{R} \\times \\mathbb{R}\\) (the plane)</p>"},{"location":"Algorithms/Math/Appendix/Set_Theoretic_Notation/#tuples-and-sequences","title":"Tuples and Sequences","text":"<p>Ordered pair: \\((a,b)\\) where order matters (\\((a,b) \\neq (b,a)\\) unless \\(a = b\\)).</p> <p>n-tuple: \\((a_1, a_2, \\ldots, a_n)\\) is an ordered collection of \\(n\\) elements.</p> <p>Sequence: An ordered list, often infinite:</p> <ul> <li>Finite sequence: \\((a_1, a_2, \\ldots, a_n)\\)</li> <li>Infinite sequence: \\((a_1, a_2, a_3, \\ldots)\\) or \\((a_n)_{n=1}^{\\infty}\\)</li> </ul>"},{"location":"Algorithms/Math/Appendix/Set_Theoretic_Notation/#advanced-set-theory-concepts","title":"Advanced Set Theory Concepts","text":"<p>These concepts appear in more sophisticated mathematical contexts.</p>"},{"location":"Algorithms/Math/Appendix/Set_Theoretic_Notation/#topology-and-closure","title":"Topology and Closure","text":"<p>Closure: \\(\\overline{A}\\) or \\(\\text{cl}(A)\\) represents the closure of set \\(A\\) in a topological space.</p> <p>Interior: \\(\\text{int}(A)\\) or \\(A^\\circ\\) represents the interior of set \\(A\\).</p> <p>Boundary: \\(\\partial A\\) represents the boundary of set \\(A\\).</p>"},{"location":"Algorithms/Math/Appendix/Set_Theoretic_Notation/#ordinal-and-cardinal-numbers","title":"Ordinal and Cardinal Numbers","text":"<p>Ordinal numbers: \\(\\omega, \\omega + 1, \\omega \\cdot 2, \\ldots\\) represent positions in well-ordered sets.</p> <p>Cardinal numbers: \\(\\aleph_0, \\aleph_1, \\aleph_2, \\ldots\\) represent sizes of infinite sets.</p> <p>Continuum hypothesis: \\(2^{\\aleph_0} = \\aleph_1\\) (there is no cardinality strictly between \\(\\aleph_0\\) and \\(2^{\\aleph_0}\\)).</p>"},{"location":"Algorithms/Math/Appendix/Set_Theoretic_Notation/#choice-and-well-ordering","title":"Choice and Well-Ordering","text":"<p>Axiom of Choice: For any collection of non-empty sets, there exists a choice function that selects one element from each set.</p> <p>Well-ordering: Every non-empty subset has a least element.</p> <p>Zorn's Lemma: If every chain in a partially ordered set has an upper bound, then the set has a maximal element.</p>"},{"location":"Algorithms/Math/Appendix/Set_Theoretic_Notation/#logical-quantifiers-in-set-context","title":"Logical Quantifiers in Set Context","text":"<p>Set theory frequently uses logical notation to express properties precisely.</p>"},{"location":"Algorithms/Math/Appendix/Set_Theoretic_Notation/#universal-and-existential-quantifiers","title":"Universal and Existential Quantifiers","text":"<p>Universal quantifier: \\(\\forall x \\in A, P(x)\\) means \"for all \\(x\\) in \\(A\\), property \\(P(x)\\) holds.\"</p> <p>Existential quantifier: \\(\\exists x \\in A, P(x)\\) means \"there exists an \\(x\\) in \\(A\\) such that property \\(P(x)\\) holds.\"</p> <p>Unique existence: \\(\\exists! x \\in A, P(x)\\) means \"there exists a unique \\(x\\) in \\(A\\) such that \\(P(x)\\) holds.\"</p> <p>Examples: - \\(\\forall x \\in \\mathbb{R}, x^2 \\geq 0\\) (all real numbers have non-negative squares) - \\(\\exists x \\in \\mathbb{Q}, x^2 = 2\\) is false (no rational number squares to 2) - \\(\\exists! x \\in \\mathbb{R}, x^3 = 8\\) (exactly one real cube root of 8)</p>"},{"location":"Algorithms/Math/Appendix/Set_Theoretic_Notation/#logical-connectives","title":"Logical Connectives","text":"<p>Conjunction: \\(P \\land Q\\) means \"\\(P\\) and \\(Q\\)\"</p> <p>Disjunction: \\(P \\lor Q\\) means \"\\(P\\) or \\(Q\\)\"</p> <p>Negation: \\(\\neg P\\) means \"not \\(P\\)\"</p> <p>Implication: \\(P \\rightarrow Q\\) means \"if \\(P\\) then \\(Q\\)\"</p> <p>Biconditional: \\(P \\leftrightarrow Q\\) means \"\\(P\\) if and only if \\(Q\\)\"</p>"},{"location":"Algorithms/Math/Appendix/Set_Theoretic_Notation/#understanding-complex-set-expressions","title":"Understanding Complex Set Expressions","text":"<p>Reading complex set-theoretic statements requires understanding how notation combines.</p>"},{"location":"Algorithms/Math/Appendix/Set_Theoretic_Notation/#parsing-complex-statements","title":"Parsing Complex Statements","text":"<p>Multiple operations: \\(A \\cup (B \\cap C)\\) means \"\\(A\\) union with the intersection of \\(B\\) and \\(C\\).\"</p> <p>Quantified statements: \\({x \\in \\mathbb{R} : \\forall \\epsilon &gt; 0, \\exists \\delta &gt; 0, |x| &lt; \\delta \\rightarrow |f(x)| &lt; \\epsilon}\\) describes points where function \\(f\\) approaches 0.</p> <p>Nested operations: \\(\\bigcup_{n=1}^{\\infty} \\bigcap_{k=n}^{\\infty} A_k\\) represents the limit superior of a sequence of sets.</p>"},{"location":"Algorithms/Math/Appendix/Set_Theoretic_Notation/#common-patterns","title":"Common Patterns","text":"<p>Characterizing elements: Statements like \\(x \\in A \\leftrightarrow P(x)\\) define set membership by properties.</p> <p>Set equality proofs: To show \\(A = B\\), prove both \\(A \\subseteq B\\) and \\(B \\subseteq A\\).</p> <p>Distributive laws: \\(A \\cap (B \\cup C) = (A \\cap B) \\cup (A \\cap C)\\)</p> <p>This comprehensive guide provides the essential notation for understanding set theory concepts in mathematical literature, from basic membership to advanced topics in modern set theory.</p>"},{"location":"Algorithms/Math/Arithmetic_Series_and_Gauss_Summation/","title":"Arithmetic Series","text":"<p>An arithmetic series is one in which the difference between any two consecutive terms is constant.</p>"},{"location":"Algorithms/Math/Arithmetic_Series_and_Gauss_Summation/#gauss-summation","title":"Gauss' Summation","text":"<p>As the story goes, when Gauss was a schoolboy his teacher gave the class a task of adding the numbers 1..100 and that is when Gauss came up with this insight :</p> <pre><code>#terms for 3 and 4 are ommitted, but you get the picture..\n\n1 2 3 4 5 6 7 8 9 10\n|_|_____|_|_____|__| \n  |   1+10=11   |\n  |_____|_|_____|\n      2+9=11\n        |_|\n      5+6=11\n</code></pre> <p>There are 10//2 11s in this sum. So the sum for first 10 numbers is <code>10//2 * (10+1)</code>.</p> <p>For odd numbers we can compe with the same formula through a different route.Reverse the series and add each individual term of reverserse serioes to the original series :</p> <p><pre><code>1   2  3  4  5  6  7  8  9  10  11\n11 10  9  8  7  6  5  4  3  2   1\n----------------------------------\n12 12  12 12 12 12 12 12 12 12 12\n</code></pre> We are multiplying n+1,  n times.  And since we multiplied the entire series sum by two by adding it to itsself, we finally divide by two. </p> <p>Therefore for 11 the series sum is <code>( (11+1)*11 ) // 2</code>.</p> <p>So, utlimately the formula is the same for both even and odd number of terms.</p> <p>More formally, this is the formula for the sum of the first \\( n \\) natural numbers:</p> \\[ S = 1 + 2 + 3 + \\cdots + n = \\frac{n(n+1)}{2} \\]"},{"location":"Algorithms/Math/Arithmetic_Series_and_Gauss_Summation/#generalizing-to-any-arithmetic-series","title":"Generalizing to Any Arithmetic Series","text":"<p>The formula for the sum of an arithmetic series can be applied to any sequence where the difference between consecutive terms is constant.</p> <p>Given an arithmetic series:</p> \\[ a_1,\\, a_2,\\, a_3,\\, \\ldots,\\, a_n \\] <p>where:</p> <ul> <li>\\( a_1 \\) is the first term,</li> <li>\\( d \\) is the common difference,</li> <li>\\( n \\) is the number of terms,</li> </ul> <p>the sum \\( S \\) of the series is:</p> \\[ S = \\frac{n}{2} \\times (a_1 + a_n) \\] <p>where \\( a_n \\) is the last term.</p> <p>Alternatively, since \\( a_n = a_1 + (n-1)d \\), the formula can also be written as:</p> \\[ S = \\frac{n}{2} \\times [2a_1 + (n-1)d] \\] <p>Example:</p> <p>For the series \\( 2,\\, 4,\\, 6,\\, 8 \\) :</p> <ul> <li>\\( a_1 = 2 \\)</li> <li>\\( d = 2 \\)</li> <li>\\( n = 4 \\)</li> <li>\\( a_n = 8 \\)</li> </ul> <p>Applying the formula:</p> \\[ S = \\frac{4}{2} \\times (2 + 8) = 2 \\times 10 = 20 \\] <p>This formula works for any arithmetic series, not just the sum of the first \\( n \\) natural numbers.</p>"},{"location":"Algorithms/Math/Arithmetic_Series_and_Gauss_Summation/#additional-resources","title":"Additional Resources","text":"<ol> <li>https://mathbitsnotebook.com/Algebra2/Sequences/SSGauss.html </li> </ol>"},{"location":"Algorithms/Math/Arithmetic_Series_and_Gauss_Summation/#practice","title":"Practice","text":"Arranging Coins <pre><code>\"\"\"\nSolution for : https://leetcode.com/problems/arranging-coins/description/\n\nRef : https://mathbitsnotebook.com/Algebra2/Sequences/SSGauss.html\n\n#Gauss summation\n1 + 2 + 3 + 4\n5 + 5  = (x+1)*x//2 = 4(5) //2 # x = 3\n1+ 2 + 3 + 4 + 5\n6 + 6 +3   = (6*5)//2 = 15 # x = 5        \n\nFind x which is closest to n using bisect left\n   Question : for this x is gauss summation less than n. \n\nlo could be equal to n -&gt; return lo\nelse return lo -1\n\"\"\"\nclass Solution:\n    @staticmethod\n    def is_less(x,n) :\n        \"\"\"\n        Is gauss sum less than n \n        \"\"\"\n        return ( (x*(x+1))//2 ) &lt; n\n\n    def arrangeCoins(self, n: int) -&gt; int:\n        lo,hi = 1, n+1\n        while lo &lt; hi :\n            mid = (lo+hi)//2\n            #print(lo,hi,mid)\n            if self.is_less(mid,n) :\n                lo = mid+1\n            else :  \n                hi = mid \n        if lo*(lo+1)//2 == n :\n            return lo\n        else :\n            return lo-1 \n</code></pre>"},{"location":"Algorithms/Math/Fundamental_Theorem_Of_Arithmetic/Introduction/","title":"Fundamental Theorem of Arithmetic","text":"<p>Also called the unique Factorization Theorem.</p> <p>Unique Factoprization Theorem</p> <p>The Fundamental Theorem of Arithmetic states that every integer greater than 1 has a unique prime factorization.</p> <p>Additional Resources :</p> <ol> <li>Proof explained by Bruce Ikenaga</li> <li>Flexbooks - Prime Factorization</li> <li>Wikipedia Article</li> </ol>"},{"location":"Algorithms/Math/Fundamental_Theorem_Of_Arithmetic/claude_ugly_number_analysis/","title":"The Ugly Number Problem","text":"<p>This is a Conversation with Claude 4.0 about the solution to the Ugly Number Problem.</p>"},{"location":"Algorithms/Math/Fundamental_Theorem_Of_Arithmetic/claude_ugly_number_analysis/#prompt","title":"PROMPT","text":"<p>Can you add mathematical expressions for this explanation : </p> <p>Solution for : https://leetcode.com/problems/ugly-number/</p> <p>Problem Statement: An ugly number is a positive integer which does not have a prime factor other than 2, 3, and 5.Given an integer n, return true if n is an ugly number.</p> <p>This is a great number theory problem, The fundamental theorem of arithmetic, also callled the prime factorization theorem states that any number greater than 1 is either a prime number or can be expressed as a product of prime numbers (ignoring the order).</p> <p>12 = 2 * 2 * 3 = 2**2 * 3</p> <p>210 = 2 * 3 * 5 * 7</p> <p>By definition of ugnliness as stated in the problem, 12 is  ugly but 210 is not because it has 7 as a factor. To test ugliness, all you have to do is remove all the factors of 2,3 and 5  and assert that only 1 remains.</p> <p>But, what does it mean to remove, say ONE factor of 2 from some number x. It means, that the number x has is made up by adding some other number twice x = x/2 + x/2. We can test this by checking if 2 divides x evenly i.e. x%2 = 0 Removing one factor of two means removing one copy of x/2 from x. We keep doing this until x%2 != 0, to remove all factors of 2. Of the remaining number y, it can be said that there is no other number which if added to itself will yield y.</p> <p>Similarly, for 3, after removing all factors of 3 from x,  of the remaining number y, it can be said that there is no other number which if added to itself thrice will yield y.</p>"},{"location":"Algorithms/Math/Fundamental_Theorem_Of_Arithmetic/claude_ugly_number_analysis/#response","title":"RESPONSE","text":"<p>Here's the solution with mathematical expressions added to make the concepts clearer:</p>"},{"location":"Algorithms/Math/Fundamental_Theorem_Of_Arithmetic/claude_ugly_number_analysis/#solution-for-ugly-number-problem","title":"Solution for Ugly Number Problem","text":"<p>Problem Statement: An ugly number is a positive integer which does not have a prime factor other than 2, 3, and 5.</p>"},{"location":"Algorithms/Math/Fundamental_Theorem_Of_Arithmetic/claude_ugly_number_analysis/#mathematical-foundation","title":"Mathematical Foundation","text":"<p>The Fundamental Theorem of Arithmetic states that every integer <code>n &gt; 1</code> can be uniquely expressed as:</p> <pre><code>n = p\u2081^a\u2081 \u00d7 p\u2082^a\u2082 \u00d7 p\u2083^a\u2083 \u00d7 ... \u00d7 p\u2096^a\u2096\n</code></pre> <p>where <code>p\u2081, p\u2082, ..., p\u2096</code> are distinct prime numbers and <code>a\u2081, a\u2082, ..., a\u2096</code> are positive integers.</p> <p>Examples: - <code>12 = 2\u00b2 \u00d7 3\u00b9</code> (ugly - only primes 2 and 3) - <code>210 = 2\u00b9 \u00d7 3\u00b9 \u00d7 5\u00b9 \u00d7 7\u00b9</code> (not ugly - contains prime 7)</p>"},{"location":"Algorithms/Math/Fundamental_Theorem_Of_Arithmetic/claude_ugly_number_analysis/#definition-of-ugliness","title":"Definition of Ugliness","text":"<p>A number <code>n</code> is ugly if and only if: <pre><code>n = 2^a \u00d7 3^b \u00d7 5^c\n</code></pre> where <code>a, b, c \u2265 0</code> are non-negative integers.</p>"},{"location":"Algorithms/Math/Fundamental_Theorem_Of_Arithmetic/claude_ugly_number_analysis/#algorithm-factor-removal","title":"Algorithm: Factor Removal","text":"<p>To test ugliness, we systematically remove all factors of 2, 3, and 5:</p> <p>Step 1: Remove all factors of 2 <pre><code>While n \u2261 0 (mod 2):\n    n \u2190 n/2\n</code></pre> This continues until <code>n \u2262 0 (mod 2)</code>, meaning no more factors of 2 remain.</p> <p>Step 2: Remove all factors of 3 <pre><code>While n \u2261 0 (mod 3):\n    n \u2190 n/3\n</code></pre></p> <p>Step 3: Remove all factors of 5 <pre><code>While n \u2261 0 (mod 5):\n    n \u2190 n/5\n</code></pre></p>"},{"location":"Algorithms/Math/Fundamental_Theorem_Of_Arithmetic/claude_ugly_number_analysis/#mathematical-proof-of-correctness","title":"Mathematical Proof of Correctness","text":"<p>After removing all factors of 2, 3, and 5, we have: <pre><code>n_final = n_original / (2^a \u00d7 3^b \u00d7 5^c)\n</code></pre></p> <p>Case 1: If <code>n_original</code> was ugly, then <code>n_original = 2^a \u00d7 3^b \u00d7 5^c</code>, so: <pre><code>n_final = (2^a \u00d7 3^b \u00d7 5^c) / (2^a \u00d7 3^b \u00d7 5^c) = 1\n</code></pre></p> <p>Case 2: If <code>n_original</code> was not ugly, then <code>n_original = 2^a \u00d7 3^b \u00d7 5^c \u00d7 p\u2081^d\u2081 \u00d7 ... \u00d7 p\u2096^d\u2096</code> where <code>p\u1d62 \u2209 {2,3,5}</code>, so: <pre><code>n_final = (2^a \u00d7 3^b \u00d7 5^c \u00d7 p\u2081^d\u2081 \u00d7 ... \u00d7 p\u2096^d\u2096) / (2^a \u00d7 3^b \u00d7 5^c) = p\u2081^d\u2081 \u00d7 ... \u00d7 p\u2096^d\u2096 &gt; 1\n</code></pre></p>"},{"location":"Algorithms/Math/Fundamental_Theorem_Of_Arithmetic/claude_ugly_number_analysis/#modular-arithmetic-interpretation","title":"Modular Arithmetic Interpretation","text":"<p>When we check <code>n % 2 == 0</code>, we're testing the congruence: <pre><code>n \u2261 0 (mod 2)\n</code></pre></p> <p>This is equivalent to asking: \"Does there exist an integer <code>k</code> such that <code>n = 2k</code>?\"</p> <p>If yes, then 2 is a factor, and we can \"remove\" it by setting <code>n \u2190 k = n/2</code>.</p>"},{"location":"Algorithms/Math/Fundamental_Theorem_Of_Arithmetic/claude_ugly_number_analysis/#implementation","title":"Implementation","text":"<pre><code>def isUgly(n):\n    \"\"\"\n    Determines if n is an ugly number.\n\n    Mathematical approach: n is ugly iff n = 2^a \u00d7 3^b \u00d7 5^c for some a,b,c \u2265 0\n\n    Args:\n        n: integer to test\n\n    Returns:\n        bool: True if n is ugly, False otherwise\n    \"\"\"\n    # Base case: ugly numbers must be positive\n    if n &lt;= 0:\n        return False\n\n    # Remove all factors of 2: n \u2190 n/2 while n \u2261 0 (mod 2)\n    while n % 2 == 0:\n        n //= 2\n\n    # Remove all factors of 3: n \u2190 n/3 while n \u2261 0 (mod 3)\n    while n % 3 == 0:\n        n //= 3\n\n    # Remove all factors of 5: n \u2190 n/5 while n \u2261 0 (mod 5)\n    while n % 5 == 0:\n        n //= 5\n\n    # If n = 2^a \u00d7 3^b \u00d7 5^c originally, then n = 1 now\n    # If n had other prime factors, then n &gt; 1 now\n    return n == 1\n</code></pre>"},{"location":"Algorithms/Math/Fundamental_Theorem_Of_Arithmetic/claude_ugly_number_analysis/#implementation-time-and-space-complexity","title":"Implementation### Time and Space Complexity","text":"<p>Time Complexity: <code>O(log n)</code></p> <ul> <li>In the worst case, we divide by 2 approximately <code>log\u2082(n)</code> times</li> <li>Similarly for factors of 3 and 5</li> <li>Total: <code>O(log\u2082(n) + log\u2083(n) + log\u2085(n)) = O(log n)</code></li> </ul> <p>Space Complexity: <code>O(1)</code></p> <ul> <li>Only using a constant amount of extra space</li> </ul> <p>The mathematical foundation shows that this algorithm correctly identifies ugly numbers by leveraging the unique prime factorization and systematically removing the allowed prime factors through modular arithmetic operations.</p>"},{"location":"Algorithms/Math/Fundamental_Theorem_Of_Arithmetic/claude_ugly_number_analysis/#prompt_1","title":"PROMPT","text":"<p>Explain this notation :n \u2190 n/5 while n \u2261 0 (mod 5)</p>"},{"location":"Algorithms/Math/Fundamental_Theorem_Of_Arithmetic/claude_ugly_number_analysis/#response_1","title":"RESPONSE","text":"<p>Excellent! Let me break down this chain of mathematical equivalences step by step:</p>"},{"location":"Algorithms/Math/Fundamental_Theorem_Of_Arithmetic/claude_ugly_number_analysis/#the-triple-equivalence","title":"The Triple Equivalence","text":"<pre><code>n \u2261 0 (mod 5) \u27fa n % 5 == 0 \u27fa \u2203k \u2208 \u2124 such that n = 5k\n</code></pre> <p>The symbol <code>\u27fa</code> means \"if and only if\" - these three statements are completely equivalent and mean exactly the same thing.</p>"},{"location":"Algorithms/Math/Fundamental_Theorem_Of_Arithmetic/claude_ugly_number_analysis/#statement-1-n-0-mod-5","title":"Statement 1: <code>n \u2261 0 (mod 5)</code>","text":"<p>Reading: \"n is congruent to 0 modulo 5\"</p> <p>Mathematical Definition: Two integers are congruent modulo m if they have the same remainder when divided by m.</p> <p>Since we're comparing n to 0: - <code>n \u2261 0 (mod 5)</code> means n and 0 have the same remainder when divided by 5 - The remainder when 0 is divided by 5 is 0 - Therefore: n has remainder 0 when divided by 5</p>"},{"location":"Algorithms/Math/Fundamental_Theorem_Of_Arithmetic/claude_ugly_number_analysis/#statement-2-n-5-0","title":"Statement 2: <code>n % 5 == 0</code>","text":"<p>Reading: \"n modulo 5 equals 0\" or \"the remainder when n is divided by 5 is 0\"</p> <p>Programming Context: This is how we write modular arithmetic in most programming languages.</p> <p>Examples: - <code>10 % 5 = 0</code> \u2713 - <code>15 % 5 = 0</code> \u2713 - <code>17 % 5 = 2</code> \u2717 - <code>23 % 5 = 3</code> \u2717</p>"},{"location":"Algorithms/Math/Fundamental_Theorem_Of_Arithmetic/claude_ugly_number_analysis/#statement-3-k-z-such-that-n-5k","title":"Statement 3: <code>\u2203k \u2208 \u2124 such that n = 5k</code>","text":"<p>Reading: \"There exists an integer k such that n equals 5 times k\"</p> <p>Symbol Breakdown:</p> <ul> <li><code>\u2203</code> = \"there exists\"</li> <li><code>k \u2208 \u2124</code> = \"k is an element of the integers\" (k is a whole number: ..., -2, -1, 0, 1, 2, ...)</li> <li><code>n = 5k</code> = \"n equals 5 times k\"</li> </ul> <p>Meaning: n can be written as some integer multiplied by 5 (i.e., n is a multiple of 5)</p>"},{"location":"Algorithms/Math/Fundamental_Theorem_Of_Arithmetic/claude_ugly_number_analysis/#why-these-are-all-equivalent","title":"Why These Are All Equivalent","text":"<p>Let me prove these equivalences with examples:</p>"},{"location":"Algorithms/Math/Fundamental_Theorem_Of_Arithmetic/claude_ugly_number_analysis/#example-1-n-20","title":"Example 1: n = 20","text":"<p>Statement 1: <code>20 \u2261 0 (mod 5)</code> - 20 \u00f7 5 = 4 remainder 0 \u2713</p> <p>Statement 2: <code>20 % 5 == 0</code> - 20 % 5 = 0 \u2713</p> <p>Statement 3: <code>\u2203k \u2208 \u2124 such that 20 = 5k</code> - k = 4 works: 20 = 5 \u00d7 4 \u2713</p>"},{"location":"Algorithms/Math/Fundamental_Theorem_Of_Arithmetic/claude_ugly_number_analysis/#example-2-n-17","title":"Example 2: n = 17","text":"<p>Statement 1: <code>17 \u2261 0 (mod 5)</code> - 17 \u00f7 5 = 3 remainder 2 \u2260 0 \u2717</p> <p>Statement 2: <code>17 % 5 == 0</code> - 17 % 5 = 2 \u2260 0 \u2717</p> <p>Statement 3: <code>\u2203k \u2208 \u2124 such that 17 = 5k</code> - No integer k works: 17/5 = 3.4 (not an integer) \u2717</p>"},{"location":"Algorithms/Math/Fundamental_Theorem_Of_Arithmetic/claude_ugly_number_analysis/#the-mathematical-connection","title":"The Mathematical Connection","text":""},{"location":"Algorithms/Math/Fundamental_Theorem_Of_Arithmetic/claude_ugly_number_analysis/#division-algorithm","title":"Division Algorithm","text":"<p>For any integers n and 5, we can write: <pre><code>n = 5q + r    where 0 \u2264 r &lt; 5\n</code></pre> - q is the quotient - r is the remainder</p>"},{"location":"Algorithms/Math/Fundamental_Theorem_Of_Arithmetic/claude_ugly_number_analysis/#the-three-statements-are-saying","title":"The Three Statements Are Saying:","text":"<ol> <li>Modular: The remainder r equals 0</li> <li>Programming: The remainder operation gives 0  </li> <li>Algebraic: We can write n as 5 times some integer (when r = 0, then n = 5q)</li> </ol>"},{"location":"Algorithms/Math/Fundamental_Theorem_Of_Arithmetic/claude_ugly_number_analysis/#practical-examples","title":"Practical Examples","text":"n n \u00f7 5 Quotient (q) Remainder (r) n \u2261 0 (mod 5)? n % 5 k exists? n = 5k 0 0 \u00f7 5 0 0 \u2713 0 \u2713 (k=0) 0 = 5\u00d70 5 5 \u00f7 5 1 0 \u2713 0 \u2713 (k=1) 5 = 5\u00d71 10 10 \u00f7 5 2 0 \u2713 0 \u2713 (k=2) 10 = 5\u00d72 12 12 \u00f7 5 2 2 \u2717 2 \u2717 12 \u2260 5k -15 -15 \u00f7 5 -3 0 \u2713 0 \u2713 (k=-3) -15 = 5\u00d7(-3)"},{"location":"Algorithms/Math/Fundamental_Theorem_Of_Arithmetic/claude_ugly_number_analysis/#key-insight","title":"Key Insight","text":"<p>All three expressions are different ways of asking the same fundamental question:</p> <p>\"Is n a multiple of 5?\"</p> <ul> <li>Modular arithmetic asks: \"Does n have remainder 0 when divided by 5?\"</li> <li>Programming asks: \"Does the modulo operation return 0?\"  </li> <li>Algebraic asks: \"Can we write n as 5 times some integer?\"</li> </ul> <p>They're all equivalent ways of expressing divisibility!</p>"},{"location":"Algorithms/Merge_Sort/","title":"Merge Sort","text":""},{"location":"Algorithms/Merge_Sort/#sorting-an-array","title":"Sorting an array","text":"<p>tl;dr</p> <p>Think of Merge Sort as a binary recursive application of 2-way merge where the left and right recursive calls sort the left and right subarrays and we merge the two halves in the post order position.  </p> <p>Thats it.</p> <p>The actual sorting is a result of the merge function applied in a bottom up fashion from smallest subarray lengths.The actual sorting is a result of the merge function applied in a bottom up fashion from smallest subarray lengths.</p> <p>As for the implementation details :</p> <p>Instead of passing arrays to the merge method, pass pointers. Three pointers are sufficient : lo , mid and hi. The subarrays to be merged are always adjacent: arr[lo...mid] and arr[mid+1...hi]</p> <p>Base case is hit when lo = hi . A subarray of size one cannot be divided an further and is sorted by definition.</p> <p>Merging cannot be done in place. So merge sort does require extra space. Instead of allocating extra temp space in each recursive call, just allocate extra space as external variable, equal to length of array. This temp space can be used to perform 2-way merge using the lo, mid and hi pointers passed to merge method. </p> Code Merge Sort on Array <pre><code>\"\"\"\nMerge Sort Solution for https://leetcode.com/problems/sort-an-array/description/\n\"\"\"\nclass Solution:\n    def merge(self, lo,mid,hi) :\n        \"\"\"\n        Merge self.nums[lo:mid+1] and self.nums[mid+1,hi+1] into self.temp[lo:hi+1] \n        Write back self.temp[lo:hi+1] into self.nums[lo:hi+1]\n        \"\"\"\n        print(\"merge \" ,lo,mid,hi)\n        for i in range(lo,hi+1) :\n            self.temp[i] = self.nums[i]\n\n        p1 = lo \n        p2 = mid+1\n        p3 = lo \n\n        while p1 &lt;= mid and p2 &lt;= hi :\n            if self.temp[p1] &lt; self.temp[p2] :\n                self.nums[p3] =  self.temp[p1]\n                p1+=1\n            else : \n                self.nums[p3] =  self.temp[p2]\n                p2+=1\n            p3+=1\n        while p1 &lt;= mid :\n            self.nums[p3] = self.temp[p1]\n            p1+=1\n            p3+=1\n\n        while p2 &lt;= hi :\n            self.nums[p3] = self.temp[p2]\n            p2+=1\n            p3+=1\n\n    def sort(self, lo,hi) :\n        \"\"\"\n        DRY RUN UNTIL FIRST BASE CASE IS HIT :\n\n         0           2            5  \n        [5,    1,    1,  2,  0  ,0]\n        lo           m           hi\n        lo     m     hi\n        lomhi \n        ^BASE CASE HIT  \n        \"\"\"\n        if lo &gt;=  hi  :\n            return\n        # print(self.nums, self.temp)\n        mid = (lo+hi)//2\n        self.sort(lo,mid)\n        self.sort(mid+1,hi)\n        self.merge(lo,mid,hi)\n\n    def sortArray(self, nums: List[int]) -&gt; List[int]:\n        #For writing intermediate merge results \n        self.temp = [0]*len(nums)\n        #For accessing array which needs to be sorted\n        self.nums = nums\n        self.sort(0,len(nums)-1)\n        return self.nums\n</code></pre>"},{"location":"Algorithms/Merge_Sort/#sorting-a-linked-list","title":"Sorting a linked list","text":"<p>Sorting a linked list is easier. We can actually break up the linked list in two halves in each recursive call. Sort the two halves and finally merge them. The base case is when the linked list is empty or only has a single element.</p> <p>Pay attention to the difference in the recursive implementation for array and linked list.</p> <p>For arrays, we pass pointers to the merge function to denote subarrays to be merged. For linked lists, we actually split the list and pass two separate lists.</p> Code Merge Sort on Linked List <pre><code>\"\"\"\nSolution for : https://leetcode.com/problems/sort-list/description/\nRecursively  :\n  Split the list in two halves .\n  Sort first half\n  Sort second half\n  Merge the two SORTED halves.\n\"\"\"\n# Definition for singly-linked list.\n class ListNode:\n     def __init__(self, val=0, next=None):\n         self.val = val\n         self.next = next\n\nfrom typing import Optional\n\nclass Solution:\n    def split_list(self, head: Optional[ListNode]) -&gt; (Optional[ListNode],Optional[ListNode]) :\n        \"\"\"\n        Splitting the list into two :\n        dummy-&gt;1-&gt; 2 -&gt;3-&gt;None\n            sf\n               s   f\n                   s      f  \n        s.next -&gt; second half  # s.next = None\n        dummy.next --&gt; first half\n        \"\"\"\n        dummy = ListNode(0)\n        dummy.next = head\n        s,f = dummy,dummy \n        while f and f.next :\n            f = f.next.next\n            s = s.next\n        #slow pointer stops at the PREVIOUS NODE of second half\n        second_half = s.next\n        s.next = None\n        first_half = dummy.next\n        # print(\"first half\" , first_half)\n        # print(\"second half\", second_half)\n        return first_half,second_half\n\n    def merge(self,list1 : Optional[ListNode] , list2 : Optional[ListNode]) -&gt; Optional[ListNode] : \n        p1, p2  = list1,list2\n        # print(\"merging\", p1, \" and \", p2)\n        dummy = ListNode(0)\n        p3 = dummy\n\n        while p1 and p2 :\n            if p1.val &lt; p2.val:\n                p3.next = p1\n                p1 = p1.next\n            else: \n                p3.next = p2\n                p2 = p2.next\n            p3 = p3.next\n\n        if p1 :\n            p3.next = p1\n        if p2 :\n            p3.next = p2\n        # print(\"merged\" , dummy.next)\n        return dummy.next \n\n    def _sort(self,head) :\n        if not head or not head.next :\n            return head\n        #We first SPLIT the list halves\n        first_half, second_half = self.split_list(head)\n        #Then we SORT both halves\n        #Sorting is not in place\n        #So, you have to pass the sorted lists to merge\n        sorted_first_half = self._sort(first_half)\n        sorted_second_half = self._sort(second_half)\n        #Merge the sorted halves\n        return self.merge(sorted_first_half,sorted_second_half)\n\n\n    def sortList(self, head: Optional[ListNode]) -&gt; Optional[ListNode]:\n        return self._sort(head)\n</code></pre>"},{"location":"Algorithms/Merge_Sort/#important-things-to-note","title":"Important things to note","text":"<p>As we saw earlier, quicksort is just an application of array partitioning where we can think of a partition happening at the preorder position in the traversal of a binary tree. Similarly : </p> <p>tl;dr (Again!)</p> <p>Merge sort is an application of 2-way merge where we can think of a merge happening in the post order position of a binary tree traversal.</p>"},{"location":"Algorithms/Misc_Array_Problems/Copying_and_allocating/","title":"Allocate, Copy and Transpose 2D Matrix","text":"<p>Avoid shallow copy bugs and stick to list comprehensions when copying and allocating 2D arrays.</p>"},{"location":"Algorithms/Misc_Array_Problems/Copying_and_allocating/#allocating-a-2d-array","title":"Allocating a 2D array","text":"<p>Allocate target Matrix of num_rows* num_cols intialized to 0 </p> <p>Allocate a 2D matrix</p> <p>target = [ [0]*num_cols for _ in range(num_rows) ]</p>"},{"location":"Algorithms/Misc_Array_Problems/Copying_and_allocating/#copying-a-2d-matrix","title":"Copying a 2D Matrix","text":"<p>Create copy of 2D array</p> <pre><code>class Solution:\n  def get_copy(self,matrix: List[List[int]] ) -&gt; List[List[int]]  : \n      num_rows, num_cols = len(matrix), len(matrix[0])\n      # Using Nested List Comprehension.\n      copy = [[matrix[r][c] for c in range(num_cols)] for r in range(num_rows)]\n      return copy\n</code></pre>"},{"location":"Algorithms/Misc_Array_Problems/Copying_and_allocating/#transposing-a-2d-matrix","title":"Transposing a 2D matrix","text":"<p>In a transposed matrix, the rows become columns and columns become rows. Which means :</p> <p><code>src[r][c] == target[c][r]</code> </p> <p>OR</p> <p><code>target[r][c] == src[c][r]</code></p> <p>Transpose of a matrix</p> <pre><code>\"\"\"\nSolution for : https://leetcode.com/problems/transpose-matrix/\nYou could alternately preallocate an n*m target matrix and copy over each element from src to target matrix using a nested loop over the original matrix.\n\"\"\"\nclass Solution:\n    def transpose(self, matrix: List[List[int]]) -&gt; List[List[int]]:\n        num_rows = len(matrix)\n        num_cols = len(matrix[0])\n        # Using Nested List Comprehension. Number of Columns equal number of rows.\n        # Remember : target[r][c] = src[c][r]\n        transpose = [[matrix[c][r] for c in range(num_rows)] for r in range(num_cols)]\n        return transpose\n</code></pre>"},{"location":"Algorithms/Misc_Array_Problems/Diagonal_and_Antidiagonal/","title":"Diagonal and Antidiagonal traversal of Square Matrix","text":"<p>The important thing to note here is that the diagonal and antidiagonal overlap only in cases when the square matrix has an odd number of rows/columns and the overlapping cell will always have the same row and column index.</p> <p>Another thing to not is that the number of elements in the diagonal and antidiagonal of an n*n matrix is the same as number of rows/columns.</p> <p>As for the diagonal traversal it is easy:</p> <pre><code>Iterate n times :\n   start from [0,0] and keep adding one to both r and c.\n</code></pre> <p>For the antidiagonal, if you start from the bottom left,  the column index remains the same as that of the diagonal and the row index is n-1 - row_index of diagonal.</p> <p>So, we can iterate over the same antidiagonal:</p> <pre><code>Iterate n times :\n   start from [n-1,0] and keep adding one to c and subtracting 1 from r.\n</code></pre> <p>Of course, we only need a single loop to iterate over both. Just take care of the overlapping cell in case you are doing arithmetic on the elements.</p>"},{"location":"Algorithms/Misc_Array_Problems/Diagonal_and_Antidiagonal/#practice","title":"Practice","text":"Matrix Diagonal Sum <pre><code>\"\"\"\nSolution for : https://leetcode.com/problems/matrix-diagonal-sum/\n\nEyeball this and you will get the pattern :\n\n00 01 02\n10 11 12\n20 21 22\n\"\"\"\nclass Solution:\n    def diagonalSum(self, mat: List[List[int]]) -&gt; int:\n        total = 0\n        n = len(mat) \n        for i in range(len(mat)) :\n            total+=mat[i][i] \n            # Do not count if this is an overlapping cell\n            if (n-1) - i == i : continue\n            #Traverse Anti Diagonal from bottom left to top right\n            total+=mat[(n-1)-i][i]\n        return total \n</code></pre>"},{"location":"Algorithms/Misc_Array_Problems/In_Place_Transpose_of_a_Square_Matrix/","title":"In Place Transpose of a Square Matrix","text":"<p>A square matrix can be transposed in place by just swapping elements along the diagonals. The trick is to traverse only one side of the diagonal.</p> <p>The code only iterates over the lower triangle ( lower half over the diagonal excluding the diagonal itself). When you iterate over columns, iterate over [o,curr_row).</p> <p>For upper triangle iterate over [curr_row+1,n) </p> <pre><code>[\n  00  01   02  03\n *10  11   12  13\n *20 *21   22  23\n *30 *31  *32  33\n]\n</code></pre> Transposing a square matrix <pre><code>def transpose(matrix) :\n    \"\"\"\n    Transpose an n*n matrix\n    \"\"\"\n    n = len(matrix)\n    try :\n        assert n == len(matrix[0])\n    except AssertionError :\n        print(\"Not a square matrix\")\n        raise\n\n    for r in range(n) :\n        #for c in range(r+1, n) : # Iterate over the upper triangle\n        for c in range(r) : # Iterate over the lower triangle\n            print(f\"Swapping ({r},{c}) with ({c},{r})\")\n            matrix[r][c], matrix[c][r] = matrix[c][r],matrix[r][c]\n\n    return matrix\n\nif __name__ == \"__main__\" :\n    matrix = [\n        [1, 2, 3],\n        [4, 5, 6],\n        [7, 8, 9]\n    ]\n\n    print(\"Original Matrix:\")\n    for row in matrix:\n        print(row)\n\n    transposed_matrix = transpose(matrix)\n    print(\"\\nTransposed Matrix:\")\n    for row in transposed_matrix:\n        print(row)\n</code></pre> <p>If the matrix is not square, then for an input array of m*n, you will have to allocate a separate target array of dimensions n*m and copy over each element to the target array : <code>src[r][c] = target[c][r]</code></p>"},{"location":"Algorithms/Quickselect/","title":"QuickSelect","text":""},{"location":"Algorithms/Quickselect/#defining-the-problem","title":"Defining the problem.","text":"<p>Given an array, find the sorted value in non decreasing order at the kth index (Lets assume indices are counted from zero). </p> <p>In other words what would be the value at the kth index if the array were sorted in non decreasing order?</p> <p>This is also called the kth order statistic.</p>"},{"location":"Algorithms/Quickselect/#the-algorithm","title":"The Algorithm","text":"<p>The core of the quickselect algorithm is the partitioning scheme. There are different ways of implementing quickselet based on whether the partitioning scheme puts the pivot in its sorted position or not. Here we will consider only the implementation which uses partition schemes which do, because this is simpler and more intuitive.</p> <p>Assume, you have a partitioning scheme which returns some partition index / pivot index s.t. the value at the pivot index is in its sorted position. The quickeselect algorithm makes one recursive call based on the relative position of k to the pivot index. The base case is when the pivot index is k.</p> <p>Algorithm</p> <pre><code>def quickselect(arr,lo,hi, k ) :\n    p = partition(arr,lo,hi)\n    if p == k :\n       return arr[k]\n    elif  k&lt; p :\n       return quickselect(arr,lo,p-1,k)\n    else :\n       return quickselect(arr,p+1,hi,k)\n</code></pre> <p>Here is the implementation using Hoare partitionining.Notice the difference that random pivot selection makes in the comments :</p> Code Hoare Partition with Weak Condition and fixed pivotHoare Partition with Weak Condition and random pivot <pre><code>        from typing import List\n        import random\n\n        def quickselect(arr: List[int],lo: int,hi: int,k : int) -&gt; int :\n            \"\"\"\n            Quickselect using Hoare partition with a weak condition and fixed pivot. \n\n            This is the implementation which you will find in textbooks using do..while style loops which in python becomes while True loops.\n            I prefer avoiding white True loops. \n            It's still using Hoare partition with a weak condition. But the pivot selection is not random.\n            Notice how the performance degrades to O(n^2) for sorted input array because the pivot selection is not random.\n            Compare this with next version which uses a random pivot.\n            \"\"\"\n            v = arr[lo]\n            i = lo\n            j = hi+1\n            while True:\n                while True:\n                    i += 1\n                    if not (i &lt; hi and arr[i] &lt; v):\n                        break\n                while True:\n                    j -= 1\n                    if not (j &gt; lo and arr[j] &gt; v):\n                        break\n                if i &gt;= j:\n                    break\n                arr[i], arr[j] = arr[j], arr[i]\n            arr[lo], arr[j] = arr[j], arr[lo]\n\n            print(f\"Pivot Value: {v}  ,Partitioned list : {arr}, Partition Index = {j} , le = {lo}, hi = {hi}, k = {1}\")\n            if k -1  == j :\n                print(f\"Returning index {j}\")\n                return nums[j]\n            elif k -1 &gt; j : \n                return quickselect(nums,j+1,hi,k)\n            else :\n                return quickselect(nums,lo,j-1,k)\n\n        if __name__ == \"__main__\" :\n            # nums = [5,2,1,1,1,1,1,1,1,1,1,5,5,-3,1,1,1,1,1,1,1,1,1,1,1-2,-5]               # O(nlogn)\n            nums = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]  # O(nlogn)\n            # nums = [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24]  #degrades to O(n^2)\n            # nums = [24,23,22,21,20,19,18,17,16,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1]  #O(nlogn)\n            print( \n                   f\"kth_largest = { quickselect(nums, 0, len(nums)-1 ,23) }\" \n                  )\n\n            # nums = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1] # O(nlogn)\n            # print( \n            #        f\"kth_largest = { quickselect(nums, 0, len(nums)-1 ,1) }\" \n            #       )\n</code></pre> <pre><code>         from typing import List\n         import random\n\n\n         def hoare_partition(arr, lo, hi):\n             \"\"\"\n             Entropy Optimal Hoare partition.\n             Produces balanced partitions when there are large number of duplicates.\n             [lo,right] contains elements less than or equal to pivot.\n             (right,hi]  contains elements greater than or equal to pivot.\n             Invariants for the while loop :\n             [lo] has pivot  \n             [lo, left)  &lt;= pivot    #Has values &lt;= pivot\n             (right, hi] &gt;= pivot\n             [pivot|--- &lt;=pivot-----|-----Undetermined-------|---&gt;=pivot----]\n                                     left               right\n             After execution of while loop : \n             [pivot|----&lt;=pivot------|-----&gt;=pivot------]\n             lo                  right                  hi\n             [lo,right] &lt;= pivot\n             After Putting pivot in sorted position : \n             [----&lt;=pivot----|pivot|-----&gt;=pivot------]\n             lo               right                  hi\n             Finally, return right.\n             \"\"\"\n             #PIVOT SELECTION\n             #Pick a random pivot index and always keep pivot at index lo\n             #NB:  random.randint(0,0) is 0.\n             pivot_index = random.randint(lo,hi)\n             arr[lo],arr[pivot_index] =  arr[pivot_index],arr[lo]\n             #read pivot value\n             pivot = arr[lo]\n\n             #PARTITIONING\n             #partition [lo+1,hi] ; \n             #NB : when lo == hi , while loop will not be executed\n             left,right = lo+1, hi\n             while left&lt;=right:\n                 #Move left ahead if arr[left] is strictly less than pivot value\n                 while left &lt;= right and arr[left] &lt; pivot :\n                     left+=1\n                 #Move right to the left if it is strictly higher than pivot\n                 while left &lt;= right  and arr[right] &gt; pivot :\n                     right-=1\n                 #Swap left and right and move pointers\n                 #If both values are equal to pivot this will do a swap,move pointers and effectively leave pivot values where they are. \n                 if left &lt;=  right :\n                     arr[left], arr[right] = arr[right], arr[left]\n                     right-=1\n                     left+=1\n             #Put pivot in sorted position\n             arr[lo], arr[right] = arr[right], arr[lo]\n             return right\n\n         def quickselect(arr: List[int],lo: int,hi: int,k : int) -&gt; int :\n             \"\"\"\n             Quicselect using Hoare partition with weak condition and random pivot.\n             Return vlaue at Kth SMALLEST Index,\n             Returns the non decreasingly sorted value at kth Index with indices starting from lo. [lo ,hi] is inclusive.\n             This uses Hoare's partition scheme with a weak condition and which also puts pivot in its sorted position.\n             \"\"\" \n             pivot_index = hoare_partition(arr,lo,hi)\n             print(f\"Pivot Value: {arr[pivot_index]}  ,Partitioned list : {arr}, Partition Index = {pivot_index} , le = {lo}, hi = {hi}, k = {k}\")\n             if k == pivot_index :\n                 return arr[k]\n             if k &lt; pivot_index :\n                 return quickselect(arr,lo, pivot_index-1,k)\n             elif k &gt; pivot_index :  \n                  return quickselect(arr,pivot_index+1,hi,k)\n\n\n\n         if __name__ == \"__main__\" :\n             # nums = [5,2,1,1,1,1,1,1,1,1,1,5,5,-3,1,1,1,1,1,1,1,1,1,1,1-2,-5]                 # O(nlogn)\n             # nums = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]  # Close to O(nlogn)\n             nums = [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24]  #STill close to O(nlogn) . WIthout random pivot degrades to O(n^2)\n             # nums = [24,23,22,21,20,19,18,17,16,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1]  #O(nlogn)\n             print( \n                    f\"kth_largest = { quickselect(nums, 0, len(nums)-1 ,23) }\" \n                   )\n</code></pre>"},{"location":"Algorithms/Quickselect/#additional-resources","title":"Additional Resources","text":"<ol> <li>https://en.wikipedia.org/wiki/Median_of_medians</li> </ol>"},{"location":"Algorithms/Quicksort/","title":"QuickSort","text":""},{"location":"Algorithms/Quicksort/#defining-the-problem","title":"Defining the problem.","text":"<p>Given an array, sort it in non decreasing order.</p>"},{"location":"Algorithms/Quicksort/#the-algorithm","title":"The Algorithm","text":"<p>Once you understand partitioning, quickselect and quicksort are just recursive applications of it. In quickselect, we make a single recursive call, while in quicksort, we make two recursive calls to eventually put all elements in their sorted position. Quick Sort can be visualized as a preorder traversal of a binary tree, where in the preorder position, you call the partitioning function and recursively call quicksort on both partitions. The base case is when lo &gt;= hi.</p> <p>Assume, you have a partitioning scheme which returns some partition index / pivot index s.t. the value at the pivot index is in its sorted position.</p> <p>The algorithm is :</p> <p>Algorithm</p> <pre><code>def quicksort(arr,lo,hi, k ) :\n    if lo &gt;= hi  :\n       return \n    p = partition(arr,lo,hi)\n    quicksort(arr,lo,p-1,k)\n    quicksort(arr,p+1,hi,k)\n</code></pre> <p>Implementation using Hoare Partitioning :</p> Code Hoare Partition with Weak Condition and random pivot <pre><code>         from typing import List\n         import random\n\n\n         def hoare_partition(arr, lo, hi):\n             \"\"\"\n             Entropy Optimal Hoare partition.\n             Produces balanced partitions when there are large number of duplicates.\n             [lo,right] contains elements less than or equal to pivot.\n             (right,hi]  contains elements greater than or equal to pivot.\n             Invariants for the while loop :\n             [lo] has pivot  \n             [lo, left)  &lt;= pivot    #Has values &lt;= pivot\n             (right, hi] &gt;= pivot\n             [pivot|--- &lt;=pivot-----|-----Undetermined-------|---&gt;=pivot----]\n                                     left               right\n             After execution of while loop : \n             [pivot|----&lt;=pivot------|-----&gt;=pivot------]\n             lo                  right                  hi\n             [lo,right] &lt;= pivot\n             After Putting pivot in sorted position : \n             [----&lt;=pivot----|pivot|-----&gt;=pivot------]\n             lo               right                  hi\n             Finally, return right.\n             \"\"\"\n             #PIVOT SELECTION\n             #Pick a random pivot index and always keep pivot at index lo\n             #NB:  random.randint(0,0) is 0.\n             pivot_index = random.randint(lo,hi)\n             arr[lo],arr[pivot_index] =  arr[pivot_index],arr[lo]\n             #read pivot value\n             pivot = arr[lo]\n\n             #PARTITIONING\n             #partition [lo+1,hi] ; \n             #NB : when lo == hi , while loop will not be executed\n             left,right = lo+1, hi\n             while left&lt;=right:\n                 #Move left ahead if arr[left] is strictly less than pivot value\n                 while left &lt;= right and arr[left] &lt; pivot :\n                     left+=1\n                 #Move right to the left if it is strictly higher than pivot\n                 while left &lt;= right  and arr[right] &gt; pivot :\n                     right-=1\n                 #Swap left and right and move pointers\n                 #If both values are equal to pivot this will do a swap,move pointers and effectively leave pivot values where they are. \n                 if left &lt;=  right :\n                     arr[left], arr[right] = arr[right], arr[left]\n                     right-=1\n                     left+=1\n             #Put pivot in sorted position\n             arr[lo], arr[right] = arr[right], arr[lo]\n             return right\n\n         def quicksort(arr: List[int],lo: int,hi: int) -&gt; int :\n             \"\"\"\n             Quicksort using Hoare partition with weak condition and random pivot.\n             This uses Hoare's partition scheme with a weak condition and which also puts pivot in its sorted position.\n             \"\"\" \n             if lo &gt;= hi :\n                 return\n             p = hoare_partition(arr,lo,hi)\n             quicksort(arr,lo, p-1)\n             quicksort(arr,p+1,hi)\n\n\n\n         if __name__ == \"__main__\" :\n             nums = [24,23,22,21,20,19,18,17,16,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1]\n             quicksort(nums,0,len(nums)-1)\n             print (nums)\n</code></pre>"},{"location":"Algorithms/Quicksort/#additional-references","title":"Additional References :","text":"<p>https://algs4.cs.princeton.edu/23quicksort/</p>"},{"location":"Algorithms/Two_Way_Merge/","title":"Merging Sorted Containers","text":"<p>Before considering K-way merge, let us first look at the simpler version of the problem.</p>"},{"location":"Algorithms/Two_Way_Merge/#the-2-way-merge-problem","title":"The 2-Way merge problem.","text":""},{"location":"Algorithms/Two_Way_Merge/#merging-two-sorted-arrays","title":"Merging two sorted arrays","text":"<p>Given 2 integer arrays, sorted in non decreasing order, merge them into a single sorted array. </p> Code Two Way Merge on ArraysUnit Tests for Two Merge on Arrays <pre><code>from typing import List\n\ndef merge_two(arr1 :  List[int], arr2 : List[int]) -&gt; List[int] :\n    \"\"\"\n    Given two arrays sorted in non decreasing order, return a merged array \n    \"\"\"\n\n    m, n = len(arr1) , len(arr2)\n\n    res = [0]*(m+n)\n\n    p1,p2,p3 = 0,0,0\n\n    while p1 &lt; m  and p2 &lt; n  :\n\n        if arr1[p1] &lt; arr2[p2] : \n            res[p3] = arr1[p1]\n            p3+=1\n            p1+=1\n        else :\n            res[p3] = arr2[p2]\n            p3+=1\n            p2+=1\n\n\n    while p1 &lt; m :\n        res[p3] = arr1[p1]\n        p1+=1\n        p3+=1\n\n    while p2 &lt; n :\n        res[p3] = arr2[p2]\n        p2+=1\n        p3+=1\n\n    return res\n\n\nif __name__ == \"__main__\" :\n    print(merge_two([4,5,7] , [3,6,7,8]))\n    print(merge_two([4,5,7] , [3,6]))\n    print(merge_two([] , [1,2,3]))\n    print( merge_two( [1, 2, 2, 5] , [2, 3, 4, 4] ) ) # [1, 2, 2, 2, 3, 4, 4, 5]\n    print( merge_two( [1, 3, 5, 7] , [2, 4, 6, 8] ) ) # [1, 2, 3, 4, 5, 6, 7, 8]\n</code></pre> <pre><code>\"\"\"\nUnit Tests for Two way merge.\n\n# Aside : See these for an introduction to pytest : https://realpython.com/pytest-python-testing/\n              https://gist.github.com/kwmiebach/3fd49612ef7a52b5ce3a\n\"\"\"\nimport pytest\nfrom two_way_merge import merge_two \n\n\n# --- Test Cases ---\n\ndef test_merge_basic():\n    \"\"\"Tests merging two non-empty lists.\"\"\"\n    arr1 = [1, 3, 5, 7]\n    arr2 = [2, 4, 6, 8]\n    expected = [1, 2, 3, 4, 5, 6, 7, 8]\n    # Note: The original buggy code might return [1, 2, 3, 4, 5, 6, 7, 9]\n    assert merge_two(arr1, arr2) == expected\n\ndef test_merge_empty_first():\n    \"\"\"Tests merging when the first list is empty.\"\"\"\n    arr1 = []\n    arr2 = [2, 4, 6]\n    expected = [2, 4, 6]\n    assert merge_two(arr1, arr2) == expected\n    # Test swap logic too\n    assert merge_two(arr2, arr1) == expected\n\ndef test_merge_empty_second():\n    \"\"\"Tests merging when the second list is empty.\"\"\"\n    arr1 = [1, 3, 5]\n    arr2 = []\n    expected = [1, 3, 5]\n    assert merge_two(arr1, arr2) == expected\n    # Test swap logic too\n    assert merge_two(arr2, arr1) == expected\n\ndef test_merge_both_empty():\n    \"\"\"Tests merging when both lists are empty.\"\"\"\n    arr1 = []\n    arr2 = []\n    expected = []\n    assert merge_two(arr1, arr2) == expected\n\ndef test_merge_duplicates():\n    \"\"\"Tests merging lists with duplicate numbers.\"\"\"\n    arr1 = [1, 2, 2, 5]\n    arr2 = [2, 3, 4, 4]\n    expected = [1, 2, 2, 2, 3, 4, 4, 5]\n    # Note: Original buggy code might return [1, 2, 2, 2, 3, 4, 4, 9]\n    assert merge_two(arr1, arr2) == expected\n\ndef test_merge_duplicates_across_lists():\n    \"\"\"Tests merging lists where duplicates exist between lists.\"\"\"\n    arr1 = [1, 3, 5]\n    arr2 = [1, 3, 5]\n    expected = [1, 1, 3, 3, 5, 5]\n    # Note: Original buggy code might return [1, 1, 3, 3, 5, 9]\n    assert merge_two(arr1, arr2) == expected\n\ndef test_merge_interleaved():\n    \"\"\"Tests merging lists with highly interleaved numbers.\"\"\"\n    arr1 = [1, 4, 5, 8]\n    arr2 = [2, 3, 6, 7]\n    expected = [1, 2, 3, 4, 5, 6, 7, 8]\n    # Note: Original buggy code might return [1, 2, 3, 4, 5, 6, 7, 9]\n    assert merge_two(arr1, arr2) == expected\n\ndef test_merge_one_list_much_shorter():\n    \"\"\"Tests merging when one list is significantly shorter.\"\"\"\n    arr1 = [1, 2, 10, 11, 12]\n    arr2 = [3, 4]\n    expected = [1, 2, 3, 4, 10, 11, 12]\n    # Note: Original buggy code might return [1, 2, 3, 4, 10, 11, 9]\n    assert merge_two(arr1, arr2) == expected\n    # Test swap logic too\n    assert merge_two(arr2, arr1) == expected # Should give same result\n\ndef test_merge_all_elements_smaller():\n    \"\"\"Tests merging when all elements of one list are smaller than the other.\"\"\"\n    arr1 = [1, 2, 3]\n    arr2 = [4, 5, 6]\n    expected = [1, 2, 3, 4, 5, 6]\n    # Note: Original buggy code might return [1, 2, 3, 4, 5, 9]\n    assert merge_two(arr1, arr2) == expected\n    # Test swap logic too\n    assert merge_two(arr2, arr1) == expected # Should give same result\n\ndef test_merge_negative_numbers():\n    \"\"\"Tests merging lists with negative numbers.\"\"\"\n    arr1 = [-5, -1, 0, 10]\n    arr2 = [-3, -2, 8, 12]\n    expected = [-5, -3, -2, -1, 0, 8, 10, 12]\n    # Note: Original buggy code might return [-5, -3, -2, -1, 0, 8, 10, 9]\n    assert merge_two(arr1, arr2) == expected\n\ndef test_merge_single_element_lists():\n    \"\"\"Tests merging lists with single elements.\"\"\"\n    arr1 = [5]\n    arr2 = [2]\n    expected = [2, 5]\n    # Note: Original buggy code might return [2, 9] due to off-by-one loop\n    assert merge_two(arr1, arr2) == expected\n    assert merge_two(arr2, arr1) == expected\n</code></pre>"},{"location":"Algorithms/Two_Way_Merge/#merging-two-sorted-lists","title":"Merging two sorted lists","text":"<p>The same thing for a linked list :</p> Code Two Way Merge on Lists <pre><code>\"\"\"\nSolution for  https://leetcode.com/problems/merge-two-sorted-lists/description/\n\"\"\"\nfrom typing import Optional\nclass ListNode:\n    def __init__(self, val=0, next=None):\n        self.val = val\n        self.next = next\n\nclass Solution:\n    def mergeTwoLists(self, list1: Optional[ListNode], list2: Optional[ListNode]) -&gt; Optional[ListNode]:\n        p1, p2  = list1,list2\n        dummy = ListNode(0)\n        p3 = dummy\n\n        while p1 and p2 :\n            if p1.val &lt; p2.val:\n                p3.next = p1\n                p1 = p1.next\n            else: \n                p3.next = p2\n                p2 = p2.next\n            p3 = p3.next\n\n        if p1 :\n            p3.next = p1\n        else :\n            p3.next = p2\n\n        return dummy.next \n</code></pre> <p>So, how are we solving the problem of two way merging?</p> <p>We have a write pointer p3 and we have two read pointers, p1 and p2. In each iteration of the while loop, we pick the smaller read pointer to write to the write location and move both pointers (write pointer and one of the read pointers) ahead.</p> <p>If either p1 or p2 have still not reached the end of their read containers, we just write the rest of that read container at the write pointer.</p>"},{"location":"Algorithms/Two_Way_Merge/#practice","title":"Practice","text":"Merge sorted arrays backwards <pre><code>\"\"\"\nSolution for https://leetcode.com/problems/merge-sorted-array/\n\nA slightly different variation of the two-way merge problem. \nThe challenge here is to merge one array into another array \nbackwards into extra space allocated at the end of one of the arrays.\n\nTo accomplish this at each step we pick the greatest among the the two choices and put it at the end of the target array.\n\np3  -&gt; write pointer\np1, p2 -&gt; read pointers\nnums1 -&gt; target array\n\n\n\np3 will never overwrite p1 because :\n\nInitially p3 - p1 = n,\nIf p2 is decremented, the gap p3 - p1 reduces by 1, because p3 is incremented and p1 remains the same.\nIf p1 is decremented, the gap p3 - p1 remains the same \n0&lt;= p3-p1 &lt;= n # The gap can at most become zero.\nWhen,\np3 -p1 = 0 &lt;-&gt; p2  = -1\nWhich means all of nums2 has been written into nums1, the while loop will not execute.\nTherefore p3 will never overwrite p1.\n\n\nDry Run these to see how it works out :\n\nnums1 :[2,3,0]\nnums2 :[4]\n\nnums1 :[2,3,0]\nnums2 : [-1]\n\"\"\"\nclass Solution:\n    def merge(self, nums1: List[int], m: int, nums2: List[int], n: int) -&gt; None:\n        \"\"\"\n        Do not return anything, modify nums1 in-place instead.\n        \"\"\"\n        p1,p2,p3 = m-1, n-1, len(nums1) -1 \n        while p2 &gt;= 0 :\n            if p1 &gt;=0 and nums1[p1] &gt; nums2[p2] : \n                nums1[p3] = nums1[p1]\n                p1-=1\n                p3-=1\n            else :\n                nums1[p3] = nums2[p2]\n                p2-=1\n                p3-=1\n\n        return nums1\n</code></pre> Merge Strings Alternately <pre><code>\"\"\"\nA simpler version of the two way merge problem.\nAll you have to do is pick the next element alternately among the two choices.\nThe EFFECT can be achieved by simply picking a letter from each array one after the other.\nWrap this up in s while loop runs until either one of the arrays have not finished being read. \n\"\"\"\nclass Solution:\n    def mergeAlternately(self, word1: str, word2: str) -&gt; str:\n\n        res = []\n        p1,p2 =0,0\n\n        while p1 &lt; len(word1) or p2 &lt; len(word2) :\n            if p1 &lt; len(word1) : \n                res.append(word1[p1]) \n                p1+=1\n            if p2 &lt; len(word2) : \n                res.append(word2[p2])\n                p2+=1\n        return \"\".join(res)\n</code></pre>"},{"location":"blog/","title":"Blog","text":""},{"location":"blog/Anatomy%20of%20an%20AWS%20Policy/","title":"Anatomy of an AWS Policy","text":"<p>AWS Policies are json documents which declare the access permissions for certain services or resources at a granular level. They can be attached to roles, users or resources. Let us understand how this works by looking at an example policy. </p>","tags":["AWS","IAM"]},{"location":"blog/Anatomy%20of%20an%20AWS%20Policy/#the-philosophy-of-aws-security","title":"The philosophy of AWS Security","text":"<p>AWS IAM operates on the principle of the shared responsibility model. This means AWS provides the MEANS of implementing security but YOU are responsible for using them effectively. This means having to manage lots of policies.</p> <p>Another important principle is the principle of LEAST PRIVILEGE. In case of a hypthetical security breach, the BLAST RADIUS should be as small as possible.</p> <p>Blast Radius for the following policy is a single Lambda Function being invoked: </p> <pre><code>{\n          \"Version\": \"2012-10-17\",\n          \"Statement\": [\n            {\n              \"Sid\": \"VisualEditor1\",\n              \"Effect\": \"Allow\",\n              \"Action\": \"lambda:InvokeFunction\",\n              \"Resource\": [\n                \"arn:aws:lambda:&lt;your_account_id&gt;:&lt;us-east-1&gt;:function:MyFunction\"\n              ]\n            }\n          ]\n}\n</code></pre> <p>Now , if you want to use a generic role to invoke ANY lambda function in the acccount, you would use a wild card in the Resource : \"arn:aws:lambda:::function:*\" . <p>Now the blast radius is invocation of any function.</p> <p>And if you want to have a generic role which can pretty much do anything with lambda functions like create, modify, delete and invoke lambdas, you would use a wildcard to define the Action as \"lambda:*\". So, the blast radius just increased to include anything which can be done to or with lambda functions. Somebody could just list and delete all your lambda functions.</p>","tags":["AWS","IAM"]},{"location":"blog/Anatomy%20of%20an%20AWS%20Policy/#when-does-iam-apply","title":"When does IAM apply?","text":"<p>Whenever you request anything through an AWS API its going to go through IAM. But but, there is a case when it does not apply : When you have an HTTP server running on an AWS EC2 instance and http requests are directly made to the elastic ip address, there is no AWS service in between, so no IAM policy evaluation is done in this case.</p> <pre><code>{\n          \"Version\": \"2012-10-17\",\n          \"Statement\": [\n            {\n              \"Sid\": \"VisualEditor1\",\n              \"Effect\": \"Allow\",\n              \"Action\": \"lambda:InvokeFunction\",\n              \"Resource\": [\n                \"arn:aws:lambda:&lt;your_account_id&gt;:&lt;us-east-1&gt;:function:MyFunction\"\n              ]\n            }\n          ]\n}\n</code></pre>","tags":["AWS","IAM"]},{"location":"blog/Anatomy%20of%20an%20AWS%20Policy/#trust-policy","title":"TRUST POlICY","text":"<p>A trust policy is an IAM policy attached to a Role which specifies who (which Principal) can assume it. This is typically used to allow services to assume the role. A typical example is API Gateway assuming a role which Has permissions to invoke a Lambda function.</p> <p>https://aws.amazon.com/blogs/security/how-to-use-trust-policies-with-iam-roles/</p>","tags":["AWS","IAM"]},{"location":"blog/Anatomy%20of%20an%20AWS%20Policy/#invocation-role-vs-execution-role","title":"Invocation Role vs Execution Role","text":"<p>A question may arise: Does the invoking role/user need all the permisions of the execution role? Say, a user does not have direct permission to delete an item from an s3 bucket, but he does have access to invoke a lambda function which can perform this action through its execution role. Can the user invoke the lambda and delete files? The answer is yes. Once the Lambda is invoked it autmoatically assumes the execution roles and performs whatever actions it does with the execution role. </p> <p>\"The policies that are attached to the credentials that made the original call to AssumeRole are not evaluated by AWS when making the \"allow\" or \"deny\" authorization decision. The user temporarily gives up its original permissions in favor of the permissions assigned by the assumed role. In the case of the AssumeRoleWithSAML and AssumeRoleWithWebIdentity API operations, there are no policies to evaluate because the caller of the API is not an AWS identity.\" [Ref 5]</p>","tags":["AWS","IAM"]},{"location":"blog/Anatomy%20of%20an%20AWS%20Policy/#resource-policies","title":"Resource Policies","text":"","tags":["AWS","IAM"]},{"location":"blog/Anatomy%20of%20an%20AWS%20Policy/#a-special-note-on-s3-policies","title":"A special Note on S3 Policies","text":"<p>When it comes to S3 buckets there two types of ARNs 1. ARN to match the bucket : arn:aws:s3:::my-bucket 2. ARN to match objects: arn:aws:s3:::my-bucket/ , arn:aws:s3:::my-bucket/somepath/ ,arn:aws:s3:::my-bucket/somepath/*, arn:aws:s3:::my-bucket/somepath/somekey.txt</p> <p>There are also two types of actions : Bucket level and Object level. </p> <pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\"s3:GetObject\", \"s3:ListBucket\"],\n            \"Resource\": [\n                \"arn:aws:s3:::mybucket\",\n                \"arn:aws:s3:::mybucket/resports/diabetes/*\"\n            ]\n        }\n    ]\n}\n</code></pre> <p>In the policy above, s3:ListBucket applies to the Bucket ARN and s3:GetObject applies to the arn matching objects with this pattern mybucket/resports/diabetes/*. A clearer way to write the same policy is using two separate statemnets, one for bucket level actions and another for Object level actions :</p> <pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\"s3:ListBucket\"],\n            \"Resource\": \"arn:aws:s3:::mybucket\"\n        }.\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": \"s3:GetObject\",\n            \"Resource\": \"arn:aws:s3:::mybucket/reports/diabetes/*\"\n        }\n    ]\n}\n</code></pre> <p>The most common example of Bucket Level action is ListBucket. PutObject, GetObject are examples of  Object Level actions.</p> <p>A common mistake when writing bucket policies is not including the bucket arn as a Resource in the policy. Then you will be left wondowering why I cannot list operations fail even though I have s3:Listbucket action in my policy.</p>","tags":["AWS","IAM"]},{"location":"blog/Anatomy%20of%20an%20AWS%20Policy/#cross-account-resource-access","title":"Cross account resource access","text":"<p>Consider this quote from S3 ducumentation : </p> <p>\"If the requester is an IAM principal, Amazon S3 must determine if the parent AWS account to which the principal belongs has granted the principal necessary permission to perform the operation. In addition, if the request is for a bucket operation, such as a request to list the bucket content, Amazon S3 must verify that the bucket owner has granted permission for the requester to perform the operation. To perform a specific operation on a resource, an IAM principal needs permission from both the parent AWS account to which it belongs and the AWS account that owns the resource\"  [ Ref 7 ]</p> <p>This applies in general to any resource. What this means in practice is :</p> <ol> <li> <p>If the principal and the resource belong to the same account: Either the resource policy OR the policy attached to the principal (i.e. user/role) should explicitly Allow the action on the target resource.</p> </li> <li> <p>If they belong to different accounts: Both, the resource policy AND the policy attached to the principal (i.e. user/role) MUST explicitly Allow the action on the target resource.</p> </li> </ol>","tags":["AWS","IAM"]},{"location":"blog/Anatomy%20of%20an%20AWS%20Policy/#user-groups","title":"User Groups","text":"<p>A user group is a collection of IAM users. User groups are used to specify permissions to a collection of users. Groups simplify permission management by letting us grant, change, and remove permissions for multiple users at once.</p>","tags":["AWS","IAM"]},{"location":"blog/Anatomy%20of%20an%20AWS%20Policy/#references","title":"References","text":"<ol> <li> <p>https://docs.aws.amazon.com/pdfs/whitepapers/latest/aws-fault-isolation-boundaries/aws-fault-isolation-boundaries.pdf</p> </li> <li> <p>https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_aws-services-that-work-with-iam.html</p> </li> <li> <p>https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_condition-keys.html</p> </li> <li> <p>https://docs.aws.amazon.com/awscloudtrail/latest/userguide/view-cloudtrail-events-cli.html</p> </li> <li> <p>https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_temp_control-access_assumerole.html</p> </li> <li> <p>https://docs.aws.amazon.com/service-authorization/latest/reference/reference_policies_actions-resources-contextkeys.html</p> </li> <li> <p>https://docs.aws.amazon.com/IAM/latest/UserGuide/reference-arns.html#supported-arns</p> </li> <li> <p>https://docs.aws.amazon.com/AmazonS3/latest/userguide/how-s3-evaluates-access-control.html</p> </li> </ol>","tags":["AWS","IAM"]},{"location":"blog/test/","title":"Building Serverless ETL Pipelines on AWS","text":"<p>In this post, I'll share my experience building a serverless ETL pipeline to extract and map medical entities using various AWS services including Lambda, SNS, Textract, Translate, Comprehend Medical, Glue, and Athena.</p>","tags":["tag1","tag2"]},{"location":"blog/test/#the-challenge","title":"The Challenge","text":"<p>Working at Genentech, I faced the challenge of extracting medical entities from multilingual clinical notes that came in various formats including handwritten images, PDFs, and DOCX files. These entities needed to be mapped to standardized ICD-10 codes for analysis.</p>","tags":["tag1","tag2"]},{"location":"blog/test/#architecture-overview","title":"Architecture Overview","text":"<p>Our solution leveraged a serverless architecture with these key components:</p> <ol> <li>Document Ingestion: AWS S3 for storage with event triggers</li> <li>Text Extraction: AWS Textract for converting documents to machine-readable text</li> <li>Translation: AWS Translate for handling multilingual content</li> <li>Entity Recognition: AWS Comprehend Medical for identifying medical terms</li> <li>Code Mapping: Custom Lambda function to map to ICD-10 codes</li> <li>Data Storage: Processed results stored in S3 and cataloged in Glue</li> <li>Analysis: Athena for SQL-based querying of the processed data</li> </ol>","tags":["tag1","tag2"]},{"location":"blog/test/#implementation-details","title":"Implementation Details","text":"<p>[Content continues with technical implementation details]</p>","tags":["tag1","tag2"]},{"location":"blog/test/#benefits-and-results","title":"Benefits and Results","text":"<p>The serverless approach provided several advantages:</p> <ol> <li>Cost Efficiency: Pay-per-use model reduced operational costs by 40%</li> <li>Scalability: Automatic scaling handled varying document loads</li> <li>Maintenance: Reduced operational overhead compared to EC2-based solutions</li> <li>Accuracy: Achieved 92% accuracy in entity recognition and mapping</li> </ol>","tags":["tag1","tag2"]},{"location":"blog/test/#lessons-learned","title":"Lessons Learned","text":"<p>[Content continues with lessons learned and best practices]</p>","tags":["tag1","tag2"]},{"location":"blog/test/#conclusion","title":"Conclusion","text":"<p>Serverless ETL pipelines offer significant advantages for processing unstructured medical data at scale. By leveraging AWS's managed services, we created a solution that was both cost-effective and powerful.</p> <p>Feel free to reach out if you have questions about implementing similar solutions in your organization!</p>","tags":["tag1","tag2"]},{"location":"blog/Fun%20Facts%20about%20two%27s%20complement%20encoding/","title":"Fun Facts about two's complement encoding","text":"<p>While revisiting my old notes on Standard C , I came across this fun fact: In two\u2019s complement, values like 0xF, 0xFF, and 0xFFFF all represent -1. Let's see why this is and explore some other properties of 2's complement encoding. </p>","tags":["C","Basics"]},{"location":"blog/Fun%20Facts%20about%20two%27s%20complement%20encoding/#signed-vs-unsigned-encoding-for-integers","title":"SIgned Vs Unsigned encoding for integers","text":"<p>Most computers use two\u2019s complement to represent negative values. In this system, the most significant bit (MSB) is treated as having a negative weight.</p> <p>Unsigned Example: 1111 \u2192 8 + 4 + 2 + 1 = 15</p> <p>Signed Example: 1111 \u2192 -8 + 4 + 2 + 1 = -1</p> <p>This means: - 0xF = -1 - 0xFF = -1 - 0xFFF = -1 - 0xFFFF = -1 \u2026and so on, for all bits set in any size.</p> <p>For a byte, numbers from 0x00 to 0x7F are positive and have the same encoding in both signed and unsigned (two\u2019s complement) form.</p>","tags":["C","Basics"]},{"location":"blog/Fun%20Facts%20about%20two%27s%20complement%20encoding/#range-for-n-bit-signed-integers","title":"Range for N-bit Signed Integers","text":"<p>Let us dig a little bit deeper. What could be the the range for an N-Bit Signed Integer.</p> <p>The Lowest value would be if the Most Significant Bit is set and no other bit is set.</p> <p>For a 4- bit Integer: </p> <pre><code>BIT representation        :   1     0     0      0\nPostition Value of bits   :  2^3   2^2   2^1    2^0 \nSign                      :  Negative since MSB is set \nDecimal Value             : -8                     \n</code></pre> <p>The lowest value will always be -(2^(N-1))</p> <p>Conversely, the highest positive value will be when the MSB is not set and all other bits are set. This is equal to 2^(N-1) -1.</p> <p>For a 4- bit Integer: </p> <pre><code>BIT representation        :   0     1     1      1\nPostition Value of bits   :  2^3   2^2   2^1    2^0 \nSign                      :  Positive since MSB is not set \nDecimal Value             :  7                     \n</code></pre> <p>Did you also notice that -(2^(N-1)) + (2^(N-1)-1) = -1?</p> <p>This is why 0xFFFFF... is always -1 in 2's complement.</p>","tags":["C","Basics"]},{"location":"blog/Fun%20Facts%20about%20two%27s%20complement%20encoding/#extension","title":"Extension","text":"<p>In fact, you can represent any N-bit signed integers in N+x bits bits simply by extending the Most Significant Bit x times.</p> <pre><code>    4-bit       8-bit       16-bit              Decimal\n    0111        00000111    0000000000000111    +7\n    1110        11111110    1111111111111110    -2\n</code></pre>","tags":["C","Basics"]},{"location":"blog/Fun%20Facts%20about%20two%27s%20complement%20encoding/#truncation","title":"Truncation","text":"<p>An N-bit signed integer can be truncated to N-x bits and hold the same value by removing the Highest Order x Bits excluding the  MSB as long as they match the MSB (Sign Bit).</p> <pre><code>11100  is -4\n1100   is -4\n100    is -4\n10     is NOT -4 . It is -2.   \n</code></pre>","tags":["C","Basics"]},{"location":"blog/Fun%20Facts%20about%20two%27s%20complement%20encoding/#getting-twos-complement-of-a-number","title":"Getting Two's Complement of a Number","text":"<p>For getting 2's complement of a number you  flip the bits and add 1.</p> <pre><code>4-bit   Decimal \n0011      3       \n1100     -4          \n1101     -3          #Two's complement of 0011\n</code></pre> <p>[ As two why this works see Ref 2 below ] </p>","tags":["C","Basics"]},{"location":"blog/Fun%20Facts%20about%20two%27s%20complement%20encoding/#overflow-detection","title":"Overflow Detection","text":"<p>In 2's complement overflow is not possible when adding numbers with opposite signs. When adding numbers of the same sign, if there is an overflow you will get the wrong sign bit. </p> <ul> <li> <p>If both numbers are positive and you get a negative result.</p> </li> <li> <p>If both numbers are negative and you get a positive result.</p> </li> </ul> <p>[Ref 1]</p>","tags":["C","Basics"]},{"location":"blog/Fun%20Facts%20about%20two%27s%20complement%20encoding/#why-2s-complement","title":"Why 2's complement?","text":"<p>It's convenient. The same Adders in Hardware can be used for doing addition and subtraction. All you have to do is flip bits and add 1 to get the negative representation of a positive number and vice-versa.</p> <p>It is also Bijective, which is to say that it can uniquely represent negative integers, positive integers and zero.</p>","tags":["C","Basics"]},{"location":"blog/Fun%20Facts%20about%20two%27s%20complement%20encoding/#additional-references","title":"Additional References","text":"<ol> <li>https://web.archive.org/web/20131031093811/http://www.cs.uwm.edu/~cs151/Bacon/Lecture/HTML/ch03s09.html</li> <li>https://www.cs.cornell.edu/~tomf/notes/cps104/twoscomp.html</li> <li>https://wiki.sei.cmu.edu/confluence/display/c/INT32-C.+Ensure+that+operations+on+signed+integers+do+not+result+in+overflow</li> </ol>","tags":["C","Basics"]},{"location":"resources/","title":"Resources","text":"<p>A curated collection of learning materials, documentation, and references.</p>"},{"location":"resources/#learning-platforms-playgrounds","title":"Learning Platforms &amp; Playgrounds","text":""},{"location":"resources/#data-engineering","title":"Data Engineering","text":"<ul> <li>DataTalks Club</li> </ul>"},{"location":"resources/#containers-linux","title":"Containers &amp; Linux","text":"<ul> <li>Container &amp; Linux Learning Labs</li> </ul>"},{"location":"resources/#sql","title":"SQL","text":"<ul> <li>Mode SQL Tutorial</li> <li>SQL Bolt</li> </ul>"},{"location":"resources/#spark","title":"Spark","text":"<ul> <li>Spark Playground</li> </ul>"},{"location":"resources/#mlai","title":"ML/AI","text":"<ul> <li>AFAIK.io</li> </ul>"},{"location":"resources/#documentation-white-papers","title":"Documentation &amp; White Papers","text":""},{"location":"resources/#aws","title":"AWS","text":"<ul> <li>AWS Fault Isolation Boundaries</li> <li>Using Trust Policies with IAM Roles</li> <li>S3 Object Lifecycle Management</li> </ul>"},{"location":"resources/#azure","title":"Azure","text":"<ul> <li>Azure Architecture Center</li> <li>AWS IAM to Azure Concepts Mapping</li> <li>Azure for AWS Professionals - Security &amp; Identity</li> <li>AWS &amp; Azure AD Security Integration</li> <li>Big Data Architecture Style</li> </ul>"},{"location":"resources/#snowflake","title":"Snowflake","text":"<ul> <li>Snowflake Documentation</li> </ul>"},{"location":"resources/#trino","title":"Trino","text":"<ul> <li>Trino Documentation</li> </ul>"},{"location":"resources/#devops","title":"DevOps","text":"<ul> <li>Terraform Language Documentation</li> <li>Helm Documentation</li> <li>Terraform CLI Workspaces Guide</li> <li>Terraform Configuration Organization</li> </ul>"},{"location":"resources/#technical-blog-posts-articles","title":"Technical Blog Posts &amp; Articles","text":""},{"location":"resources/#sql_1","title":"SQL","text":"<ul> <li>Mastering EXPLAIN ANALYZE</li> <li>N+1 Query Problem</li> <li>GraphQL N+1 Problem</li> </ul>"},{"location":"resources/#dataframe-data-processing","title":"DataFrame &amp; Data Processing","text":"<ul> <li>SQL vs DataFrames Discussion</li> <li>10 Minutes to Dask</li> <li>TPC-H Benchmarks</li> <li>Dask Join Order Optimization</li> <li>Spark MapReduce Internals</li> <li>JDBC - a short guide</li> </ul>"},{"location":"resources/#cloud-computing","title":"Cloud Computing","text":"<ul> <li>AWS App Runner Discussion</li> <li>AWS App Runner Issues</li> <li>Cloud Platform Comparison for Startups</li> </ul>"},{"location":"resources/#aiml","title":"AI/ML","text":"<ul> <li>LLM Document Summarization</li> <li>Text Chunking Strategies</li> <li>Podcast Transcripts Summarization</li> </ul>"},{"location":"resources/#books-long-form-content","title":"Books &amp; Long-form Content","text":""},{"location":"resources/#data-engineering_1","title":"Data Engineering","text":"<ul> <li>How Query Engines Work</li> <li>Data Engineering Design Patterns</li> </ul>"},{"location":"resources/#cloud-computing_1","title":"Cloud Computing","text":"<ul> <li>AWS SAP Certification Notes</li> <li>Azure Data Lakehouse Toolkit</li> </ul>"},{"location":"resources/#version-control","title":"Version Control","text":"<ul> <li>Pro Git Book</li> </ul>"},{"location":"resources/#web-development","title":"Web Development","text":"<ul> <li>HTTP Crash Course</li> </ul>"},{"location":"resources/#networking","title":"Networking","text":"<ul> <li>Networking Crash Course</li> </ul>"},{"location":"resources/#computer-engineering","title":"Computer Engineering","text":"<p>What Every Computer Scientist should know about Floating Point Arithmetic</p>"},{"location":"resources/#technical-blogs","title":"Technical Blogs","text":""},{"location":"resources/#data-engineering_2","title":"Data Engineering","text":"<ul> <li>AS Rathore's Blog</li> </ul>"},{"location":"resources/#sql_2","title":"SQL","text":"<ul> <li>Erland Sommarskog's SQL Blog</li> </ul>"},{"location":"resources/#ai-engineering","title":"AI Engineering","text":"<ul> <li>Amit Chaudhary's Blog</li> <li>Coding The Smart Way</li> <li>Rik AI</li> </ul>"},{"location":"resources/#java","title":"Java","text":"<ul> <li>Marco Behler's Blog</li> </ul>"},{"location":"resources/#cheat-sheets","title":"Cheat Sheets","text":"<ul> <li>PySpark RDD Cheat Sheet</li> <li>Trino Cheat Sheet</li> </ul>"},{"location":"resources/Dimensional_Modeling_Cheatseet/","title":"Dimensional Data Modeling Cheatsheet","text":"<p>AI Assisted (Claude Sonnet 4)</p>"},{"location":"resources/Dimensional_Modeling_Cheatseet/#core-concepts","title":"Core Concepts","text":""},{"location":"resources/Dimensional_Modeling_Cheatseet/#what-is-dimensional-modeling","title":"What is Dimensional Modeling?","text":"<p>A design technique for data warehouses that organizes data into facts (measurable events) and dimensions (descriptive context) to support fast analytical queries and business intelligence.</p>"},{"location":"resources/Dimensional_Modeling_Cheatseet/#key-benefits","title":"Key Benefits","text":"<ul> <li>Query Performance: Optimized for analytical workloads</li> <li>Business User Friendly: Intuitive structure matching business processes</li> <li>Scalable: Handles large data volumes efficiently</li> <li>Flexible: Supports drilling down and rolling up through hierarchies</li> </ul>"},{"location":"resources/Dimensional_Modeling_Cheatseet/#building-blocks","title":"Building Blocks","text":""},{"location":"resources/Dimensional_Modeling_Cheatseet/#fact-tables","title":"Fact Tables","text":"<p>Central tables containing quantitative, measurable data about business events.</p> <p>Characteristics: - Large number of rows (transactions, events) - Foreign keys to dimension tables - Additive measures (sales amount, quantity) - Sparse data (many null values acceptable)</p> <p>Types of Facts: - Transactional: Individual business events - Periodic Snapshot: Regular intervals (daily, monthly) - Accumulating Snapshot: Process lifecycle tracking</p>"},{"location":"resources/Dimensional_Modeling_Cheatseet/#dimension-tables","title":"Dimension Tables","text":"<p>Provide descriptive context for facts - the \"who, what, when, where, why\" of business events.</p> <p>Characteristics: - Relatively small number of rows - Wide tables with many descriptive attributes - Primary keys (often surrogate keys) - Denormalized for query performance</p> <p>Common Dimensions: - Customer: Demographics, preferences, segments - Product: Categories, brands, specifications - Time: Hierarchies, fiscal calendars, holidays - Geography: Regions, territories, locations</p>"},{"location":"resources/Dimensional_Modeling_Cheatseet/#schema-patterns","title":"Schema Patterns","text":""},{"location":"resources/Dimensional_Modeling_Cheatseet/#star-schema","title":"Star Schema","text":"<p>Structure: Central fact table directly connected to dimension tables <pre><code>    Customer\n        |\nProduct \u2014 Fact \u2014 Store\n        |\n      Date\n</code></pre></p> <p>Advantages: - Simple structure, easy to understand - Faster query performance (fewer joins) - Optimized for business intelligence tools</p> <p>Use When: - Query performance is priority - Business users need simple models - Storage redundancy is acceptable</p>"},{"location":"resources/Dimensional_Modeling_Cheatseet/#snowflake-schema","title":"Snowflake Schema","text":"<p>Structure: Normalized dimensions broken into multiple related tables <pre><code>    Customer\n        |\n     City \u2014 State \u2014 Country\n        |\nProduct \u2014 Fact \u2014 Store\n   |              |\nCategory       Region\n   |\n Brand\n</code></pre></p> <p>Advantages: - Reduced data redundancy - Better data integrity - Smaller storage footprint</p> <p>Use When: - Storage efficiency is critical - Large dimension tables need normalization - Data integrity is paramount</p>"},{"location":"resources/Dimensional_Modeling_Cheatseet/#galaxy-schema","title":"Galaxy Schema","text":"<p>Structure: Multiple fact tables sharing common dimensions <pre><code>Dimension1 \u2014 Fact1 \u2014 Dimension2\n    |           |\n    |         Fact2 \u2014 Dimension3\n    |           |\nDimension4 \u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\n</code></pre></p> <p>Use When: - Multiple business processes - Complex enterprise requirements - Shared dimensions across processes</p>"},{"location":"resources/Dimensional_Modeling_Cheatseet/#essential-design-principles","title":"Essential Design Principles","text":""},{"location":"resources/Dimensional_Modeling_Cheatseet/#grain-definition","title":"Grain Definition","text":"<p>Most Critical Decision: What does each row in the fact table represent?</p> <p>Examples: - One row per sales transaction - One row per daily inventory snapshot - One row per customer interaction</p> <p>Rules: - Must be consistent throughout fact table - Determines what questions can be answered - All measures must be at same grain level</p>"},{"location":"resources/Dimensional_Modeling_Cheatseet/#slowly-changing-dimensions-scd","title":"Slowly Changing Dimensions (SCD)","text":"<p>Handle changes to dimensional attributes over time.</p> <p>Type 1: Overwrite - Replace old value with new value - No history preserved - Use for: Corrections, low-importance changes     <pre><code>UPDATE Customer SET City = 'New York' WHERE CustomerID = 123\n</code></pre></p> <p>Type 2: Add New Record - Create new record with effective dates - Preserves complete history - Use for: Important attribute changes     <pre><code>-- Original record\nCustomerKey | CustomerID | Name | City | EffectiveDate | ExpirationDate\n501         | 123        | John | Boston | 2020-01-01  | 2023-12-31\n\n-- New record after move\n502         | 123        | John | NYC    | 2024-01-01  | 9999-12-31\n</code></pre></p> <p>Type 3: Add New Column - Keep both old and new values - Limited history (usually current + previous) - Use for: Predictable changes, limited tracking     <pre><code>CustomerKey | Name | CurrentCity | PreviousCity\n501         | John | NYC         | Boston\n</code></pre></p>"},{"location":"resources/Dimensional_Modeling_Cheatseet/#surrogate-keys","title":"Surrogate Keys","text":"<p>Artificial primary keys (usually integers) for dimension records.</p> <p>Benefits: - Stable references when business keys change - Better join performance - Handle multiple source systems - Support slowly changing dimensions</p> <p>Implementation: <pre><code>-- Instead of natural key\nProductKey | ProductCode | ProductName | Category\n201        | SKU-12345   | iPhone 15   | Electronics\n\n-- Fact table references surrogate key\nSalesKey | ProductKey | CustomerKey | SalesAmount\n1001     | 201        | 501         | 999.00\n</code></pre></p>"},{"location":"resources/Dimensional_Modeling_Cheatseet/#common-patterns","title":"Common Patterns","text":""},{"location":"resources/Dimensional_Modeling_Cheatseet/#conformed-dimensions","title":"Conformed Dimensions","text":"<p>Shared dimensions across multiple fact tables ensuring consistent analysis.</p> <p>Example: - Customer dimension used by Sales, Support, and Marketing facts - Date dimension shared across all business processes - Product dimension used by Sales, Inventory, and Returns</p> <p>Benefits: - Consistent metrics across business areas - Integrated reporting capabilities - Simplified ETL processes</p>"},{"location":"resources/Dimensional_Modeling_Cheatseet/#degenerate-dimensions","title":"Degenerate Dimensions","text":"<p>Dimensional data stored directly in fact table (no separate dimension table).</p> <p>Common Examples: - Order numbers, invoice numbers - Transaction IDs, ticket numbers - Serial numbers, batch codes</p> <p>When to Use: - Only identifier exists (no descriptive attributes) - High cardinality with little analytical value - Performance optimization for grouping</p> <pre><code>-- OrderNumber stored in fact table\nSalesID | CustomerKey | ProductKey | OrderNumber | SalesAmount\n1001    | 501         | 201        | ORD-12345   | 125.99\n1002    | 501         | 203        | ORD-12345   | 89.50\n</code></pre>"},{"location":"resources/Dimensional_Modeling_Cheatseet/#junk-dimensions","title":"Junk Dimensions","text":"<p>Consolidate low-cardinality flags and indicators.</p> <p>Example: <pre><code>-- Instead of separate dimensions for each flag\nJunkKey | PaymentType | ShippingType | GiftWrap | Priority\n901     | Credit      | Standard     | No       | Normal\n902     | Credit      | Express      | Yes      | High\n903     | Debit       | Standard     | No       | Normal\n</code></pre></p> <p>Benefits: - Reduces fact table width - Eliminates tiny dimension tables - Groups related operational flags</p>"},{"location":"resources/Dimensional_Modeling_Cheatseet/#role-playing-dimensions","title":"Role-Playing Dimensions","text":"<p>Same dimension used multiple times in single fact table.</p> <p>Example: <pre><code>-- Date dimension used for multiple date types\nSalesID | CustomerKey | OrderDateKey | ShipDateKey | DeliveryDateKey\n1001    | 501         | 20240115     | 20240116    | 20240118\n</code></pre></p> <p>Implementation: - Create views or aliases for each role - Use descriptive names in fact table - Maintain same dimension structure</p>"},{"location":"resources/Dimensional_Modeling_Cheatseet/#design-best-practices","title":"Design Best Practices","text":""},{"location":"resources/Dimensional_Modeling_Cheatseet/#fact-table-design","title":"Fact Table Design","text":"<ol> <li>Choose Atomic Grain: Store data at lowest level of detail</li> <li>Additive Measures: Prefer measures that can be summed</li> <li>Consistent Grain: All measures at same level of detail</li> <li>Minimize NULLs: Use dimension attributes instead of fact NULLs</li> </ol>"},{"location":"resources/Dimensional_Modeling_Cheatseet/#dimension-design","title":"Dimension Design","text":"<ol> <li>Descriptive Attributes: Include all relevant business context</li> <li>User-Friendly Names: Use business terminology</li> <li>Hierarchies: Support drill-down analysis paths</li> <li>Surrogate Keys: Always use for dimension primary keys</li> </ol>"},{"location":"resources/Dimensional_Modeling_Cheatseet/#performance-optimization","title":"Performance Optimization","text":"<ol> <li> <p>Indexing Strategy:</p> <ul> <li>Clustered indexes on fact table date columns</li> <li>Foreign key indexes on fact tables</li> <li>Covering indexes for common query patterns</li> </ul> </li> <li> <p>Partitioning:</p> <ul> <li>Partition large fact tables by date</li> <li>Consider customer-based partitioning for very large customers</li> <li>Implement partition elimination in queries</li> </ul> </li> <li> <p>Aggregation:</p> <ul> <li>Pre-aggregate common summary levels</li> <li>Create OLAP cubes for frequent analysis patterns</li> <li>Maintain incremental refresh processes</li> </ul> </li> </ol>"},{"location":"resources/Dimensional_Modeling_Cheatseet/#implementation-checklist","title":"Implementation Checklist","text":""},{"location":"resources/Dimensional_Modeling_Cheatseet/#planning-phase","title":"Planning Phase","text":"<ul> <li> Define business requirements and KPIs</li> <li> Identify source systems and data quality</li> <li> Determine fact table grain and measures</li> <li> Design dimension hierarchies and attributes</li> <li> Plan for slowly changing dimensions</li> </ul>"},{"location":"resources/Dimensional_Modeling_Cheatseet/#development-phase","title":"Development Phase","text":"<ul> <li> Create staging area for data cleansing</li> <li> Implement surrogate key generation</li> <li> Build ETL processes with error handling</li> <li> Create data quality validation rules</li> <li> Implement SCD processing logic</li> </ul>"},{"location":"resources/Dimensional_Modeling_Cheatseet/#testing-phase","title":"Testing Phase","text":"<ul> <li> Validate business rules and calculations</li> <li> Test slowly changing dimension scenarios</li> <li> Verify referential integrity</li> <li> Performance test with realistic data volumes</li> <li> User acceptance testing with business users</li> </ul>"},{"location":"resources/Dimensional_Modeling_Cheatseet/#deployment-phase","title":"Deployment Phase","text":"<ul> <li> Set up production environments</li> <li> Implement monitoring and alerting</li> <li> Create user documentation and training</li> <li> Establish maintenance procedures</li> <li> Plan for ongoing support and enhancements</li> </ul>"},{"location":"resources/Dimensional_Modeling_Cheatseet/#common-pitfalls-to-avoid","title":"Common Pitfalls to Avoid","text":""},{"location":"resources/Dimensional_Modeling_Cheatseet/#design-mistakes","title":"Design Mistakes","text":"<ul> <li>Inconsistent Grain: Mixing transaction and summary data in same fact</li> <li>Over-normalization: Creating snowflake when star would suffice</li> <li>Missing Business Keys: Not storing original system identifiers</li> <li>Inadequate SCD Planning: Not considering how dimensions change</li> </ul>"},{"location":"resources/Dimensional_Modeling_Cheatseet/#performance-issues","title":"Performance Issues","text":"<ul> <li>Too Many Dimensions: Overly wide fact tables</li> <li>Large Dimension Tables: Not using mini-dimensions for large cardinality</li> <li>Poor Partitioning: Not aligning with query patterns</li> <li>Missing Indexes: Inadequate indexing strategy</li> </ul>"},{"location":"resources/Dimensional_Modeling_Cheatseet/#data-quality-problems","title":"Data Quality Problems","text":"<ul> <li>Duplicate Records: Lack of proper matching and deduplication</li> <li>Orphaned Facts: Facts without corresponding dimensions</li> <li>Inconsistent Formatting: Different formats across source systems</li> <li>Late Arriving Data: Not handling out-of-sequence data loads</li> </ul>"},{"location":"resources/Dimensional_Modeling_Cheatseet/#quick-reference","title":"Quick Reference","text":""},{"location":"resources/Dimensional_Modeling_Cheatseet/#when-to-use-star-vs-snowflake","title":"When to Use Star vs Snowflake","text":"<p>Star Schema: - Performance is critical - Business users directly query - Storage cost is not major concern - Simpler maintenance requirements</p> <p>Snowflake Schema: - Storage efficiency important - Large dimension tables - Strong normalization requirements - Complex data relationships</p>"},{"location":"resources/Dimensional_Modeling_Cheatseet/#scd-type-selection","title":"SCD Type Selection","text":"<p>Type 1: Corrections, unimportant changes Type 2: Critical business changes requiring history Type 3: Predictable changes with limited history needs</p>"},{"location":"resources/Dimensional_Modeling_Cheatseet/#grain-selection-guidelines","title":"Grain Selection Guidelines","text":"<ul> <li>Start with most atomic level possible</li> <li>Consider storage and performance implications</li> <li>Ensure all business questions can be answered</li> <li>Plan for future analytical requirements</li> </ul>"},{"location":"resources/Dimensional_Modeling_Cheatseet/#e-commerce-star-schema-example","title":"E-commerce Star Schema Example","text":""},{"location":"resources/Dimensional_Modeling_Cheatseet/#business-scenario","title":"Business Scenario","text":"<p>Imagine you're working at an online retailer that sells consumer electronics. The business stakeholders come to you with a requirement: \"We need to analyze our sales performance to understand which products are selling well, who our best customers are, and how our different stores are performing.\"</p> <p>This is a classic dimensional modeling challenge. Let's walk through the thought process of designing a data warehouse to solve this problem.</p>"},{"location":"resources/Dimensional_Modeling_Cheatseet/#understanding-the-business-questions","title":"Understanding the Business Questions","text":"<p>Before we start designing tables, we need to understand what questions the business wants to answer. The stakeholders tell us they want to know:</p> <ul> <li>Which products are our top sellers by month?</li> <li>Who are our highest-value customers by region?</li> <li>How do weekend sales compare to weekday sales?</li> <li>Which stores generate the most revenue?</li> <li>What's the average order value by customer segment?</li> </ul> <p>These questions give us clues about what our dimensional model should look like. We can see that we need to track sales events (facts) and analyze them by product, customer, time, and store (dimensions).</p>"},{"location":"resources/Dimensional_Modeling_Cheatseet/#choosing-the-grain","title":"Choosing the Grain","text":"<p>The most critical decision in dimensional modeling is selecting the grain - what does each row in our fact table represent? We have several options:</p> <p>We could track at the order level (one row per order), but this wouldn't let us analyze individual products. We could track at the daily summary level (one row per product per day), but this might lose important detail.</p> <p>For our e-commerce scenario, we choose line item level - one row per product in each order. This atomic grain gives us maximum flexibility. We can always aggregate up to order level or daily level, but we can't drill down if we start with summarized data.</p>"},{"location":"resources/Dimensional_Modeling_Cheatseet/#designing-our-star-schema","title":"Designing Our Star Schema","text":"<pre><code>erDiagram\n    FACT_SALES {\n        bigint sales_key\n        int customer_key\n        int product_key\n        int date_key\n        int store_key\n        varchar order_number\n        int quantity\n        decimal unit_price\n        decimal discount_amount\n        decimal total_amount\n    }\n\n    DIM_CUSTOMER {\n        int customer_key\n        varchar customer_id\n        varchar customer_name\n        varchar email\n        varchar city\n        varchar state\n        varchar country\n        varchar customer_segment\n        date registration_date\n    }\n\n    DIM_PRODUCT {\n        int product_key\n        varchar product_id\n        varchar product_name\n        varchar category\n        varchar brand\n        decimal cost\n        decimal list_price\n        varchar status\n    }\n\n    DIM_DATE {\n        int date_key\n        date full_date\n        int year\n        int quarter\n        int month\n        varchar month_name\n        int day\n        varchar day_name\n        char is_weekend\n        char is_holiday\n    }\n\n    DIM_STORE {\n        int store_key\n        varchar store_id\n        varchar store_name\n        varchar store_type\n        varchar city\n        varchar state\n        varchar region\n        varchar manager\n    }\n\n    DIM_CUSTOMER ||--o{ FACT_SALES : \"purchases\"\n    DIM_PRODUCT ||--o{ FACT_SALES : \"sold\"\n    DIM_DATE ||--o{ FACT_SALES : \"on_date\"\n    DIM_STORE ||--o{ FACT_SALES : \"sold_at\"</code></pre>"},{"location":"resources/Dimensional_Modeling_Cheatseet/#fact-table-design-reasoning","title":"Fact Table Design Reasoning","text":"<p>Our fact table contains the measurable events - the actual sales transactions. Let's think through each component:</p> <p>Surrogate Key: <code>sales_key</code> is an artificial primary key. We don't use the natural order number because multiple line items can exist per order.</p> <p>Foreign Keys: Each foreign key points to a dimension table. Notice how <code>customer_key</code>, <code>product_key</code>, <code>date_key</code>, and <code>store_key</code> connect our fact to the descriptive information.</p> <p>Degenerate Dimension: <code>order_number</code> stays in the fact table because while it's dimensional (we want to group by order), it has no descriptive attributes that warrant a separate table.</p> <p>Measures: These are the numbers we want to analyze: - <code>quantity</code>: How many items were sold - <code>unit_price</code>: Price per item (may differ from list price due to promotions) - <code>discount_amount</code>: Discount applied to this line item - <code>total_amount</code>: Final amount for this line item</p> <p>Notice that all measures are at the same grain (per line item) and are mostly additive - we can sum them across different dimensions to get meaningful totals.</p>"},{"location":"resources/Dimensional_Modeling_Cheatseet/#dimension-design-reasoning","title":"Dimension Design Reasoning","text":""},{"location":"resources/Dimensional_Modeling_Cheatseet/#customer-dimension-the-who","title":"Customer Dimension - The \"Who\"","text":"<p>We chose SCD Type 1 (overwrite changes) for simplicity. In a real scenario, you might want SCD Type 2 if tracking customer moves is important for regional analysis.</p> <p>The dimension includes geographic information (city, state, country) because the business wants regional analysis. Customer segmentation helps with targeted analysis.</p>"},{"location":"resources/Dimensional_Modeling_Cheatseet/#product-dimension-the-what","title":"Product Dimension - The \"What\"","text":"<p>Products are organized in a simple hierarchy: Category \u2192 Brand \u2192 Product. This denormalized approach (keeping all levels in one table) follows star schema principles - it's faster to query than a normalized snowflake approach.</p> <p>We include both cost and list price because: - Cost enables profit analysis - List price vs. unit price in facts shows promotional impact</p>"},{"location":"resources/Dimensional_Modeling_Cheatseet/#date-dimension-the-when","title":"Date Dimension - The \"When\"","text":"<p>The date dimension is crucial for time-based analysis. We include: - Calendar attributes (year, quarter, month) - Day-of-week information for weekday vs. weekend analysis - Holiday flags for seasonal analysis</p> <p>Each date gets its own row (one row per day), and we'll have thousands of rows spanning several years.</p>"},{"location":"resources/Dimensional_Modeling_Cheatseet/#store-dimension-the-where","title":"Store Dimension - The \"Where\"","text":"<p>This covers both physical stores and online channels. The <code>store_type</code> field distinguishes between \"Physical\", \"Online\", and \"Mobile\" channels.</p> <p>Geographic hierarchy (city, state, region) enables territorial analysis for operations and sales management.</p>"},{"location":"resources/Dimensional_Modeling_Cheatseet/#how-the-model-answers-business-questions","title":"How the Model Answers Business Questions","text":"<p>Let's trace through how our design answers the original business questions:</p> <p>\"Which products are our top sellers by month?\" <pre><code>SELECT \n    d.year, d.month_name, p.product_name,\n    SUM(f.quantity) as units_sold,\n    SUM(f.total_amount) as revenue\nFROM fact_sales f\nJOIN dim_date d ON f.date_key = d.date_key\nJOIN dim_product p ON f.product_key = p.product_key\nWHERE d.year = 2024\nGROUP BY d.year, d.month_name, p.product_name\nORDER BY d.month, revenue DESC;\n</code></pre></p> <p>\"How do weekend sales compare to weekday sales?\" <pre><code>SELECT \n    CASE WHEN d.is_weekend = 'Y' THEN 'Weekend' ELSE 'Weekday' END as day_type,\n    COUNT(*) as transaction_count,\n    SUM(f.total_amount) as total_sales,\n    AVG(f.total_amount) as avg_transaction_value\nFROM fact_sales f\nJOIN dim_date d ON f.date_key = d.date_key\nGROUP BY CASE WHEN d.is_weekend = 'Y' THEN 'Weekend' ELSE 'Weekday' END;\n</code></pre></p> <p>Notice how the star schema makes these queries intuitive - we join the fact table to the dimensions we need and aggregate the measures.</p>"},{"location":"resources/Dimensional_Modeling_Cheatseet/#design-trade-offs-and-alternatives","title":"Design Trade-offs and Alternatives","text":"<p>Why Star Instead of Snowflake? We could have normalized the product dimension into separate Category and Brand tables. However, the star schema approach offers: - Simpler queries (fewer joins) - Better performance for analytical workloads - Easier understanding for business users - Most BI tools are optimized for star schemas</p> <p>Why This Grain? Line-item level gives us maximum analytical flexibility. The trade-off is larger fact table size, but storage is usually less expensive than the loss of analytical capability.</p> <p>SCD Type Decisions We chose Type 1 for simplicity, but in production you might want: - Type 2 for Customer if regional sales history matters when customers move - Type 2 for Product if price history analysis is important - Type 1 for Store (store characteristics rarely change significantly)</p>"},{"location":"resources/Dimensional_Modeling_Cheatseet/#extending-the-model","title":"Extending the Model","text":"<p>As business requirements evolve, this model can grow:</p> <p>Additional Dimensions: Promotion, Sales Rep, Payment Method Additional Facts: Returns, Inventory, Customer Service interactions Enhanced Attributes: Customer demographics, product specifications, store performance metrics</p> <p>The key is starting with a solid foundation that answers the core business questions, then evolving the model based on new requirements.</p>"},{"location":"resources/Dimensional_Modeling_Cheatseet/#sample-data-to-visualize-the-concept","title":"Sample Data to Visualize the Concept","text":"<pre><code>-- Sample fact data showing the grain\nsales_key | customer_key | product_key | date_key | store_key | order_number | quantity | unit_price | total_amount\n1001      | 501          | 201         | 20240315 | 101       | ORD-12345    | 2        | 99.99      | 199.98\n1002      | 501          | 203         | 20240315 | 101       | ORD-12345    | 1        | 49.99      | 49.99\n1003      | 502          | 201         | 20240316 | 102       | ORD-12346    | 1        | 99.99      | 99.99\n\n-- Customer dimension sample\ncustomer_key | customer_id | customer_name | city    | state | customer_segment\n501          | CUST-001    | John Smith    | Seattle | WA    | VIP\n502          | CUST-002    | Jane Doe      | Portland| OR    | Regular\n\n-- Product dimension sample  \nproduct_key | product_id | product_name | category    | brand  | list_price\n201         | SKU-001    | iPhone 15    | Electronics | Apple  | 999.99\n203         | SKU-003    | AirPods Pro  | Electronics | Apple  | 249.99\n</code></pre> <p>Notice how the same order (ORD-12345) appears in multiple fact records because John Smith bought two different products. This is the line-item grain in action.</p>"},{"location":"resources/Dimensional_Modeling_Cheatseet/#key-takeaways-for-interviews","title":"Key Takeaways for Interviews","text":"<p>When discussing this model in an interview context, emphasize:</p> <ol> <li>Grain Selection: Explain why line-item level was chosen and what alternatives existed</li> <li>Business Alignment: Show how the design directly supports stated business requirements  </li> <li>Query Performance: Demonstrate how star schema enables fast, intuitive queries</li> <li>Scalability: Discuss how the model can evolve with new requirements</li> <li>Trade-offs: Acknowledge the storage vs. flexibility trade-off and why it's worth it</li> </ol> <p>This example demonstrates solid dimensional modeling fundamentals while remaining simple enough to explain clearly in an interview setting.</p>"},{"location":"resources/Dimensional_Modeling_Cheatseet/#clinical-trial-galaxy-schema-example","title":"Clinical Trial Galaxy Schema Example","text":""},{"location":"resources/Dimensional_Modeling_Cheatseet/#business-scenario_1","title":"Business Scenario","text":"<p>You're working as a data architect for a pharmaceutical company conducting a Phase III clinical trial for a new diabetes medication. The study involves 500 patients across 25 clinical sites over 18 months. The regulatory team, biostatisticians, and clinical operations team have come to you with complex analytical requirements.</p> <p>This scenario differs fundamentally from typical business intelligence challenges because clinical trials operate under strict regulatory requirements (FDA, EMA), require sophisticated statistical analysis, and involve patient safety monitoring. Let's explore how dimensional modeling adapts to these unique challenges.</p>"},{"location":"resources/Dimensional_Modeling_Cheatseet/#understanding-clinical-trial-complexity","title":"Understanding Clinical Trial Complexity","text":"<p>Clinical trials generate multiple types of data that need to be analyzed together:</p> <p>Efficacy Data: How well does the drug work? Primary endpoint is reduction in HbA1c (blood sugar control). Secondary endpoints include weight loss, blood pressure improvement, and quality of life scores.</p> <p>Safety Data: What adverse events occur? Every headache, nausea, or serious event must be tracked, categorized, and analyzed for patterns. Regulatory agencies require detailed safety reporting.</p> <p>Operational Data: Are we following the protocol? Visit scheduling, protocol deviations, and site performance all impact trial success and regulatory approval.</p> <p>The key insight is that we have fundamentally different types of business events happening - patient measurements and adverse events - that need separate fact tables but must be analyzed together. This leads us to a galaxy schema design.</p>"},{"location":"resources/Dimensional_Modeling_Cheatseet/#designing-the-galaxy-schema","title":"Designing the Galaxy Schema","text":"<pre><code>erDiagram\n    FACT_PATIENT_MEASUREMENT {\n        bigint measurement_key\n        int patient_key\n        int visit_key\n        int measurement_type_key\n        int date_key\n        int site_key\n        varchar measurement_id\n        decimal measurement_value\n        varchar measurement_unit\n        decimal baseline_value\n        decimal change_from_baseline\n        char within_normal_range\n        char measurement_quality_flag\n    }\n\n    FACT_ADVERSE_EVENT {\n        bigint adverse_event_key\n        int patient_key\n        int visit_key\n        int event_type_key\n        int date_key\n        int severity_key\n        varchar event_id\n        varchar event_description\n        int duration_days\n        char related_to_study_drug\n        char serious_event_flag\n        char resolved_flag\n    }\n\n    DIM_PATIENT {\n        int patient_key\n        varchar patient_id\n        varchar site_patient_id\n        int age_at_enrollment\n        varchar gender\n        varchar race\n        varchar ethnicity\n        decimal baseline_hba1c\n        decimal baseline_weight\n        int diabetes_duration_years\n        varchar enrollment_status\n        date enrollment_date\n        date completion_date\n    }\n\n    DIM_VISIT {\n        int visit_key\n        varchar visit_code\n        varchar visit_name\n        varchar visit_type\n        int visit_number\n        int study_day\n        varchar visit_window\n        char baseline_visit_flag\n        char primary_endpoint_flag\n        varchar visit_status\n    }\n\n    DIM_MEASUREMENT_TYPE {\n        int measurement_type_key\n        varchar measurement_code\n        varchar measurement_name\n        varchar measurement_category\n        varchar body_system\n        decimal normal_range_min\n        decimal normal_range_max\n        varchar standard_unit\n        char primary_endpoint_flag\n        char safety_parameter_flag\n    }\n\n    DIM_SITE {\n        int site_key\n        varchar site_id\n        varchar site_name\n        varchar principal_investigator\n        varchar city\n        varchar state\n        varchar country\n        varchar site_type\n        date site_activation_date\n        char site_status\n    }\n\n    DIM_DATE {\n        int date_key\n        date full_date\n        int year\n        int quarter\n        int month\n        varchar month_name\n        int week\n        int day_of_week\n        varchar day_name\n        int study_day\n        varchar study_phase\n        char is_visit_window\n    }\n\n    DIM_EVENT_TYPE {\n        int event_type_key\n        varchar event_code\n        varchar event_term\n        varchar system_organ_class\n        varchar preferred_term\n        varchar event_category\n        char expected_event_flag\n        varchar regulatory_classification\n    }\n\n    DIM_SEVERITY {\n        int severity_key\n        varchar severity_code\n        varchar severity_name\n        varchar severity_description\n        int severity_rank\n        char hospitalization_required\n        char life_threatening_flag\n    }\n\n    DIM_PATIENT ||--o{ FACT_PATIENT_MEASUREMENT : \"measured_for\"\n    DIM_VISIT ||--o{ FACT_PATIENT_MEASUREMENT : \"during_visit\"\n    DIM_MEASUREMENT_TYPE ||--o{ FACT_PATIENT_MEASUREMENT : \"type_of\"\n    DIM_DATE ||--o{ FACT_PATIENT_MEASUREMENT : \"on_date\"\n    DIM_SITE ||--o{ FACT_PATIENT_MEASUREMENT : \"at_site\"\n\n    DIM_PATIENT ||--o{ FACT_ADVERSE_EVENT : \"experienced_by\"\n    DIM_VISIT ||--o{ FACT_ADVERSE_EVENT : \"during_visit\"\n    DIM_EVENT_TYPE ||--o{ FACT_ADVERSE_EVENT : \"type_of\"\n    DIM_DATE ||--o{ FACT_ADVERSE_EVENT : \"on_date\"\n    DIM_SEVERITY ||--o{ FACT_ADVERSE_EVENT : \"severity_level\"</code></pre>"},{"location":"resources/Dimensional_Modeling_Cheatseet/#why-galaxy-schema-instead-of-star","title":"Why Galaxy Schema Instead of Star?","text":"<p>The fundamental decision here is using multiple fact tables instead of trying to force everything into a single fact. Here's the reasoning:</p> <p>Different Grains: Patient measurements happen at scheduled visits with precise protocols. Adverse events can happen anytime and are reported whenever discovered. Trying to combine these into one fact table would create a confusing grain.</p> <p>Different Measures: Efficacy measurements are quantitative (HbA1c = 7.2%) while adverse events are qualitative (severity = \"Moderate\"). The measures don't make sense together.</p> <p>Different Analysis Patterns: Efficacy analysis focuses on statistical change over time. Safety analysis focuses on event rates and patterns. These require different analytical approaches.</p> <p>Regulatory Requirements: FDA submissions require separate efficacy and safety sections with different statistical methodologies. Our data model should align with regulatory reporting needs.</p>"},{"location":"resources/Dimensional_Modeling_Cheatseet/#fact-table-design-deep-dive","title":"Fact Table Design Deep Dive","text":""},{"location":"resources/Dimensional_Modeling_Cheatseet/#patient-measurement-fact-the-efficacy-story","title":"Patient Measurement Fact - The Efficacy Story","text":"<p>This fact table captures every measurement taken on every patient at every visit. The grain is: one row per measurement type per patient per visit.</p> <p>Pre-calculated Change from Baseline: Rather than calculating this at query time, we store <code>baseline_value</code> and <code>change_from_baseline</code>. This is crucial because: - Baseline calculations are complex (what if a patient missed their baseline visit?) - Statistical software expects pre-calculated changes - Performance is critical for large datasets with frequent analysis</p> <p>Data Quality Flags: <code>within_normal_range</code> and <code>measurement_quality_flag</code> capture data quality issues that are crucial for regulatory submissions. Outlier values need flagging and explanation.</p> <p>Why This Grain: We could aggregate to visit level, but measurement-level detail enables: - Analysis of specific biomarkers - Missing data pattern analysis - Quality control at the measurement level</p>"},{"location":"resources/Dimensional_Modeling_Cheatseet/#adverse-event-fact-the-safety-story","title":"Adverse Event Fact - The Safety Story","text":"<p>This fact table captures every adverse event reported during the study. The grain is: one row per adverse event occurrence.</p> <p>Relationship to Study Drug: <code>related_to_study_drug</code> is a critical regulatory field. Investigators must assess whether each event is related to the study medication.</p> <p>Duration Tracking: <code>duration_days</code> enables analysis of how long events last, which impacts quality of life and drug tolerability.</p> <p>Regulatory Classification: Different event types have different regulatory reporting requirements. Serious events must be reported to authorities within 24 hours.</p>"},{"location":"resources/Dimensional_Modeling_Cheatseet/#dimension-design-for-clinical-context","title":"Dimension Design for Clinical Context","text":""},{"location":"resources/Dimensional_Modeling_Cheatseet/#patient-dimension-the-study-population","title":"Patient Dimension - The Study Population","text":"<p>Unlike typical customer dimensions, patient characteristics are mostly fixed at enrollment. We use SCD Type 1 because: - Demographics don't change during the study - Medical history is captured at baseline - Any changes (like diabetes duration) are captured through measurements, not dimension updates</p> <p>Baseline Medical Information: <code>baseline_hba1c</code>, <code>baseline_weight</code>, and <code>diabetes_duration_years</code> enable subgroup analysis. Regulatory agencies want to know if the drug works differently in different patient populations.</p> <p>Enrollment Tracking: <code>enrollment_status</code> tracks patients through the study lifecycle (Screened, Enrolled, Completed, Withdrawn). This is crucial for statistical analysis and regulatory compliance.</p>"},{"location":"resources/Dimensional_Modeling_Cheatseet/#visit-dimension-the-protocol-framework","title":"Visit Dimension - The Protocol Framework","text":"<p>Clinical trials follow strict visit schedules defined in the protocol. This dimension captures that structure:</p> <p>Study Day: Calculated from enrollment date, enabling analysis aligned with the study timeline rather than calendar dates.</p> <p>Visit Windows: Protocols allow visits within acceptable windows (e.g., Week 12 visit can occur between Day 80-90). <code>visit_window</code> enables protocol compliance analysis.</p> <p>Primary Endpoint Visits: <code>primary_endpoint_flag</code> identifies visits where the main efficacy measurement occurs. These are crucial for the primary statistical analysis.</p>"},{"location":"resources/Dimensional_Modeling_Cheatseet/#measurement-type-dimension-clinical-standardization","title":"Measurement Type Dimension - Clinical Standardization","text":"<p>This dimension standardizes all the different measurements taken in the study:</p> <p>Clinical Hierarchy: <code>body_system</code> and <code>measurement_category</code> organize measurements (Endocrine \u2192 Glucose Control \u2192 HbA1c).</p> <p>Reference Ranges: <code>normal_range_min</code> and <code>normal_range_max</code> enable automated flagging of abnormal values for safety monitoring.</p> <p>Study Relevance: <code>primary_endpoint_flag</code> and <code>safety_parameter_flag</code> distinguish between different analysis categories for regulatory reporting.</p>"},{"location":"resources/Dimensional_Modeling_Cheatseet/#event-type-dimension-medical-coding","title":"Event Type Dimension - Medical Coding","text":"<p>Adverse events must be coded using standardized medical terminology (MedDRA - Medical Dictionary for Regulatory Activities):</p> <p>Hierarchy: <code>system_organ_class</code> \u2192 <code>preferred_term</code> \u2192 <code>event_term</code> follows MedDRA structure. For example: \"Gastrointestinal disorders\" \u2192 \"Nausea\" \u2192 \"Nausea after meals\".</p> <p>Regulatory Classification: <code>expected_event_flag</code> indicates whether the event is listed in the drug's safety profile. Unexpected events require special reporting.</p>"},{"location":"resources/Dimensional_Modeling_Cheatseet/#complex-analysis-examples","title":"Complex Analysis Examples","text":""},{"location":"resources/Dimensional_Modeling_Cheatseet/#primary-efficacy-analysis","title":"Primary Efficacy Analysis","text":"<pre><code>-- Mean change from baseline in HbA1c by visit\nSELECT \n    v.visit_name,\n    v.study_day,\n    COUNT(DISTINCT pm.patient_key) as patient_count,\n    AVG(pm.change_from_baseline) as mean_change,\n    STDDEV(pm.change_from_baseline) as std_deviation\nFROM fact_patient_measurement pm\nJOIN dim_measurement_type mt ON pm.measurement_type_key = mt.measurement_type_key\nJOIN dim_visit v ON pm.visit_key = v.visit_key\nWHERE mt.measurement_code = 'HBA1C'\n    AND v.primary_endpoint_flag = 'Y'\nGROUP BY v.visit_name, v.study_day, v.visit_number\nORDER BY v.visit_number;\n</code></pre> <p>This query answers the fundamental question: \"Is the drug working?\" Statistical significance testing would be applied to these results.</p>"},{"location":"resources/Dimensional_Modeling_Cheatseet/#safety-signal-detection","title":"Safety Signal Detection","text":"<pre><code>-- Adverse events by body system - looking for concerning patterns\nSELECT \n    et.system_organ_class,\n    COUNT(*) as total_events,\n    COUNT(DISTINCT ae.patient_key) as patients_affected,\n    COUNT(DISTINCT ae.patient_key) * 100.0 / \n        (SELECT COUNT(DISTINCT patient_key) FROM dim_patient WHERE enrollment_status = 'Enrolled') as incidence_rate,\n    SUM(CASE WHEN ae.related_to_study_drug = 'Y' THEN 1 ELSE 0 END) as drug_related_events\nFROM fact_adverse_event ae\nJOIN dim_event_type et ON ae.event_type_key = et.event_type_key\nGROUP BY et.system_organ_class\nORDER BY incidence_rate DESC;\n</code></pre> <p>This analysis helps identify safety</p>"},{"location":"resources/Terraform_Data_Engineer_Cheatsheet/","title":"Terraform Cheatsheet","text":"<p>AI Assisted (Grok 3)</p> <p>A cheatsheet which is actually useful to setup production grade IaC project using Terraform and Gitlab covering :</p> <ul> <li>Multi-Environment setup</li> <li>CI/CD</li> <li>State Management</li> <li>Rollbacks</li> <li>Capturing and Logging Outputs</li> </ul> <p>And more..</p>"},{"location":"resources/Terraform_Data_Engineer_Cheatsheet/#1-core-concepts","title":"1. Core Concepts","text":"<ul> <li>Infrastructure as Code (IaC): Define  infrastructure (e.g., Lambdas, databases, data lakes, compute clusters) in <code>.tf</code> files.</li> <li>Providers: Plugins for cloud platforms (e.g., <code>aws</code>, <code>google</code>, <code>azurerm</code>) or tools (e.g., <code>snowflake</code>, <code>databricks</code>).</li> <li>Resources: Infrastructure components (e.g., S3 buckets, RDS instances, BigQuery datasets).</li> <li>Modules: Reusable, parameterized templates for common infrastructure patterns.</li> <li>State: Tracks infrastructure state, stored locally (<code>terraform.tfstate</code>) or remotely (e.g., S3, GCS).</li> <li>Workspaces: Isolate environments (e.g., <code>dev</code>, <code>staging</code>, <code>prod</code>) within a single configuration directory using multiple state files. Note: This project uses folder-based isolation (separate directories for each environment) instead of workspaces for better clarity and flexibility.</li> </ul>"},{"location":"resources/Terraform_Data_Engineer_Cheatsheet/#2-key-terraform-components-and-state-management","title":"2. Key Terraform Components and State Management","text":"<ul> <li>State Management: <ul> <li>The <code>terraform.tfstate</code> file records the current state of your infrastructure, mapping Terraform configurations to real-world resources, including dynamically generated attributes (e.g., S3 bucket ARNs, RDS endpoints).</li> <li>Local State: Stored locally by default; suitable for solo or small projects but risky for teams due to potential conflicts or loss.</li> <li>Remote State: Store state in a remote backend (e.g., S3, GCS, Azure Blob) with locking (e.g., DynamoDB) to prevent concurrent modifications and enable team collaboration.</li> <li>Environment Isolation: This project uses separate directories (<code>environments/dev</code>, <code>environments/staging</code>, <code>environments/prod</code>) with distinct state files (e.g., <code>data-infra/dev/terraform.tfstate</code>) instead of Terraform workspaces, providing clear separation and environment-specific configurations.</li> <li>Best Practices: Use remote backends for production, encrypt state files, restrict access via IAM, and regularly back up state files.</li> <li>State Drift: Occurs when manual changes bypass Terraform. Use <code>terraform refresh</code> to update state or <code>terraform import</code> to bring resources under management.</li> </ul> </li> <li> <p>outputs.tf: </p> <ul> <li>Defines output values exposed after <code>terraform apply</code>, such as resource IDs, endpoints, or computed values (e.g., S3 bucket ARN, RDS endpoint).</li> <li>How It Works: Outputs reference attributes stored in the state file, which captures dynamically generated values during resource creation. <code>outputs.tf</code> itself does not store values but declares what to extract from the state.</li> <li>Example:    <pre><code>output \"s3_bucket_arn\" {\n  value       = aws_s3_bucket.bucket.arn\n  description = \"ARN of the S3 bucket\"\n}\n</code></pre></li> <li>After <code>terraform apply</code>, the state file stores the bucket\u2019s ARN, and the output retrieves it.</li> <li>Updating Outputs: Edit <code>outputs.tf</code> in the repository to add, modify, or remove outputs. Commit changes to Git, and they take effect on the next <code>terraform apply</code> or <code>terraform output</code>.</li> <li>Usage: Outputs are displayed in the CLI (<code>terraform output</code>), exported to JSON (<code>terraform output -json</code>), used in CI/CD pipelines, or referenced by other modules via remote state.</li> <li>Best Practices: Use descriptive names and descriptions, avoid sensitive data in outputs, and capture outputs in pipelines for automation.</li> </ul> </li> <li> <p>variables.tf: Declares input variables with types, defaults, and validations to parameterize configurations.</p> </li> <li>terraform.tfvars: Provides default variable values, overridden by environment-specific <code>.tfvars</code> files or CLI flags.</li> <li>backend.tf: Configures the remote state backend, specifying where and how state is stored and locked.</li> </ul>"},{"location":"resources/Terraform_Data_Engineer_Cheatsheet/#3-repository-folder-structure","title":"3. Repository Folder Structure","text":"<p>A well-organized repository structure ensures clarity, modularity, and scalability for managing data infrastructure across multiple environments.</p> <pre><code>data-infra/\n\u251c\u2500\u2500 modules/                     # Reusable Terraform modules\n\u2502   \u251c\u2500\u2500 s3_bucket/               # S3 bucket module\n\u2502   \u2502   \u251c\u2500\u2500 main.tf\n\u2502   \u2502   \u251c\u2500\u2500 variables.tf\n\u2502   \u2502   \u2514\u2500\u2500 outputs.tf\n\u2502   \u251c\u2500\u2500 rds/                     # RDS module\n\u2502   \u2502   \u251c\u2500\u2500 main.tf\n\u2502   \u2502   \u251c\u2500\u2500 variables.tf\n\u2502   \u2502   \u2514\u2500\u2500 outputs.tf\n\u2502   \u251c\u2500\u2500 databricks/              # Databricks module\n\u2502   \u2502   \u251c\u2500\u2500 main.tf\n\u2502   \u2502   \u251c\u2500\u2500 variables.tf\n\u2502   \u2502   \u2514\u2500\u2500 outputs.tf\n\u2502   \u2514\u2500\u2500 glue_job/                # Glue job module\n\u2502       \u251c\u2500\u2500 main.tf\n\u2502       \u251c\u2500\u2500 variables.tf\n\u2502       \u2514\u2500\u2500 outputs.tf\n\u251c\u2500\u2500 environments/                # Environment-specific configurations\n\u2502   \u251c\u2500\u2500 dev/\n\u2502   \u2502   \u251c\u2500\u2500 main.tf            # Dev-specific resources\n\u2502   \u2502   \u251c\u2500\u2500 variables.tf\n\u2502   \u2502   \u251c\u2500\u2500 outputs.tf         # Environment-specific outputs\n\u2502   \u2502   \u251c\u2500\u2500 backend.tf\n\u2502   \u2502   \u2514\u2500\u2500 terraform.tfvars    # Dev variable values\n\u2502   \u251c\u2500\u2500 staging/\n\u2502   \u2502   \u251c\u2500\u2500 main.tf\n\u2502   \u2502   \u251c\u2500\u2500 variables.tf\n\u2502   \u2502   \u251c\u2500\u2500 outputs.tf\n\u2502   \u2502   \u251c\u2500\u2500 backend.tf\n\u2502   \u2502   \u2514\u2500\u2500 terraform.tfvars\n\u2502   \u2514\u2500\u2500 prod/\n\u2502       \u251c\u2500\u2500 main.tf\n\u2502       \u251c\u2500\u2500 variables.tf\n\u2502       \u251c\u2500\u2500 outputs.tf\n\u2502       \u251c\u2500\u2500 backend.tf\n\u2502       \u2514\u2500\u2500 terraform.tfvars\n\u251c\u2500\u2500 scripts/                     # Supporting scripts (e.g., PySpark for Glue)\n\u2502   \u2514\u2500\u2500 transform.py\n\u251c\u2500\u2500 .gitignore                   # Git ignore file\n\u251c\u2500\u2500 README.md                    # Project documentation\n\u2514\u2500\u2500 .gitlab-ci.yml               # CI/CD pipeline configuration\n</code></pre>"},{"location":"resources/Terraform_Data_Engineer_Cheatsheet/#explanation","title":"Explanation","text":"<ul> <li>modules/: Contains reusable modules for resources like S3, RDS, Databricks, and Glue jobs, each with its own <code>outputs.tf</code>.</li> <li>environments/: Separates configurations for <code>dev</code>, <code>staging</code>, and <code>prod</code>, each with its own <code>main.tf</code>, <code>variables.tf</code>, <code>outputs.tf</code>, <code>backend.tf</code>, and <code>terraform.tfvars</code>, providing isolation without workspaces.</li> <li>scripts/: Stores scripts (e.g., PySpark scripts for Glue jobs) uploaded to S3.</li> <li>.gitignore: Ignores <code>.terraform/</code>, <code>*.tfstate</code>, <code>*.tfstate.backup</code>, and sensitive files like <code>*.tfvars</code> with secrets.</li> <li>README.md: Documents setup instructions, module usage, CI/CD pipeline details, artifact review, rollback procedures, output management, and environment isolation.</li> </ul>"},{"location":"resources/Terraform_Data_Engineer_Cheatsheet/#4-basic-commands","title":"4. Basic Commands","text":"<pre><code>terraform init            # Initialize project, download providers\nterraform plan            # Preview changes\nterraform apply           # Apply changes\nterraform destroy         # Tear down infrastructure\nterraform state list      # View managed resources\nterraform state show &lt;resource&gt;   # Inspect resource state\nterraform output          # Display all output values\nterraform output -json    # Export outputs as JSON\n# Note: The following workspace commands are included for reference but are not used in this project, which uses folder-based environment isolation instead.\nterraform workspace list  # List workspaces (not used)\nterraform workspace select &lt;env&gt;  # Switch to environment (not used)\nterraform workspace new &lt;env&gt;     # Create new workspace (not used)\n</code></pre>"},{"location":"resources/Terraform_Data_Engineer_Cheatsheet/#5-provider-configuration","title":"5. Provider Configuration","text":"<pre><code># environments/dev/main.tf\nterraform {\n  required_providers {\n    aws = {\n      source  = \"hashicorp/aws\"\n      version = \"~&gt; 5.0\"\n    }\n    snowflake = {\n      source  = \"Snowflake-Labs/snowflake\"\n      version = \"~&gt; 0.76\"\n    }\n  }\n}\n\nprovider \"aws\" {\n  region = var.region\n  assume_role {\n    role_arn = \"arn:aws:iam::${var.account_id}:role/TerraformRole\"\n  }\n}\n\nprovider \"snowflake\" {\n  account = var.snowflake_account\n  role    = \"ACCOUNTADMIN\"\n}\n</code></pre>"},{"location":"resources/Terraform_Data_Engineer_Cheatsheet/#6-multi-environment-configuration","title":"6. Multi-Environment Configuration","text":""},{"location":"resources/Terraform_Data_Engineer_Cheatsheet/#variables","title":"Variables","text":"<pre><code># environments/dev/variables.tf\nvariable \"region\" {\n  type    = string\n  default = \"us-east-1\"\n}\n\nvariable \"account_id\" {\n  type = string\n}\n\nvariable \"environment\" {\n  type = string\n  validation {\n    condition     = contains([\"dev\", \"staging\", \"prod\"], var.environment)\n    error_message = \"Environment must be dev, staging, or prod.\"\n  }\n}\n</code></pre>"},{"location":"resources/Terraform_Data_Engineer_Cheatsheet/#environment-specific-tfvars","title":"Environment-Specific <code>.tfvars</code>","text":"<pre><code># environments/dev/terraform.tfvars\nregion      = \"us-east-1\"\naccount_id  = \"123456789012\"\nenvironment = \"dev\"\n</code></pre>"},{"location":"resources/Terraform_Data_Engineer_Cheatsheet/#applying-environment-config","title":"Applying Environment Config","text":"<pre><code>cd environments/dev\nterraform init\nterraform apply -var-file=terraform.tfvars\n</code></pre>"},{"location":"resources/Terraform_Data_Engineer_Cheatsheet/#7-remote-state-management","title":"7. Remote State Management","text":"<pre><code># environments/dev/backend.tf\nterraform {\n  backend \"s3\" {\n    bucket         = \"my-terraform-state\"\n    key            = \"data-infra/dev/terraform.tfstate\"\n    region         = \"us-east-1\"\n    dynamodb_table = \"terraform-locks\"\n  }\n}\n</code></pre>"},{"location":"resources/Terraform_Data_Engineer_Cheatsheet/#8-module-example-s3-bucket","title":"8. Module Example: S3 Bucket","text":"<pre><code># modules/s3_bucket/main.tf\nresource \"aws_s3_bucket\" \"bucket\" {\n  bucket = \"${var.environment}-${var.bucket_name}\"\n  acl    = \"private\"\n\n  versioning {\n    enabled = true\n  }\n\n  tags = {\n    Environment = var.environment\n  }\n}\n\n# modules/s3_bucket/variables.tf\nvariable \"bucket_name\" {\n  type = string\n}\n\nvariable \"environment\" {\n  type = string\n}\n\n# modules/s3_bucket/outputs.tf\noutput \"bucket_arn\" {\n  value       = aws_s3_bucket.bucket.arn\n  description = \"ARN of the S3 bucket\"\n}\n\noutput \"bucket_name\" {\n  value       = aws_s3_bucket.bucket.id\n  description = \"Name of the S3 bucket\"\n}\n</code></pre>"},{"location":"resources/Terraform_Data_Engineer_Cheatsheet/#using-the-module","title":"Using the Module","text":"<pre><code># environments/dev/main.tf\nmodule \"data_lake\" {\n  source      = \"../../modules/s3_bucket\"\n  bucket_name = \"data-lake\"\n  environment = var.environment\n}\n\n# environments/dev/outputs.tf\noutput \"data_lake_arn\" {\n  value       = module.data_lake.bucket_arn\n  description = \"ARN of the data lake S3 bucket\"\n}\n</code></pre>"},{"location":"resources/Terraform_Data_Engineer_Cheatsheet/#9-data-infrastructure-examples","title":"9. Data Infrastructure Examples","text":""},{"location":"resources/Terraform_Data_Engineer_Cheatsheet/#snowflake-database","title":"Snowflake Database","text":"<pre><code># Fetch existing Snowflake role\ndata \"snowflake_role\" \"admin\" {\n  name = \"ACCOUNTADMIN\"\n}\n\nresource \"snowflake_database\" \"data_warehouse\" {\n  name = \"${var.environment}_DATA_WAREHOUSE\"\n}\n\nresource \"snowflake_schema\" \"raw\" {\n  database = snowflake_database.data_warehouse.name\n  name     = \"RAW\"\n}\n\n# outputs.tf\noutput \"snowflake_database_name\" {\n  value       = snowflake_database.data_warehouse.name\n  description = \"Name of the Snowflake database\"\n}\n</code></pre>"},{"location":"resources/Terraform_Data_Engineer_Cheatsheet/#databricks-cluster","title":"Databricks Cluster","text":"<pre><code># Fetch existing Databricks workspace\ndata \"databricks_spark_version\" \"latest\" {\n  latest = true\n}\n\nresource \"databricks_cluster\" \"data_processing\" {\n  cluster_name            = \"${var.environment}-data-processing\"\n  spark_version           = data.databricks_spark_version.latest.id\n  node_type_id            = var.environment == \"prod\" ? \"i3.2xlarge\" : \"i3.xlarge\"\n  autotermination_minutes = 30\n}\n\n# outputs.tf\noutput \"databricks_cluster_id\" {\n  value       = databricks_cluster.data_processing.cluster_id\n  description = \"ID of the Databricks cluster\"\n}\n</code></pre>"},{"location":"resources/Terraform_Data_Engineer_Cheatsheet/#aws-glue-crawler","title":"AWS Glue Crawler","text":"<pre><code># Fetch existing Glue database\ndata \"aws_glue_catalog_database\" \"main\" {\n  name = \"${var.environment}_catalog\"\n}\n\nresource \"aws_glue_crawler\" \"s3_crawler\" {\n  name          = \"${var.environment}-s3-crawler\"\n  database_name = data.aws_glue_catalog_database.main.name\n  role          = aws_iam_role.glue_role.arn\n  s3_target {\n    path = module.data_lake.bucket\n  }\n}\n\n# outputs.tf\noutput \"glue_crawler_name\" {\n  value       = aws_glue_crawler.s3_crawler.name\n  description = \"Name of the Glue crawler\"\n}\n</code></pre>"},{"location":"resources/Terraform_Data_Engineer_Cheatsheet/#aws-rds-database","title":"AWS RDS Database","text":"<pre><code># Fetch existing VPC and subnets\ndata \"aws_vpc\" \"default\" {\n  default = true\n}\n\ndata \"aws_subnets\" \"default_subnets\" {\n  filter {\n    name   = \"vpc-id\"\n    values = [data.aws_vpc.default.id]\n  }\n}\n\n# modules/rds/main.tf\nresource \"aws_db_subnet_group\" \"rds_subnet_group\" {\n  name       = \"${var.environment}-rds-subnet-group\"\n  subnet_ids = var.subnet_ids\n  tags = {\n    Environment = var.environment\n  }\n}\n\nresource \"aws_db_instance\" \"rds_instance\" {\n  identifier              = \"${var.environment}-${var.db_name}\"\n  engine                  = var.db_engine\n  engine_version          = var.engine_version\n  instance_class          = var.environment == \"prod\" ? var.prod_instance_class : var.dev_instance_class\n  allocated_storage       = var.allocated_storage\n  username                = var.db_username\n  password                = var.db_password\n  db_subnet_group_name    = aws_db_subnet_group.rds_subnet_group.name\n  vpc_security_group_ids  = var.security_group_ids\n  multi_az                = var.environment == \"prod\" ? true : false\n  backup_retention_period = var.environment == \"prod\" ? 7 : 1\n  skip_final_snapshot     = var.environment != \"prod\"\n  tags = {\n    Environment = var.environment\n  }\n}\n\n# modules/rds/variables.tf\nvariable \"db_name\" {\n  type = string\n}\n\nvariable \"environment\" {\n  type = string\n}\n\nvariable \"db_engine\" {\n  type    = string\n  default = \"postgres\"\n}\n\nvariable \"engine_version\" {\n  type    = string\n  default = \"15.3\"\n}\n\nvariable \"prod_instance_class\" {\n  type    = string\n  default = \"db.m5.large\"\n}\n\nvariable \"dev_instance_class\" {\n  type    = string\n  default = \"db.t3.micro\"\n}\n\nvariable \"allocated_storage\" {\n  type    = number\n  default = 20\n}\n\nvariable \"db_username\" {\n  type = string\n}\n\nvariable \"db_password\" {\n  type      = string\n  sensitive = true\n}\n\nvariable \"subnet_ids\" {\n  type = list(string)\n}\n\nvariable \"security_group_ids\" {\n  type = list(string)\n}\n\n# modules/rds/outputs.tf\noutput \"rds_endpoint\" {\n  value       = aws_db_instance.rds_instance.endpoint\n  description = \"Endpoint of the RDS instance\"\n}\n\n# Using the RDS Module\n# environments/dev/main.tf\nmodule \"rds_postgres\" {\n  source             = \"../../modules/rds\"\n  db_name            = \"datawarehouse\"\n  environment        = var.environment\n  db_engine          = \"postgres\"\n  engine_version     = \"15.3\"\n  prod_instance_class = \"db.m5.large\"\n  dev_instance_class  = \"db.t3.micro\"\n  allocated_storage   = 100\n  db_username        = var.db_username\n  db_password        = var.db_password\n  subnet_ids         = data.aws_subnets.default_subnets.ids\n  security_group_ids = [aws_security_group.rds_sg.id]\n}\n\n# environments/dev/outputs.tf\noutput \"rds_postgres_endpoint\" {\n  value       = module.rds_postgres.rds_endpoint\n  description = \"Endpoint of the RDS PostgreSQL instance\"\n}\n</code></pre>"},{"location":"resources/Terraform_Data_Engineer_Cheatsheet/#aws-glue-pyspark-job","title":"AWS Glue PySpark Job","text":"<pre><code># Fetch S3 bucket from another configuration\ndata \"terraform_remote_state\" \"data_lake\" {\n  backend = \"s3\"\n  config = {\n    bucket = \"my-terraform-state\"\n    key    = \"data-infra/${var.environment}/terraform.tfstate\"\n    region = \"us-east-1\"\n  }\n}\n\n# modules/glue_job/main.tf\nresource \"aws_glue_job\" \"pyspark_job\" {\n  name              = \"${var.environment}-${var.job_name}\"\n  role_arn          = var.glue_role_arn\n  glue_version      = \"4.0\"\n  worker_type       = var.environment == \"prod\" ? \"G.2X\" : \"G.1X\"\n  number_of_workers = var.environment == \"prod\" ? 10 : 2\n  max_retries       = 1\n\n  command {\n    script_location = \"s3://${var.script_bucket}/${var.script_path}\"\n    python_version  = \"python3\"\n    name            = \"glueetl\"\n  }\n\n  default_arguments = {\n    \"--job-name\"              = \"${var.environment}-${var.job_name}\"\n    \"--TempDir\"               = \"s3://${var.temp_bucket}/temp/\"\n    \"--enable-metrics\"         = \"true\"\n    \"--enable-continuous-log\" = \"true\"\n    \"--environment\"           = var.environment\n  }\n\n  tags = {\n    Environment = var.environment\n  }\n}\n\n# modules/glue_job/variables.tf\nvariable \"job_name\" {\n  type = string\n}\n\nvariable \"environment\" {\n  type = string\n}\n\nvariable \"glue_role_arn\" {\n  type = string\n}\n\nvariable \"script_bucket\" {\n  type = string\n}\n\nvariable \"script_path\" {\n  type = string\n}\n\nvariable \"temp_bucket\" {\n  type = string\n}\n\n# modules/glue_job/outputs.tf\noutput \"glue_job_name\" {\n  value       = aws_glue_job.pyspark_job.name\n  description = \"Name of the Glue PySpark job\"\n}\n\n# Using the Glue Job Module\n# environments/dev/main.tf\nmodule \"glue_job\" {\n  source         = \"../../modules/glue_job\"\n  job_name       = \"data-transform\"\n  environment    = var.environment\n  glue_role_arn  = aws_iam_role.glue_role.arn\n  script_bucket  = data.terraform_remote_state.data_lake.outputs.bucket_name\n  script_path    = \"scripts/transform.py\"\n  temp_bucket    = data.terraform_remote_state.data_lake.outputs.bucket_name\n}\n\n# environments/dev/outputs.tf\noutput \"glue_job_name\" {\n  value       = module.glue_job.glue_job_name\n  description = \"Name of the Glue job\"\n}\n</code></pre>"},{"location":"resources/Terraform_Data_Engineer_Cheatsheet/#10-gitlab-cicd-pipeline-for-terraform","title":"10. GitLab CI/CD Pipeline for Terraform","text":"<p>A GitLab CI/CD pipeline automates validation, planning, and deployment of Terraform configurations across environments, with artifacts stored for review, auditing, and output capture. This project uses folder-based environment isolation (folders like <code>environments/dev</code>, <code>environments/staging</code>, etc.) instead of Terraform workspaces for clarity and flexibility.</p>"},{"location":"resources/Terraform_Data_Engineer_Cheatsheet/#terraform-artifacts","title":"Terraform Artifacts","text":"<p>Terraform artifacts are files generated during pipeline jobs (e.g., <code>terraform plan</code>, <code>terraform apply</code>, or <code>terraform output</code>) saved in GitLab for review, auditing, or downstream use. They include:</p> <ul> <li> <p>Plan Artifacts:</p> <ul> <li><code>tfplan</code>: Binary plan file from <code>terraform plan -out=tfplan</code>, used by <code>terraform apply</code>.</li> <li><code>tfplan.json</code>: JSON representation (<code>terraform show -json tfplan</code>) for programmatic analysis.</li> <li><code>tfplan.txt</code>: Human-readable text (<code>terraform show tfplan</code>) for manual review.</li> </ul> </li> <li> <p>Apply Artifacts:</p> <ul> <li><code>apply.log</code>: Log file capturing <code>terraform apply</code> output, detailing infrastructure changes.</li> </ul> </li> <li> <p>Output Artifacts:</p> <ul> <li><code>outputs.json</code>: JSON file capturing <code>terraform output -json</code>, containing dynamically generated values (e.g., S3 bucket ARNs, RDS endpoints).</li> </ul> </li> <li> <p>Purpose:</p> <ul> <li>Review: Plan artifacts for verifying changes before applying.</li> <li>Audit: Apply logs and outputs for compliance and debugging.</li> <li>Consistency: Binary <code>tfplan</code> ensures <code>apply</code> matches the reviewed plan.</li> <li>Automation: Outputs enable integration with other systems or modules.</li> <li>Retention:</li> <li>GitLab artifacts can be stored for a specified period (e.g., <code>1 year</code> or <code>forever</code>, subject to GitLab plan).</li> <li>For long-term audit, upload artifacts to S3 with custom retention policies.</li> </ul> </li> </ul>"},{"location":"resources/Terraform_Data_Engineer_Cheatsheet/#capturing-terraform-outputs","title":"Capturing Terraform Outputs","text":"<p>Outputs defined in <code>outputs.tf</code> are captured in the pipeline to expose dynamically generated values for review, automation, or integration with other systems.</p>"},{"location":"resources/Terraform_Data_Engineer_Cheatsheet/#steps-to-capture-outputs","title":"Steps to Capture Outputs","text":"<ol> <li>Generate Outputs: Run <code>terraform output -json &gt; outputs.json</code> in the <code>apply</code> job to save outputs as a JSON file.</li> <li>Store as Artifacts: Include <code>outputs.json</code> in the <code>artifacts</code> section of <code>.gitlab-ci.yml</code>.</li> <li>Review Outputs: Download <code>outputs.json</code> from the GitLab pipeline UI or parse it programmatically (e.g., <code>jq '.s3_bucket_arn.value' outputs.json</code>).</li> <li>Use Outputs: Reference <code>outputs.json</code> in downstream jobs (e.g., to configure applications) or upload to S3 for long-term storage.</li> </ol>"},{"location":"resources/Terraform_Data_Engineer_Cheatsheet/#example-workflow","title":"Example Workflow","text":"<ul> <li>After <code>apply_dev</code> runs, <code>outputs.json</code> is generated, containing values like <code>data_lake_arn</code> and <code>rds_postgres_endpoint</code>.</li> <li>The team downloads <code>outputs.json</code> to verify resource endpoints.</li> <li>Outputs are uploaded to S3 for audit or used in another job to configure a data pipeline.</li> </ul>"},{"location":"resources/Terraform_Data_Engineer_Cheatsheet/#pipeline-configuration","title":"Pipeline Configuration","text":"<pre><code># .gitlab-ci.yml\nstages:\n  - validate\n  - plan\n  - apply\n  - backup\n\nvariables:\n  AWS_REGION: \"us-east-1\"\n  TF_VAR_account_id: \"123456789012\"\n\n# Cache Terraform plugins\ncache:\n  key: ${CI_PROJECT_ID}\n  paths:\n    - .terraform/\n    - environments/dev/.terraform/\n    - environments/staging/.terraform/\n    - environments/prod/.terraform/\n\n# Base job template\n.terraform_base:\n  image: hashicorp/terraform:1.5.7\n  before_script:\n    - terraform --version\n    - apk add --no-cache aws-cli\n    - aws configure set aws_access_key_id $AWS_ACCESS_KEY_ID\n    - aws configure set aws_secret_access_key $AWS_SECRET_ACCESS_KEY\n    - aws configure set region $AWS_REGION\n\n# Validate Terraform configuration\nvalidate:\n  stage: validate\n  extends: .terraform_base\n  script:\n    - cd environments/dev\n    - terraform init -backend-config=backend.tf\n    - terraform validate\n    - cd ../staging\n    - terraform init -backend-config=backend.tf\n    - terraform validate\n    - cd ../prod\n    - terraform init -backend-config=backend.tf\n    - terraform validate\n\n# Plan for Dev\nplan_dev:\n  stage: plan\n  extends: .terraform_base\n  script:\n    - cd environments/dev\n    - terraform init -backend-config=backend.tf\n    - terraform plan -var-file=terraform.tfvars -out=tfplan\n    - terraform show -json tfplan &gt; tfplan.json\n    - terraform show tfplan &gt; tfplan.txt\n  artifacts:\n    paths:\n      - environments/dev/tfplan\n      - environments/dev/tfplan.json\n      - environments/dev/tfplan.txt\n    expire_in: 1 week\n  only:\n    - main\n    - merge_requests\n\n# Plan for Staging\nplan_staging:\n  stage: plan\n  extends: .terraform_base\n  script:\n    - cd environments/staging\n    - terraform init -backend-config=backend.tf\n    - terraform plan -var-file=terraform.tfvars -out=tfplan\n    - terraform show -json tfplan &gt; tfplan.json\n    - terraform show tfplan &gt; tfplan.txt\n  artifacts:\n    paths:\n      - environments/staging/tfplan\n      - environments/staging/tfplan.json\n      - environments/staging/tfplan.txt\n    expire_in: 1 week\n  only:\n    - main\n    - merge_requests\n\n# Plan for Prod\nplan_prod:\n  stage: plan\n  extends: .terraform_base\n  script:\n    - cd environments/prod\n    - terraform init -backend-config=backend.tf\n    - terraform plan -var-file=terraform.tfvars -out=tfplan\n    - terraform show -json tfplan &gt; tfplan.json\n    - terraform show tfplan &gt; tfplan.txt\n  artifacts:\n    paths:\n      - environments/prod/tfplan\n      - environments/prod/tfplan.json\n      - environments/prod/tfplan.txt\n    expire_in: 1 week\n  only:\n    - main\n    - merge_requests\n\n# Apply for Dev\napply_dev:\n  stage: apply\n  extends: .terraform_base\n  script:\n    - cd environments/dev\n    - terraform init -backend-config=backend.tf\n    - terraform apply -auto-approve tfplan &gt; apply.log 2&gt;&amp;1\n    - terraform output -json &gt; outputs.json\n    - aws s3 cp apply.log s3://my-audit-logs/terraform-apply/dev/$(date +%Y-%m-%d_%H-%M-%S).log || true\n    - aws s3 cp outputs.json s3://my-audit-logs/terraform-outputs/dev/$(date +%Y-%m-%d_%H-%M-%S).json || true\n  artifacts:\n    paths:\n      - environments/dev/apply.log\n      - environments/dev/outputs.json\n    expire_in: 1 year\n  dependencies:\n    - plan_dev\n  when: manual\n  only:\n    - main\n\n# Apply for Staging\napply_staging:\n  stage: apply\n  extends: .terraform_base\n  script:\n    - cd environments/staging\n    - terraform init -backend-config=backend.tf\n    - terraform apply -auto-approve tfplan &gt; apply.log 2&gt;&amp;1\n    - terraform output -json &gt; outputs.json\n    - aws s3 cp apply.log s3://my-audit-logs/terraform-apply/staging/$(date +%Y-%m-%d_%H-%M-%S).log || true\n    - aws s3 cp outputs.json s3://my-audit-logs/terraform-outputs/staging/$(date +%Y-%m-%d_%H-%M-%S).json || true\n  artifacts:\n    paths:\n      - environments/staging/apply.log\n      - environments/staging/outputs.json\n    expire_in: 1 year\n  dependencies:\n    - plan_staging\n  when: manual\n  only:\n    - main\n\n# Apply for Prod\napply_prod:\n  stage: apply\n  extends: .terraform_base\n  script:\n    - cd environments/prod\n    - terraform init -backend-config=backend.tf\n    - terraform apply -auto-approve tfplan &gt; apply.log 2&gt;&amp;1\n    - terraform output -json &gt; outputs.json\n    - aws s3 cp apply.log s3://my-audit-logs/terraform-apply/prod/$(date +%Y-%m-%d_%H-%M-%S).log || true\n    - aws s3 cp outputs.json s3://my-audit-logs/terraform-outputs/prod/$(date +%Y-%m-%d_%H-%M-%S).json || true\n  artifacts:\n    paths:\n      - environments/prod/apply.log\n      - environments/prod/outputs.json\n    expire_in: 1 year\n  dependencies:\n    - plan_prod\n  when: manual\n  only:\n      - main\n  environment:\n    name: production\n\n# Backup State for Dev\nbackup_state_dev:\nstage:\n backup\nextends:\n .terraform_base\nscript:\n  - cd environments/dev\n  - terraform init - backend-config=backend.tf\n  - aws s3 cp s3://my-terraform-state/data-infra/dev/terraform.tfstate s3://my-terraform-state-backup/data-infra/dev/$(date +%Y-%m-%d_%H-%M-%S).tfstate || true\nwhen:\n always\nonly:\n - main\ndependencies:\n - apply_dev\n\n# Backup State for Staging\nbackup_state:\n stage: backup\n extends: .terraform_base\n script:\n  - cd environments/staging\n  - terraform init -backend-config=backend.tf\n  - aws s3 cp s3://my-terraform-state/data-infra/staging/terraform.tfstate s3://my-terraform-state-backup/data-infra/staging/$(date +%Y-%m-%d_%H-%M-%S).tfstate || true\n when: always\n  only:\n      - main\n      dependencies:\n        - apply_staging\n\n# Backup State for Prod\nbackup_state_prod:\n stage: apply\n extends: .terraform_base\n  script:\n   - cd environments/prod\n   - terraform init -backend-config=backend.tf\n   - aws s3 cp s3://my-terraform-state/data-infra/prod/terraform.tfstate s3://my-terraform-state-backup/data-infra/prod/$(date +%Y-%m-%d_%H-%M-%S).tfstate || true\n  when: always\n  only:\n   - main\n  dependencies:\n   - apply_prod\n</code></pre>"},{"location":"resources/Terraform_Data_Engineer_Cheatsheet/#reviewing-plan-apply-and-output-artifacts","title":"Reviewing Plan, Apply, and Output Artifacts","text":"<p>Plan artifacts (<code>tfplan</code>, <code>tfplan.json</code>, <code>tfplan.txt</code>), apply logs (<code>apply.log</code>), and output files (<code>outputs.json</code>) are generated during the <code>plan</code> and <code>apply</code> stages and stored in GitLab for review, auditing, and automation.</p>"},{"location":"resources/Terraform_Data_Engineer_Cheatsheet/#steps-to-review","title":"Steps to Review","text":"<ol> <li> <p>Access Artifacts: In GitLab, navigate to the pipeline or merge request, go to the <code>plan_dev</code>, <code>plan_staging</code>, <code>plan_prod</code>, <code>apply_dev</code>, <code>apply_staging</code>, or <code>apply_prod</code> job, and download the artifacts.</p> </li> <li> <p>Review Plan Artifacts:</p> <ul> <li>Open <code>tfplan.txt</code> for a human-readable summary of proposed changes.</li> <li>Use <code>tfplan.json</code> for programmatic analysis with tools like <code>jq</code> (e.g., <code>jq '.resource_changes[] | {address: .address, change: .change.actions}' tfplan.json</code>).</li> </ul> </li> <li> <p>Review Apply Logs:</p> <ul> <li>Open <code>apply.log</code> to review the changes applied, including resources created, updated, or deleted.</li> <li>For long-term audit, retrieve logs from S3: <code>aws s3 cp s3://my-audit-logs/terraform-apply/&lt;env&gt;/&lt;timestamp&gt;.log .</code></li> </ul> </li> <li> <p>Review Output Artifacts:</p> <ul> <li>Open <code>outputs.json</code> to inspect dynamically generated values (e.g., S3 bucket ARNs, RDS endpoints).</li> <li>Parse with <code>jq</code> (e.g., <code>jq '.data_lake_arn.value' outputs.json</code>).</li> <li>For audit or integration, retrieve from S3: <code>aws s3 cp s3://my-audit-logs/terraform-outputs/&lt;env&gt;/&lt;timestamp&gt;.json .</code></li> </ul> </li> <li> <p>Team Review:</p> <ul> <li>Share artifact links in merge request comments or integrate with notification tools (e.g., Slack) to alert reviewers.</li> <li>Require approval from team members before triggering <code>apply</code> jobs.</li> </ul> </li> <li> <p>Apply Changes: After approving plan artifacts, trigger the corresponding <code>apply</code> job, which uses the <code>tfplan</code> artifact, generates <code>apply.log</code>, and captures <code>outputs.json</code>.</p> </li> </ol>"},{"location":"resources/Terraform_Data_Engineer_Cheatsheet/#example-workflow_1","title":"Example Workflow","text":"<ul> <li>A developer pushes a change to the <code>main</code> branch, triggering <code>plan_dev</code>.</li> <li>The team downloads <code>environments/dev/tfplan.txt</code> to review changes (e.g., new S3 bucket).</li> <li>They check <code>tfplan.json</code> with <code>jq</code> to verify compliance (e.g., correct instance types).</li> <li>After approval, a team member triggers <code>apply_dev</code>, generating <code>apply.log</code> and <code>outputs.json</code>, which are stored as artifacts and uploaded to S3 for audit.</li> <li>The team reviews <code>outputs.json</code> to confirm resource endpoints (e.g., <code>data_lake_arn</code>).</li> </ul>"},{"location":"resources/Terraform_Data_Engineer_Cheatsheet/#11-best-practices-general","title":"11. Best Practices (General)","text":"<ul> <li>Modularize: Break infrastructure into reusable modules (e.g., S3, RDS, Databricks, Glue).</li> <li>Version Control: Store Terraform code in Git, with separate branches for environments.</li> <li>State Security: Use remote backends with encryption and locking (e.g., S3 + DynamoDB).</li> <li>Least Privilege: Use IAM roles with minimal permissions for Terraform execution.</li> <li>Tagging: Enforce consistent tagging for cost allocation and resource tracking.</li> <li>DRY Principle: Use variables, modules, and <code>for_each</code> to avoid duplication.</li> <li>Testing: Use <code>terraform plan</code> to validate changes; consider tools like <code>terratest</code>.</li> <li>CI/CD Integration: Integrate with GitLab CI/CD for automated deployments.</li> <li> <p>Environment Isolation: Use separate folders or state files for <code>dev</code>, <code>staging</code>, <code>prod</code> instead of workspaces for better clarity and flexibility.</p> </li> <li> <p>RDS-Specific Best Practices:</p> <ul> <li>Use <code>multi_az</code> for production to ensure high availability.</li> <li>Store <code>db_password</code> in AWS Secrets Manager and reference it via <code>data</code> sources.</li> <li>Enable automated backups with appropriate retention periods.</li> <li>Use parameter groups to tune database performance per environment.</li> </ul> </li> <li> <p>Glue-Specific Best Practices:</p> <ul> <li>Store PySpark scripts in S3 and reference them in <code>script_location</code>.</li> <li>Use environment-specific worker types and counts to optimize cost and performance.</li> <li>Enable CloudWatch metrics and logs for job monitoring.</li> <li>Use IAM roles with specific permissions for S3, CloudWatch, and other services.</li> </ul> </li> </ul>"},{"location":"resources/Terraform_Data_Engineer_Cheatsheet/#12-best-practices-for-cicd-pipelines-with-iac","title":"12. Best Practices for CI/CD Pipelines with IaC","text":"<ul> <li>Separate Environments: Use distinct jobs for each environment (<code>dev</code>, <code>staging</code>, <code>prod</code>) to prevent cross-contamination. This project uses separate folders (<code>environments/&lt;env&gt;</code>) instead of Terraform workspaces for clear isolation and environment-specific configurations.</li> <li>Manual Approvals: Require manual triggers for <code>apply</code> jobs, especially for production, to avoid unintended changes.</li> <li>State Management: Use remote backends with locking (e.g., S3 with DynamoDB) to prevent concurrent state modifications.</li> <li>Secrets Management: Store sensitive data (e.g., AWS credentials, DB passwords) in GitLab CI/CD variables or a secrets manager like AWS Secrets Manager.</li> <li>Linting and Validation: Run <code>terraform validate</code> and tools like <code>tflint</code> in the pipeline to catch errors early.</li> <li>Plan Artifacts: Store <code>terraform plan</code> outputs to (<code>tfplan</code>, <code>tfplan.json</code>, <code>tfplan.txt</code>) for review before applying, with JSON for automation and text for human-readable review.</li> <li>Apply Log Retention: Store <code>terraform apply</code> logs (<code>apply.log</code>) as GitLab artifacts (e.g., for 1 year) or upload to S3 (e.g., my-audit-logs) for long-term audit retention, with encryption and access controls to meet compliance requirements.</li> <li> <p>Output Management: Capture <code>terraform output -json</code> as <code>outputs.json</code> artifacts for review, audit, or integration with other systems. Avoid sensitive data in outputs and store in secure storage (e.g., S3) for long-term retention.</p> </li> <li> <p>Data Source Management:</p> <ul> <li>Minimize data source queries to reduce API calls and improve pipeline performance.</li> <li>Use precise filters (e.g., tags, IDs) to avoid ambiguous results.</li> <li>Handle missing resources with <code>count</code> or <code>try</code> to prevent pipeline failures.</li> <li>Validate data source resolution in the <code>plan</code> stage to catch errors early.</li> <li>Document data source dependencies in <code>README.md</code> or comments in <code>.tf</code> files.</li> </ul> </li> <li> <p>Secure Artifacts: Restrict access to artifacts to authorized users to protect sensitive data in plan, apply, log, and output files.</p> </li> <li>Role-Based Access: Use least-privilege IAM roles for CI/CD runners, scoped to specific environments.</li> <li>Testing: Integrate testing tools like <code>terratest</code> or <code>checkov</code> for unit, integration, and compliance testing of Terraform modules and plans.</li> <li>Pipeline Triggers: Run pipelines on merge requests and main branch pushes to catch issues early.</li> <li> <p>Version Pinning: Pin Terraform and provider versions in the pipeline to avoid breaking changes.</p> </li> <li> <p>Rollback Strategy:</p> <ul> <li>State Backups: Regularly back up <code>terraform.tfstate</code> to a separate S3 bucket after <code>apply</code> to enable recovery from failures or unintended changes. Use versioning on the backup bucket for additional protection.</li> <li>Terraform Destroy: Document procedures for <code>terraform destroy</code> to remove all managed resources in an environment, including prerequisites (e.g., emptying S3 buckets) and post-cleanup steps (e.g., removing state files).</li> <li>Manual Rollback: Maintain documentation for manual rollback of specific resources (e.g., reverting RDS instance types, deleting Glue jobs) when <code>destroy</code> is too destructive, including steps to restore from backups or previous state files.</li> <li>Implementation: Automate state backups in the CI/CD pipeline post-<code>apply</code>. Include rollback instructions in <code>README.md</code> or a dedicated <code>ROLLBACK.md</code>.</li> </ul> </li> <li> <p>Monitoring and Logging: Enable verbose logging (<code>TF_LOG=DEBUG</code>) for debugging and monitor pipeline logs in GitLab.</p> </li> <li>Code Reviews: Require peer reviews for merge requests to ensure quality and catch potential issues.</li> <li>Documentation: Include pipeline setup, artifact review, log retention, rollback procedures, output management, and data source usage instructions in <code>README.md</code>.</li> </ul>"},{"location":"resources/Terraform_Data_Engineer_Cheatsheet/#rollback-strategy-details","title":"Rollback Strategy Details","text":""},{"location":"resources/Terraform_Data_Engineer_Cheatsheet/#state-backups","title":"State Backups","text":"<ul> <li>Why: The <code>terraform.tfstate</code> file tracks resource mappings, critical for recovery after failed <code>apply</code> or unintended changes.</li> <li> <p>How:</p> <ul> <li>Configure the pipeline to copy <code>terraform.tfstate</code> to a backup S3 bucket post-<code>apply</code> (see <code>backup_state_*</code> jobs).</li> <li>Enable versioning on the backup bucket (<code>my-terraform-state-backup</code>) to retain historical states.</li> <li>Example: <code>aws s3 cp s3://my-terraform-state/data-infra/dev/terraform.tfstate s3://my-terraform-state-backup/data-infra/dev/$(date +%Y-%m-%d_%H-%M-%S).tfstate</code></li> </ul> </li> <li> <p>Restore Process:</p> <ol> <li>Identify the backup state file (e.g., via S3 console or <code>aws s3 ls</code>).</li> <li>Copy to the active state location: <code>aws s3 cp s3://my-terraform-state-backup/data-infra/dev/&lt;timestamp&gt;.tfstate s3://my-terraform-state/data-infra/dev/terraform.tfstate</code>.</li> <li>Run <code>terraform plan</code> to verify alignment with infrastructure.</li> <li>Apply changes if needed: <code>terraform apply</code>.</li> </ol> </li> <li> <p>Best Practices:</p> <ul> <li>Encrypt backups with S3 server-side encryption.</li> <li>Restrict access via IAM policies.</li> <li>Schedule regular backup jobs or trigger post-<code>apply</code>.</li> <li>Test restoration periodically to ensure reliability.</li> </ul> </li> </ul>"},{"location":"resources/Terraform_Data_Engineer_Cheatsheet/#terraform-destroy","title":"Terraform Destroy","text":"<ul> <li>Why: <code>terraform destroy</code> removes all Terraform-managed resources in an environment, useful for complete rollback.</li> <li> <p>How:</p> <ul> <li>Run <code>terraform destroy</code> in the environment directory (e.g., <code>environments/dev</code>).</li> <li>Example: <code>cd environments/dev; terraform init; terraform destroy -var-file=terraform.tfvars</code>.</li> </ul> </li> <li> <p>Prerequisites:</p> <ul> <li>Empty S3 buckets (delete objects or disable versioning).</li> <li>Remove dependent resources not managed by Terraform (e.g., manually created RDS snapshots).</li> <li>Verify state file integrity before execution.</li> </ul> </li> <li> <p>Post-Cleanup:</p> <ul> <li>Remove state file from S3: <code>aws s3 rm s3://my-terraform-state/data-infra/dev/terraform.tfstate</code>.</li> <li>Update lock table (e.g., DynamoDB) if necessary.</li> </ul> </li> <li> <p>Documentation:</p> <ul> <li>Create a <code>ROLLBACK.md</code> with steps:   <pre><code># Terraform Destroy Rollback\n1. Navigate to the environment: `cd environments/&lt;env&gt;`.\n2. Initialize: `terraform init -backend-config=backend.tf`.\n3. Empty S3 buckets: `aws s3 rm s3://&lt;bucket&gt; --recursive`.\n4. Run destroy: `terraform destroy -var-file=terraform.tfvars`.\n5. Remove state: `aws s3 rm s3://my-terraform-state/data-infra/&lt;env&gt;/terraform.tfstate`.\n</code></pre></li> <li> <p>Risks:</p> </li> <li> <p>Destructive; unsuitable for partial rollbacks.</p> </li> <li>Requires careful validation to avoid data loss.</li> </ul> </li> </ul>"},{"location":"resources/Terraform_Data_Engineer_Cheatsheet/#manual-rollback","title":"Manual Rollback","text":"<ul> <li>Why: For targeted rollbacks when <code>destroy</code> is too aggressive (e.g., reverting an RDS instance type change).</li> <li> <p>How:</p> <ul> <li>Modify Terraform code to revert changes (e.g., set <code>instance_class</code> back to original value).</li> <li>Use a previous state backup to restore resource mappings.</li> <li> <p>Example (RDS rollback):</p> </li> <li> <p>Restore state: <code>aws s3 cp s3://my-terraform-state-backup/data-infra/dev/&lt;timestamp&gt;.tfstate s3://my-terraform-state/data-infra/dev/terraform.tfstate</code>.</p> </li> <li>Update <code>main.tf</code> or variables (e.g., <code>instance_class = \"db.t3.micro\"</code>).</li> <li>Run <code>terraform plan</code> and <code>terraform apply</code>.</li> </ul> </li> <li> <p>Specific Cases:</p> <ul> <li>S3: Delete objects or restore from versioning.</li> <li>RDS: Revert instance type, restore from snapshot, or adjust parameters.</li> <li>Glue: Delete jobs or revert configurations via Terraform.</li> <li>Snowflake: Drop schemas/tables or restore from time travel.</li> </ul> </li> <li> <p>Documentation:</p> </li> <li> <p>In <code>ROLLBACK.md</code>, detail resource-specific rollback steps:     <pre><code>## Manual Rollback\n# RDS Example\n\n1. Restore state: `aws s3 cp s3://my-terraform-state-backup/...`.\n2. Update `instance_class` in `modules/rds/main.tf`.\n3. Run `terraform plan` and `terraform apply`.\n\n## S3 Bucket\n\n1. Delete objects: `aws s3 rm s3://&lt;bucket&gt; --recursive`.\n2. Update Terraform to remove or bucket or adjust policies.\n</code></pre></p> </li> <li> <p>Best Practices:</p> <ul> <li>Test rollbacks in <code>dev</code> before applying to production.</li> <li>Maintain backups (e.g., RDS snapshots) before applying changes.</li> <li>Log all rollback actions for audit purposes.</li> </ul> </li> </ul>"},{"location":"resources/Terraform_Data_Engineer_Cheatsheet/#13-common-gotchas","title":"13. Common Gotchas","text":"<ul> <li>State Drift: Use <code>terraform refresh</code> to sync state with actual infrastructure.</li> <li>Provider Versions: Pin versions to avoid breaking changes.</li> <li>Resource Dependencies: Use <code>depends_on</code> or implicit dependencies (e.g., referencing outputs).</li> <li>Sensitive Data: Store secrets in AWS Secrets Manager or HashiCorp Vault, not <code>.tfvars</code>.</li> <li> <p>Rate Limits: Handle cloud provider API limits with <code>terraform plan</code> retries or delays.</p> </li> <li> <p>RDS-Specific Gotchas:</p> <ul> <li>Changing <code>allocated_storage</code> or <code>instance_class</code> may cause downtime.</li> <li>Ensure <code>subnet_ids</code> are in the same VPC as <code>security_group_ids</code>.</li> <li>Avoid <code>skip_final_snapshot</code> in production to prevent data loss.</li> </ul> </li> <li> <p>Glue-Specific Gotchas:</p> <ul> <li>Ensure the Glue IAM role has permissions for S3 bucket script and temp directories.</li> <li>Verify <code>glue_version</code> compatibility with PySpark script dependencies.</li> <li>Monitor job execution time to avoid unexpected costs in production.</li> </ul> </li> <li> <p>CI/CD-Specific Gotchas:</p> <ul> <li>Ensure CI/CD runners have correct permissions for each environment.</li> <li>Avoid caching sensitive state files or logs in CI/CD pipelines.</li> <li>Test pipeline changes in a non-production branch first.</li> </ul> </li> <li> <p>Output-Specific Gotchas:</p> <ul> <li>Avoid outputting sensitive data (e.g., database passwords) in <code>outputs.tf</code>.</li> <li>Outputs are only updated after <code>terraform apply</code> or <code>terraform refresh</code> if state changes.</li> <li>Missing outputs in <code>outputs.tf</code> can\u2019t be retrieved without redefining them.</li> </ul> </li> <li> <p>Data Source-Specific Gotchas:</p> <ul> <li>Missing resources cause errors unless handled (e.g., with <code>count</code> or <code>try</code>).</li> <li>Ambiguous data sources (e.g., no filters) may return multiple results, causing failures.</li> <li>Excessive API queries from data sources can hit provider rate limits, slowing pipelines.</li> <li>Ensure provider versions support required data source attributes.</li> </ul> </li> </ul>"},{"location":"resources/Terraform_Data_Engineer_Cheatsheet/#14-advanced-features","title":"14. Advanced Features","text":"<ul> <li> <p>Dynamic Blocks: Generate repetitive resource configurations.   <pre><code>resource \"aws_s3_bucket\" \"bucket\" {\n  bucket = \"${var.environment}-${var.bucket_name}\"\n  dynamic \"lifecycle_rule\" {\n    for_each = var.lifecycle_rules\n    content {\n      id      = lifecycle_rule.value.id\n      status  = \"Enabled\"\n      expiration {\n        days = lifecycle_rule.value.days\n      }\n    }\n  }\n}\n</code></pre></p> </li> <li> <p>Count and For_Each: Create multiple resources dynamically.   <pre><code>resource \"aws_s3_bucket\" \"buckets\" {\n  for_each = var.bucket_names\n  bucket   = \"${each.key}-${var.environment}\"\n}\n</code></pre></p> </li> <li> <p>Data Sources:</p> <ul> <li>Definition: Query existing resources or external data without managing them to retrieve attributes (e.g., VPC IDs, AMI IDs, database names).</li> <li> <p>Syntax:   <pre><code>data \"&lt;PROVIDER&gt;_&lt;TYPE&gt;\" \"&lt;NAME&gt;\" {\n  [CONFIG]\n}\n</code></pre>   Example: Fetch a VPC:   <pre><code>data \"aws_vpc\" \"default\" {\n  default = true\n}\n</code></pre></p> </li> <li> <p>Use Cases:</p> <ul> <li>Reference existing infrastructure (e.g., VPCs, IAM roles).</li> <li>Fetch dynamic values (e.g., latest AMI ID, availability zones).</li> <li>Integrate with other Terraform state files (e.g., <code>terraform_remote_state</code>).</li> <li>Query external APIs or scripts (e.g., <code>external</code> provider).</li> </ul> </li> <li> <p>Examples:</p> <ul> <li> <p>AWS: Fetch subnets for an RDS instance:   <pre><code>data \"aws_vpc\" \"default\" {\n  default = true\n}\n\ndata \"aws_subnets\" \"default_subnets\" {\n  filter {\n    name   = \"vpc-id\"\n    values = [data.aws_vpc.default.id]\n  }\n}\n\nresource \"aws_db_instance\" \"rds\" {\n  db_subnet_group_name = aws_db_subnet_group.rds.name\n  vpc_security_group_ids = [aws_security_group.rds.id]\n  subnet_ids = data.aws_subnets.default_subnets.ids\n}\n</code></pre></p> </li> <li> <p>Remote State: Access another project\u2019s outputs:   <pre><code># Fetch outputs from another Terraform configuration\ndata \"terraform_remote_state\" \"network\" {\n  backend = \"s3\"\n  config = {\n    bucket = \"my-terraform-state\"\n    key    = \"networking/terraform.tfstate\"\n    region = \"us-east-1\"\n  }\n}\n# Use the remote state output in a resource\nresource \"aws_s3_bucket\" \"bucket\" {\n  bucket = \"${var.environment}-data-lake\"\n  tags = {\n    VPC = data.terraform_remote_state.network.outputs.vpc_id\n  }\n}\n</code></pre></p> </li> <li>External Data: Fetch data from a script:   <pre><code>data \"external\" \"config\" {\n  program = [\"bash\", \"${path.module}/get_config.sh\"]\n}\n\nresource \"aws_s3_bucket\" \"bucket\" {\n  bucket = \"${var.environment}-${data.external.config.result.bucket_suffix}\"\n}\n</code></pre></li> </ul> </li> <li> <p>Best Practices:</p> <ul> <li>Minimize queries to reduce API calls and improve performance.</li> <li>Use specific filters (e.g., <code>tags</code>, <code>id</code>) to avoid ambiguity.</li> <li>Handle missing resources with <code>count</code> or <code>try</code>:   <pre><code>data \"aws_vpc\" \"existing\" {\n  count = var.vpc_id != \"\" ? 1 : 0\n  id    = var.vpc_id\n}\n</code></pre></li> <li>Avoid sensitive data in data source outputs.</li> <li>Pin provider versions to ensure data source compatibility.</li> <li>Document usage with comments in <code>.tf</code> files.</li> </ul> </li> <li> <p>Gotchas:</p> <ul> <li>Missing resources cause errors unless handled.</li> <li>Ambiguous results (e.g., no filters) cause failures.</li> <li>Rate limits may slow pipelines with many data sources.</li> <li>Ensure state file access for <code>terraform_remote_state</code>.</li> </ul> </li> <li> <p>examples:</p> <ul> <li>Snowflake: <code>data.snowflake_role</code> to fetch an existing role.</li> <li>Databricks: <code>data.databricks_spark_version</code> to get the latest Spark version.</li> <li>Glue Crawler: <code>data.aws_glue_catalog_database</code> to reference an existing database.</li> <li>RDS: <code>data.aws_vpc</code> and <code>data.aws_subnets</code> to fetch VPC and subnets.</li> <li>Glue Job: <code>data.terraform_remote_state</code> to access the S3 bucket from another configuration.</li> </ul> </li> </ul> </li> </ul>"},{"location":"resources/Terraform_Data_Engineer_Cheatsheet/#15-debugging","title":"15. Debugging","text":"<ul> <li>Verbose Logging: <code>export TF_LOG=DEBUG</code></li> <li>State Inspection: <code>terraform state show aws_s3_bucket.bucket</code></li> <li>Plan Analysis: <code>terraform plan -out=tfplan; terraform show -json tfplan</code></li> <li>Output Inspection: <code>terraform output -json</code> or <code>terraform output &lt;output_name&gt;</code></li> <li>Data Source Debugging: Check <code>TF_LOG=DEBUG</code> for data source query details; use <code>terraform plan</code> to verify fetched values.</li> <li>Error Handling: Check provider-specific error codes in logs.</li> </ul>"},{"location":"resources/Terraform_Data_Engineer_Cheatsheet/#16-tools-extensions","title":"16. Tools &amp; Extensions","text":"<ul> <li>Terraform CLI: Core tool for managing infrastructure.</li> <li>tfenv: Manage multiple Terraform versions.</li> <li>tflint: Linter for Terraform code.</li> <li>terraform-docs: Generate documentation from <code>.tf</code> files.</li> <li>VS Code Terraform Extension: Syntax highlighting and autocompletion.</li> </ul>"},{"location":"resources/Terraform_Data_Engineer_Cheatsheet/#notes","title":"Notes","text":"<ul> <li>Provider Requirements: Ensure providers (e.g., <code>aws</code>, <code>snowflake</code>, <code>databricks</code>) are configured in <code>terraform.tf</code> or <code>main.tf</code> for data sources to work. The cheatsheet\u2019s Section 5 already includes AWS and Snowflake providers.</li> <li>IAM Permissions: Data sources require read permissions for the queried resources (e.g., <code>ec2:DescribeVpcs</code> for <code>data.aws_vpc</code>). Ensure the CI/CD runner\u2019s IAM role includes these.</li> <li>Pipeline Integration: Data sources are resolved in the <code>plan</code> and <code>apply</code> jobs without changes to <code>.gitlab-ci.yml</code>. Monitor pipeline logs for data source errors.</li> <li>Testing: Run <code>terraform plan</code> locally in <code>environments/dev</code> to verify data sources fetch expected values. Check <code>tfplan.json</code> for details.</li> <li>Use <code>data</code> blocks to fetch existing resources (e.g., VPCs, Snowflake roles) or dynamic values (e.g., AMI IDs).</li> <li>Define in <code>main.tf</code> (e.g., <code>data.aws_vpc.default</code>) and reference in resources or modules.</li> <li>Document dependencies in <code>main.tf</code> comments.</li> <li>Validate in CI/CD <code>plan</code> jobs to catch errors.</li> <li>Minimize queries, use filters, handle missing resources, validate in <code>plan</code>, and document dependencies.</li> </ul>"},{"location":"resources/Terraform_Data_Engineer_Cheatsheet/#appendix","title":"Appendix","text":""},{"location":"resources/Terraform_Data_Engineer_Cheatsheet/#a-more-detailed-look-at-data-sources","title":"A More Detailed Look at Data Sources","text":"<ul> <li> <p>Definition: Data sources in Terraform allow you to fetch information about existing resources or external data that are not managed by your current Terraform configuration. They provide a way to query cloud providers, APIs, or other systems to retrieve attributes like IDs, ARNs, or configurations without creating or modifying resources.</p> </li> <li> <p>Purpose:</p> <ul> <li>Reference resources created outside Terraform (e.g., an existing VPC or IAM role).</li> <li>Access computed values (e.g., the latest AMI ID, availability zones).</li> <li>Integrate with other Terraform configurations via remote state.</li> <li>Avoid hardcoding values by dynamically retrieving them.</li> </ul> </li> <li> <p>How They Work:</p> <ul> <li>Data sources are defined using the <code>data</code> block in Terraform configuration files (e.g., <code>main.tf</code>).</li> <li>They query the provider\u2019s API (e.g., AWS, Snowflake, Databricks) during <code>terraform plan</code> or <code>terraform apply</code> to fetch data.</li> <li>The retrieved data is stored in the Terraform state file for the duration of the operation but does not manage the underlying resource.</li> <li>Data source attributes can be referenced in other resources, outputs, or modules, just like managed resources.</li> </ul> </li> </ul>"},{"location":"resources/Terraform_Data_Engineer_Cheatsheet/#syntax-and-structure","title":"Syntax and Structure","text":"<p>A data source block follows this format: <pre><code>data \"&lt;PROVIDER&gt;_&lt;TYPE&gt;\" \"&lt;NAME&gt;\" {\n  [CONFIGURATION]\n}\n</code></pre></p> <ul> <li>PROVIDER: The provider (e.g., <code>aws</code>, <code>snowflake</code>, <code>databricks</code>).</li> <li>TYPE: The resource type to query (e.g., <code>aws_vpc</code>, <code>snowflake_database</code>).</li> <li>NAME: A local name for referencing the data source in your configuration.</li> <li>CONFIGURATION: Filters or parameters to identify the resource (e.g., <code>id</code>, <code>name</code>, <code>tags</code>).</li> </ul> <p>Example: <pre><code>data \"aws_vpc\" \"default\" {\n  default = true\n}\n</code></pre></p> <ul> <li>This queries AWS for the default VPC and makes its attributes (e.g., <code>id</code>, <code>cidr_block</code>) available as <code>data.aws_vpc.default.&lt;attribute&gt;</code>.</li> </ul>"},{"location":"resources/Terraform_Data_Engineer_Cheatsheet/#key-characteristics","title":"Key Characteristics","text":"<ul> <li>Read-Only: Data sources only fetch data; they cannot create, update, or delete resources.</li> <li>Provider Dependency: Each data source is tied to a specific provider, which must be configured in your Terraform setup (e.g., <code>provider \"aws\"</code>).</li> <li>State Integration: Data source results are cached in the state file during execution but do not persist as managed resources.</li> <li>Dynamic Values: Data sources are ideal for retrieving values that change over time (e.g., latest AMI IDs, external resource IDs).</li> <li>Lifecycle: Data sources are refreshed during <code>terraform plan</code> or <code>terraform apply</code> unless explicitly skipped (e.g., using <code>-refresh=false</code>).</li> </ul>"},{"location":"resources/Terraform_Data_Engineer_Cheatsheet/#common-use-cases","title":"Common Use Cases","text":"<ol> <li>Referencing Existing Infrastructure:</li> <li>Fetch details of resources created manually or by other Terraform configurations (e.g., VPCs, subnets, IAM roles).</li> <li>Example: Use an existing security group for an RDS instance.</li> <li>Dynamic Resource Configuration:</li> <li>Retrieve dynamic values like the latest AMI ID or availability zones to configure resources.</li> <li>Example: Select the latest Ubuntu AMI for an EC2 instance.</li> <li>Cross-Module Integration:</li> <li>Access outputs from another Terraform configuration using the <code>terraform_remote_state</code> data source.</li> <li>Example: Reference an S3 bucket ARN from a different project\u2019s state.</li> <li>Environment-Specific Configurations:</li> <li>Query environment-specific resources (e.g., a Snowflake database in <code>prod</code>).</li> <li>Example: Fetch a Snowflake database name for a schema.</li> <li>External Data:</li> <li>Use the <code>http</code> or <code>external</code> provider to fetch data from APIs or scripts.</li> <li>Example: Retrieve a configuration value from an external API.</li> </ol>"},{"location":"resources/Terraform_Data_Engineer_Cheatsheet/#example-fetching-an-existing-vpc","title":"Example: Fetching an Existing VPC","text":"<pre><code># Fetch the default VPC\ndata \"aws_vpc\" \"default\" {\n  default = true\n}\n\n# Fetch subnets in the VPC\ndata \"aws_subnets\" \"default_subnets\" {\n  filter {\n    name   = \"vpc-id\"\n    values = [data.aws_vpc.default.id]\n  }\n}\n\n# Use the subnet IDs in an RDS instance\nmodule \"rds_postgres\" {\n  source         = \"../../modules/rds\"\n  db_name        = \"datawarehouse\"\n  environment    = var.environment\n  subnet_ids     = data.aws_subnets.default_subnets.ids\n  security_group_ids = [aws_security_group.rds_sg.id]\n}\n</code></pre> <ul> <li>Here, <code>data.aws_vpc.default</code> retrieves the default VPC\u2019s ID, and <code>data.aws_subnets.default_subnets</code> fetches its subnets, which are then used in the RDS module.</li> </ul>"},{"location":"resources/Terraform_Data_Engineer_Cheatsheet/#example-remote-state-data-source","title":"Example: Remote State Data Source","text":"<p><pre><code># Fetch outputs from another Terraform configuration\ndata \"terraform_remote_state\" \"network\" {\n  backend = \"s3\"\n  config = {\n    bucket = \"my-terraform-state\"\n    key    = \"networking/terraform.tfstate\"\n    region = \"us-east-1\"\n  }\n}\n\n# Use the remote state output in a resource\nresource \"aws_s3_bucket\" \"bucket\" {\n  bucket = \"${var.environment}-data-lake\"\n  tags = {\n    VPC = data.terraform_remote_state.network.outputs.vpc_id\n  }\n}\n</code></pre> - This retrieves the <code>vpc_id</code> output from a separate Terraform configuration\u2019s state file.</p>"},{"location":"resources/Terraform_Data_Engineer_Cheatsheet/#example-external-data-source","title":"Example: External Data Source","text":"<pre><code># Use the external provider to run a script\ndata \"external\" \"config\" {\n  program = [\"bash\", \"${path.module}/get_config.sh\"]\n}\n\n# Use the result in a resource\nresource \"aws_s3_bucket\" \"bucket\" {\n  bucket = \"${var.environment}-${data.external.config.result.bucket_suffix}\"\n}\n</code></pre> <ul> <li>The <code>get_config.sh</code> script returns JSON (e.g., <code>{\"bucket_suffix\": \"data-lake\"}</code>), which is used to name the bucket.</li> </ul>"},{"location":"resources/Terraform_Data_Engineer_Cheatsheet/#integration-with-cicd-setup","title":"Integration with CI/CD  Setup","text":"<ul> <li>Folder-Based Structure: The project uses separate directories (<code>environments/dev</code>, <code>environments/staging</code>, <code>environments/prod</code>) for environment isolation. Data sources can be defined in each environment\u2019s <code>main.tf</code> to fetch environment-specific resources (e.g., a <code>prod</code> VPC).</li> <li>CI/CD Pipeline: Data sources are queried during <code>terraform plan</code> and <code>terraform apply</code> jobs in the GitLab pipeline, ensuring dynamic values are resolved automatically.</li> <li>Modules: Data sources can be used in modules (e.g., <code>modules/rds</code>) to reference existing infrastructure, as shown in the RDS example above.</li> <li>State Management: Data source results are stored in the environment-specific state file (e.g., <code>data-infra/dev/terraform.tfstate</code>) during execution.</li> </ul>"},{"location":"resources/Terraform_Data_Engineer_Cheatsheet/#best-practices","title":"Best Practices","text":"<ol> <li>Minimize Queries: Avoid excessive data source queries to reduce API calls and improve performance.</li> <li>Use Filters: Specify precise filters (e.g., <code>tags</code>, <code>id</code>) to avoid ambiguous results.</li> <li>Handle Missing Resources:</li> <li>Use <code>count</code> or <code>try</code> to handle cases where a data source might not return results.</li> <li>Example:      <pre><code>data \"aws_vpc\" \"existing\" {\n  count = var.vpc_id != \"\" ? 1 : 0\n  id    = var.vpc_id\n}\n</code></pre></li> <li>Avoid Sensitive Data: Don\u2019t expose sensitive attributes (e.g., secrets) in data source outputs.</li> <li>Pin Provider Versions: Ensure the provider version supports the data source type and attributes.</li> <li>Document Usage: Include comments in <code>main.tf</code> explaining why a data source is used.</li> <li>Test in CI/CD: Verify data sources resolve correctly in the pipeline\u2019s <code>plan</code> stage.</li> </ol>"},{"location":"resources/Terraform_Data_Engineer_Cheatsheet/#common-gotchas","title":"Common Gotchas","text":"<ul> <li>Missing Resources: If a data source can\u2019t find a resource, Terraform will error unless handled (e.g., with <code>count</code> or <code>try</code>).</li> <li>Stale Data: Data sources rely on provider APIs; ensure the provider is up-to-date to avoid stale results.</li> <li>Rate Limits: Excessive data source queries may hit provider API limits, causing failures in <code>plan</code> or <code>apply</code>.</li> <li>State Dependency: Data sources are resolved during execution, so ensure the state file is accessible (e.g., in remote backends).</li> <li>Ambiguous Results: Without proper filters, data sources may return multiple results, causing errors.</li> </ul>"},{"location":"resources/Terraform_Data_Engineer_Cheatsheet/#a-deeper-look-into-environment-isolation-approaches","title":"A deeper look into Environment Isolation Approaches","text":""},{"location":"resources/Terraform_Data_Engineer_Cheatsheet/#folder-based-isolation","title":"Folder-Based Isolation","text":"<p>Folder-based isolation involves organizing Terraform configurations into separate directories for each environment (e.g., <code>environments/dev/</code>, <code>environments/staging/</code>, <code>environments/prod/</code>). Each directory contains its own <code>main.tf</code>, <code>variables.tf</code>, <code>outputs.tf</code>, <code>backend.tf</code>, and <code>terraform.tfvars</code>, with distinct state files and configurations tailored to the environment.</p>"},{"location":"resources/Terraform_Data_Engineer_Cheatsheet/#terraform-workspaces","title":"Terraform Workspaces","text":"<p>Terraform workspaces allow multiple environments to share the same configuration files within a single directory, using different state files for each workspace (e.g., <code>default</code>, <code>dev</code>, <code>staging</code>, <code>prod</code>). Workspaces are managed with commands like <code>terraform workspace new &lt;env&gt;</code> and <code>terraform workspace select &lt;env&gt;</code>, and variables are typically controlled via CLI flags or conditional logic.</p>"},{"location":"resources/Terraform_Data_Engineer_Cheatsheet/#advantages-of-folder-based-isolation","title":"Advantages of Folder-Based Isolation","text":"<p>The folder-based isolation approach, as demonstrated in the cheatsheet's <code>environments/</code> structure, offers several advantages over workspaces, particularly for data infrastructure projects involving complex resources like S3 buckets, RDS databases, Snowflake, Databricks, and Glue jobs.</p>"},{"location":"resources/Terraform_Data_Engineer_Cheatsheet/#1-clear-separation-of-configurations","title":"1. Clear Separation of Configurations","text":"<ul> <li>Advantage: Each environment has its own dedicated directory with independent configuration files, making it easier to customize resources, variables, and outputs without relying on conditional logic.</li> <li>Example: In the cheatsheet, <code>environments/dev/main.tf</code> might define a smaller RDS instance (<code>db.t3.micro</code>) compared to <code>environments/prod/main.tf</code> (<code>db.m5.large</code>). This is explicit and avoids complex <code>if</code> statements or workspace-specific variables.</li> <li>Workspace Challenge: With workspaces, all environments share the same <code>main.tf</code>, requiring conditional logic (e.g., <code>count = terraform.workspace == \"prod\" ? 1 : 0</code>) or variable overrides, which can lead to errors or reduced readability.</li> <li>Impact: Folder-based isolation improves maintainability and reduces the risk of misconfiguration, especially for data engineers managing diverse infrastructure across environments.</li> </ul>"},{"location":"resources/Terraform_Data_Engineer_Cheatsheet/#2-independent-state-files-and-backends","title":"2. Independent State Files and Backends","text":"<ul> <li>Advantage: Each environment has its own state file and backend configuration, stored in separate paths (e.g., <code>s3://my-terraform-state/data-infra/dev/terraform.tfstate</code> vs. <code>s3://my-terraform-state/data-infra/prod/terraform.tfstate</code>), ensuring complete isolation.</li> <li>Example: The cheatsheet's <code>environments/dev/backend.tf</code> specifies:   <pre><code>terraform {\n  backend \"s3\" {\n    bucket         = \"my-terraform-state\"\n    key            = \"data-infra/dev/terraform.tfstate\"\n    region         = \"us-east-1\"\n    dynamodb_table = \"terraform-locks\"\n  }\n}\n</code></pre>   This isolates <code>dev</code> state from <code>prod</code>, preventing accidental cross-environment changes.</li> <li>Workspace Challenge: Workspaces store state files in the same backend with a workspace prefix (e.g., <code>env:/dev/terraform.tfstate</code>), which can lead to accidental state overwrites if the wrong workspace is selected. Additionally, all workspaces share the same backend configuration, limiting flexibility.</li> <li>Impact: Folder-based isolation enhances security and auditability by ensuring state files are distinctly managed, critical for compliance in data infrastructure projects.</li> </ul>"},{"location":"resources/Terraform_Data_Engineer_Cheatsheet/#3-simplified-cicd-pipelines","title":"3. Simplified CI/CD Pipelines","text":"<ul> <li>Advantage: Folder-based isolation aligns naturally with CI/CD pipelines, as each environment's directory can be targeted independently, reducing complexity in pipeline scripts.</li> <li>Example: The cheatsheet's <code>.gitlab-ci.yml</code> defines separate jobs (<code>plan_dev</code>, <code>apply_dev</code>, <code>plan_staging</code>, etc.) that operate on specific directories (e.g., <code>cd environments/dev</code>). This avoids the need to switch workspaces in the pipeline, simplifying configuration and reducing errors.   <pre><code>plan_dev:\n  stage: plan\n  script:\n    - cd environments/dev\n    - terraform init -backend-config=backend.tf\n    - terraform plan -var-file=terraform.tfvars -out=tfplan\n</code></pre></li> <li>Workspace Challenge: With workspaces, pipelines must run <code>terraform workspace select &lt;env&gt;</code> before each job, increasing the risk of selecting the wrong workspace or encountering state conflicts. This adds complexity to pipeline scripts and requires careful error handling.</li> <li>Impact: Folder-based isolation streamlines GitLab CI/CD pipelines, improving reliability and auditability for automated deployments of data infrastructure.</li> </ul>"},{"location":"resources/Terraform_Data_Engineer_Cheatsheet/#4-enhanced-team-collaboration","title":"4. Enhanced Team Collaboration","text":"<ul> <li>Advantage: Separate directories make it easier for teams to work on different environments simultaneously without conflicts, as each environment's configuration and state are isolated.</li> <li>Example: In the cheatsheet, a developer can modify <code>environments/dev/main.tf</code> to test a new Glue job while another team member updates <code>environments/prod/main.tf</code> to adjust RDS settings, without risking state or configuration clashes.</li> <li>Workspace Challenge: Workspaces share a single directory, so concurrent changes to <code>main.tf</code> or state files can lead to conflicts, especially in large teams. Developers must carefully coordinate workspace selection to avoid overwriting each other's changes.</li> <li>Impact: Folder-based isolation supports parallel development, critical for data engineering teams managing complex, multi-environment setups.</li> </ul>"},{"location":"resources/Terraform_Data_Engineer_Cheatsheet/#5-better-auditability-and-traceability","title":"5. Better Auditability and Traceability","text":"<ul> <li>Advantage: Folder-based isolation provides clear, environment-specific configuration files and state files, making it easier to audit changes and track infrastructure history.</li> <li>Example: The cheatsheet's structure allows auditors to review <code>environments/prod/terraform.tfvars</code> and <code>apply.log</code> (stored as artifacts) to verify production settings and changes. State backups in <code>s3://my-terraform-state/backups/prod/</code> provide a clear history for rollback or compliance.</li> <li>Workspace Challenge: Workspaces mix configurations in a single <code>main.tf</code>, requiring auditors to parse conditional logic or variable overrides to understand environment-specific settings. State files are less distinctly separated, complicating audit trails.</li> <li>Impact: Folder-based isolation simplifies compliance with regulatory requirements (e.g., GDPR, HIPAA) by providing transparent, environment-specific records.</li> </ul>"},{"location":"resources/Terraform_Data_Engineer_Cheatsheet/#6-flexibility-for-environment-specific-customization","title":"6. Flexibility for Environment-Specific Customization","text":"<ul> <li>Advantage: Each environment can have unique configurations, providers, or even Terraform versions without affecting others, offering maximum flexibility.</li> <li>Example: In the cheatsheet, <code>environments/staging/</code> might use a different AWS region or provider version than <code>environments/prod/</code>, defined in their respective <code>main.tf</code> and <code>backend.tf</code>. This is straightforward with separate directories.</li> <li>Workspace Challenge: Workspaces share the same provider configuration and Terraform version, limiting customization unless complex workarounds (e.g., provider aliases) are used.</li> <li>Impact: Folder-based isolation supports diverse data infrastructure requirements, such as regional differences or provider-specific settings for Snowflake or Databricks.</li> </ul>"},{"location":"resources/Terraform_Data_Engineer_Cheatsheet/#7-reduced-risk-of-human-error","title":"7. Reduced Risk of Human Error","text":"<ul> <li>Advantage: Operating within a specific directory (e.g., <code>environments/dev/</code>) eliminates the need to select a workspace, reducing the chance of applying changes to the wrong environment.</li> <li>Example: Running <code>terraform apply</code> in <code>environments/dev/</code> affects only the <code>dev</code> environment, with no risk of accidentally targeting <code>prod</code>. The cheatsheet's pipeline reinforces this by scoping jobs to directories.</li> <li>Workspace Challenge: Forgetting to run <code>terraform workspace select prod</code> before <code>terraform apply</code> can result in catastrophic changes to the wrong environment, a common error in high-pressure scenarios.</li> <li>Impact: Folder-based isolation enhances safety, particularly for production data infrastructure where errors can lead to data loss or downtime.</li> </ul>"},{"location":"resources/Terraform_Data_Engineer_Cheatsheet/#when-to-use-workspaces","title":"When to Use Workspaces","text":"<p>While folder-based isolation is generally preferred, workspaces may be suitable for:</p> <ul> <li>Simple Projects: Small projects with minimal configuration differences between environments (e.g., a single S3 bucket with different names).</li> <li>Temporary Environments: Spinning up ephemeral environments (e.g., feature branches) where maintaining separate directories is overkill.</li> <li>Legacy Projects: Existing setups already using workspaces, where migration to folders is not cost-effective.</li> </ul> <p>However, for data infrastructure projects with complex resources, multiple team members, and strict compliance requirements, folder-based isolation (as in the cheatsheet) is superior.</p>"},{"location":"resources/Terraform_Data_Engineer_Cheatsheet/#recommendations","title":"Recommendations","text":"<ul> <li>Adopt Folder-Based Isolation: Use the cheatsheet's <code>environments/</code> structure to organize <code>dev</code>, <code>staging</code>, and <code>prod</code> configurations in separate directories.</li> <li>Document in <code>README.md</code>: Include setup instructions for navigating environment directories and running Terraform commands, as shown in the cheatsheet:   <pre><code>## Running Terraform\n- Navigate to the environment directory: `cd environments/dev`\n- Initialize: `terraform init`\n- Apply: `terraform apply -var-file=terraform.tfvars`\n</code></pre></li> <li>Leverage CI/CD: Configure pipelines (e.g., <code>.gitlab-ci.yml</code>) to target specific directories, as in the cheatsheet, to enforce environment isolation.</li> <li>Secure State Files: Use distinct backend paths and versioning (e.g., <code>s3://my-terraform-state/data-infra/&lt;env&gt;/terraform.tfstate</code>) to protect state files, as shown in the cheatsheet's <code>backend.tf</code>.</li> </ul>"},{"location":"resources/Terraform_Scenarios/","title":"Terraform Scenarios","text":"<p>AI Assisted (Grok 3)</p> <p>Common Scenarios and how to handle them.</p>"},{"location":"resources/Terraform_Scenarios/#project-structure-reference","title":"Project Structure Reference","text":"<pre><code>data-infra/\n\u251c\u2500\u2500 modules/                     # Reusable Terraform modules (e.g., s3_bucket, rds, glue_job)\n\u251c\u2500\u2500 environments/                # Environment-specific configurations\n\u2502   \u251c\u2500\u2500 dev/\n\u2502   \u2502   \u251c\u2500\u2500 main.tf\n\u2502   \u2502   \u251c\u2500\u2500 variables.tf\n\u2502   \u2502   \u251c\u2500\u2500 outputs.tf\n\u2502   \u2502   \u251c\u2500\u2500 backend.tf\n\u2502   \u2502   \u2514\u2500\u2500 terraform.tfvars\n\u2502   \u251c\u2500\u2500 staging/\n\u2502   \u2514\u2500\u2500 prod/\n\u251c\u2500\u2500 scripts/\n\u251c\u2500\u2500 .gitignore\n\u251c\u2500\u2500 README.md\n\u2514\u2500\u2500 .gitlab-ci.yml               # CI/CD pipeline with validate, plan, and apply stages\n</code></pre>"},{"location":"resources/Terraform_Scenarios/#1-what-happens-if-your-state-file-is-accidentally-deleted","title":"1. What happens if your state file is accidentally deleted?","text":"<p>Answer: If the Terraform state file (<code>terraform.tfstate</code>) is deleted, Terraform loses track of managed infrastructure. The next <code>terraform apply</code> assumes resources don\u2019t exist and attempts to recreate them, potentially causing duplicates or failures. Recovery involves restoring a backup or manually importing resources with <code>terraform import</code>. Always enable versioning on remote state storage (e.g., S3).</p> <p>Example:</p> <ul> <li>Project Context: The project uses an S3 backend for state storage (<code>s3://my-terraform-state/data-infra/dev/terraform.tfstate</code>) with versioning enabled.</li> <li>Scenario: The <code>dev</code> state file is deleted.</li> <li>Recovery:</li> </ul> <p><pre><code># Restore from S3 versioned backup\naws s3api get-object --bucket my-terraform-state --key data-infra/dev/terraform.tfstate --version-id &lt;version-id&gt; terraform.tfstate\ncd environments/dev\nterraform init\nterraform apply\n</code></pre>   Alternatively, import an existing S3 bucket:   <pre><code>terraform import module.data_lake.aws_s3_bucket.bucket dev-data-lake\n</code></pre> - Best Practice: Enable S3 bucket versioning and backup state files before <code>apply</code> in the pipeline (see <code>apply_dev</code> job in <code>.gitlab-ci.yml</code>)</p>"},{"location":"resources/Terraform_Scenarios/#2-how-do-you-handle-large-scale-refactoring-without-downtime","title":"2. How do you handle large-scale refactoring without downtime?","text":"<p>Answer: For most resources, use <code>terraform state mv</code> to rename them in the state file without destruction. For S3 buckets, which have immutable names in AWS, create a new bucket, copy data, and update the state. Split refactoring into smaller, non-destructive pull requests (PRs), use targeted applies (<code>terraform apply -target</code>), and verify plans to prevent resource destruction. Test in <code>dev</code> before <code>staging</code> or <code>prod</code>.</p> <p>Example:</p> <ul> <li>Project Context: Refactor the <code>s3_bucket</code> module to rename the bucket from <code>dev-data-lake</code> to <code>dev-data-lake-v2</code> in <code>environments/dev/main.tf</code>. Since S3 bucket names are immutable, a new bucket is created, and data is copied.</li> <li> <p>Step-by-Step Process:</p> </li> <li> <p>Add New Bucket Module:</p> <ul> <li>Update <code>environments/dev/main.tf</code> to define a temporary module (<code>data_lake_v2</code>) alongside <code>data_lake</code>.    <pre><code># environments/dev/main.tf\nmodule \"data_lake\" {\n  source      = \"../../modules/s3_bucket\"\n  bucket_name = \"data-lake\"  # Original: dev-data-lake\n  environment = var.environment\n}\n\nmodule \"data_lake_v2\" {\n  source      = \"../../modules/s3_bucket\"\n  bucket_name = \"data-lake-v2\"  # New: dev-data-lake-v2\n  environment = var.environment\n}\n</code></pre></li> <li>Run:    <pre><code>cd environments/dev\nterraform init\nterraform apply -var-file=terraform.tfvars\n</code></pre></li> <li>Outcome: Creates <code>dev-data-lake-v2</code>. <code>dev-data-lake</code> remains active, ensuring no downtime.</li> </ul> </li> <li> <p>Copy Data:</p> <ul> <li>Sync data from <code>dev-data-lake</code> to <code>dev-data-lake-v2</code>.    <pre><code>aws s3 sync s3://dev-data-lake s3://dev-data-lake-v2\n</code></pre></li> <li>Outcome: <code>dev-data-lake-v2</code> contains all data. Services using <code>dev-data-lake</code> are unaffected.</li> </ul> </li> <li> <p>Update State:</p> <ul> <li>Move the state entry for <code>dev-data-lake-v2</code> to replace <code>dev-data-lake</code>.    <pre><code>terraform state mv module.data_lake.aws_s3_bucket.bucket module.data_lake_v2.aws_s3_bucket.bucket\n</code></pre></li> <li>Outcome: State now maps <code>module.data_lake_v2.aws_s3_bucket.bucket</code> to <code>dev-data-lake-v2</code>.</li> </ul> </li> <li> <p>Remove Old Bucket from State:</p> <ul> <li>Remove <code>dev-data-lake</code> from state.    <pre><code>terraform state rm module.data_lake.aws_s3_bucket.bucket\n</code></pre></li> <li>Outcome: Terraform no longer manages <code>dev-data-lake</code>, which remains in AWS.</li> </ul> </li> <li> <p>Update Configuration:</p> <ul> <li>Revise <code>main.tf</code> to use only the new bucket.    <pre><code># environments/dev/main.tf\nmodule \"data_lake\" {\n  source      = \"../../modules/s3_bucket\"\n  bucket_name = \"data-lake-v2\"\n  environment = var.environment\n}\n</code></pre></li> <li>Run:    <pre><code>terraform plan -var-file=terraform.tfvars  # Verify no destroy\nterraform apply -var-file=terraform.tfvars\n</code></pre></li> <li>Outcome: Terraform manages <code>dev-data-lake-v2</code> under <code>module.data_lake</code>.</li> </ul> </li> <li> <p>Update Dependencies:</p> <ul> <li>Modify dependent resources (e.g., Glue jobs) to reference <code>dev-data-lake-v2</code>.    <pre><code># modules/glue_job/main.tf\ndata \"aws_s3_bucket\" \"bucket\" {\n  bucket = \"${var.environment}-data-lake-v2\"\n}\n</code></pre></li> <li>Apply changes in a separate PR.</li> <li>Outcome: Services transition to <code>dev-data-lake-v2</code> without disruption.</li> </ul> </li> <li> <p>Delete Old Bucket (Optional):</p> <ul> <li>If safe, delete <code>dev-data-lake</code>.    <pre><code>aws s3 rb s3://dev-data-lake --force\n</code></pre></li> <li>Ensure <code>force_destroy = false</code> in <code>modules/s3_bucket/main.tf</code> to prevent accidental deletion:    <pre><code>resource \"aws_s3_bucket\" \"bucket\" {\n  bucket        = \"${var.environment}-${var.bucket_name}\"\n  force_destroy = false\n}\n</code></pre></li> <li>Outcome: Old bucket is removed after confirmation.</li> </ul> </li> <li> <p>Best Practice: Test in <code>dev</code> using the <code>plan_dev</code> job, automate data sync in <code>apply_dev</code>, document in PRs, and ensure <code>force_destroy = false</code>. Update <code>README.md</code>:</p> </li> </ul> <pre><code>## S3 Bucket Renaming\n- Add new module in `main.tf`.\n- Sync data: `aws s3 sync s3://dev-data-lake s3://dev-data-lake-v2`.\n- Update state: `terraform state mv`.\n- Remove old module and state entry.\n- Delete old bucket if safe.\n</code></pre>"},{"location":"resources/Terraform_Scenarios/#3-what-happens-if-a-resource-fails-halfway-through-a-terraform-apply","title":"3. What happens if a resource fails halfway through a terraform apply?","text":"<p>Answer: If a resource fails during <code>terraform apply</code>, Terraform creates a partial deployment. Successful resources are applied, but failed ones are marked as tainted in the state file. Use targeted applies (<code>terraform apply -target</code>) or <code>-refresh-only</code> to recover systematically, addressing failures one by one.</p> <p>Example:</p> <ul> <li>Project Context: The <code>apply_dev</code> job fails when creating an RDS instance due to an invalid parameter.</li> <li>Scenario: The <code>module.rds_postgres.aws_db_instance.rds_instance</code> fails, but the <code>module.data_lake.aws_s3_bucket.bucket</code> is created.</li> <li>Recovery:</li> </ul> <p><pre><code>cd environments/dev\nterraform init\nterraform plan -var-file=terraform.tfvars  # Check tainted resources\nterraform apply -target=module.rds_postgres.aws_db_instance.rds_instance  # Retry specific resource\n</code></pre>   Review <code>apply.log</code> artifact from the <code>apply_dev</code> job in GitLab to diagnose the error.</p> <ul> <li>Best Practice: Use the pipeline\u2019s <code>apply.log</code> (stored in S3: <code>s3://my-audit-logs/terraform-apply/dev/</code>) for debugging and target specific resources to minimize disruption.</li> </ul>"},{"location":"resources/Terraform_Scenarios/#4-how-do-you-manage-secrets-in-terraform","title":"4. How do you manage secrets in Terraform?","text":"<p>Answer: Store secrets in external systems like AWS Secrets Manager or HashiCorp Vault, use encrypted remote state, mark outputs as sensitive, and integrate with CI/CD securely. Avoid hardcoding secrets in <code>.tfvars</code> or code, and consider managing highly sensitive values outside Terraform.</p> <p>Example:</p> <ul> <li>Project Context: The RDS module in <code>environments/dev/main.tf</code> requires a <code>db_password</code>.</li> <li>Implementation:</li> </ul> <p><pre><code># environments/dev/main.tf\ndata \"aws_secretsmanager_secret_version\" \"db_password\" {\n  secret_id = \"dev/rds/password\"\n}\n\nmodule \"rds_postgres\" {\n  source             = \"../../modules/rds\"\n  db_name            = \"datawarehouse\"\n  environment        = var.environment\n  db_username        = var.db_username\n  db_password        = data.aws_secretsmanager_secret_version.db_password.secret_string\n  subnet_ids         = data.aws_subnet_ids.default.ids\n  security_group_ids = [aws_security_group.rds_sg.id]\n}\n\n# outputs.tf\noutput \"rds_endpoint\" {\n  value     = module.rds_postgres.aws_db_instance.rds_instance.endpoint\n  sensitive = true\n}\n</code></pre>   Store <code>AWS_ACCESS_KEY_ID</code> and <code>AWS_SECRET_ACCESS_KEY</code> in GitLab CI/CD variables for pipeline authentication.</p> <ul> <li>Best Practice: Use the <code>backend.tf</code> S3 backend with encryption and restrict access via IAM policies. Avoid storing secrets in <code>terraform.tfvars</code>.</li> </ul>"},{"location":"resources/Terraform_Scenarios/#5-what-happens-if-terraform-plan-shows-no-changes-but-infrastructure-was-modified-outside-terraform","title":"5. What happens if terraform plan shows no changes but infrastructure was modified outside Terraform?","text":"<p>Answer: Terraform is unaware of external changes (state drift) until <code>terraform refresh</code> updates the state file. Implement regular drift detection in CI/CD pipelines to catch unauthorized modifications and reconcile them with <code>terraform apply</code> or <code>terraform import</code>.</p> <p>Example:</p> <ul> <li>Project Context: An S3 bucket (<code>dev-data-lake</code>) is manually modified in the AWS Console to change its ACL.</li> <li>Detection:</li> </ul> <p><pre><code># .gitlab-ci.yml\ndrift_detection:\n  stage: validate\n  extends: .terraform_base\n  script:\n    - cd environments/dev\n    - terraform init -backend-config=backend.tf\n    - terraform refresh -var-file=terraform.tfvars\n    - terraform plan -var-file=terraform.tfvars -out=tfplan\n  artifacts:\n    paths:\n      - environments/dev/tfplan.txt\n    expire_in: 1 week\n</code></pre> <pre><code>cd environments/dev\nterraform init\nterraform refresh -var-file=terraform.tfvars\nterraform plan -var-file=terraform.tfvars  # Shows drift\nterraform apply -var-file=terraform.tfvars  # Reconcile changes\n</code></pre></p> <ul> <li>Best Practice: Schedule a <code>drift_detection</code> job weekly in <code>.gitlab-ci.yml</code> and review <code>tfplan.txt</code> artifacts to identify drift.</li> </ul>"},{"location":"resources/Terraform_Scenarios/#6-what-happens-if-you-delete-a-resource-definition-from-your-configuration","title":"6. What happens if you delete a resource definition from your configuration?","text":"<p>Answer: Removing a resource from Terraform configuration causes <code>terraform apply</code> to destroy the corresponding infrastructure. Use <code>terraform state rm</code> to remove the resource from state without destroying it, or add <code>lifecycle { prevent_destroy = true }</code> for critical resources.</p> <p>Example:</p> <ul> <li>Project Context: The <code>module.data_lake.aws_s3_bucket.bucket</code> is removed from <code>environments/prod/main.tf</code>.</li> <li>Prevention:</li> </ul> <p><pre><code># modules/s3_bucket/main.tf\nresource \"aws_s3_bucket\" \"bucket\" {\n  bucket = \"${var.environment}-${var.bucket_name}\"\n  lifecycle {\n    prevent_destroy = true\n  }\n}\n</code></pre>   If removed accidentally:   <pre><code>cd environments/prod\nterraform state rm module.data_lake.aws_s3_bucket.bucket\nterraform plan -var-file=terraform.tfvars  # Verify no destroy\n</code></pre></p> <ul> <li>Best Practice: Apply <code>prevent_destroy</code> to critical resources like production S3 buckets or RDS instances in the <code>modules/</code> directory.</li> </ul>"},{"location":"resources/Terraform_Scenarios/#7-what-happens-if-terraform-provider-apis-change-between-versions","title":"7. What happens if Terraform provider APIs change between versions?","text":"<p>Answer: Provider API changes can break compatibility, causing errors in resource creation or updates. Read release notes, pin provider versions, test upgrades in <code>dev</code>, and use targeted applies for gradual migration.</p> <p>Example:</p> <ul> <li>Project Context: Upgrading the AWS provider from <code>~&gt; 4.0</code> to <code>~&gt; 5.0</code> in <code>environments/dev/main.tf</code>.</li> <li>Implementation:</li> </ul> <p><pre><code># environments/dev/main.tf\nterraform {\n  required_providers {\n    aws = {\n      source  = \"hashicorp/aws\"\n      version = \"~&gt; 5.0\"  # Upgraded from 4.0\n    }\n  }\n}\n</code></pre> <pre><code>cd environments/dev\nterraform init -upgrade\nterraform plan -var-file=terraform.tfvars  # Check for breaking changes\nterraform apply -var-file=terraform.tfvars\n</code></pre></p> <ul> <li>Best Practice: Test upgrades in the <code>plan_dev</code> job, review release notes (e.g., AWS provider changelog), and update one environment at a time.</li> </ul>"},{"location":"resources/Terraform_Scenarios/#8-how-do-you-implement-zero-downtime-infrastructure-updates","title":"8. How do you implement zero-downtime infrastructure updates?","text":"<p>Answer: Use <code>create_before_destroy</code> lifecycle blocks, blue-green deployments, health checks, and state manipulation. For databases, leverage replicas or managed services with failover capabilities to avoid downtime.</p> <p>Example:</p> <ul> <li>Project Context: Update the RDS instance class in <code>environments/prod/main.tf</code> without downtime.</li> <li>Implementation:</li> </ul> <p><pre><code># modules/rds/main.tf\nresource \"aws_db_instance\" \"rds_instance\" {\n  identifier           = \"${var.environment}-${var.db_name}\"\n  instance_class       = var.prod_instance_class\n  allocated_storage    = var.allocated_storage\n  multi_az             = true  # Enable for failover\n  apply_immediately    = false # Apply during maintenance window\n  lifecycle {\n    create_before_destroy = true\n  }\n}\n</code></pre> <pre><code>cd environments/prod\nterraform init\nterraform plan -var-file=terraform.tfvars -out=tfplan\nterraform apply tfplan\n</code></pre></p> <ul> <li>Best Practice: Enable <code>multi_az</code> for production RDS (as in the cheatsheet), use <code>apply_immediately = false</code>, and test in <code>staging</code> via the <code>apply_staging</code> job.</li> </ul>"},{"location":"resources/Terraform_Scenarios/#9-what-happens-if-you-have-circular-dependencies-in-your-terraform-modules","title":"9. What happens if you have circular dependencies in your Terraform modules?","text":"<p>Answer: Circular dependencies cause Terraform to fail with \"dependency cycle\" errors. Refactor modules using data sources, outputs, or restructured resources to establish a clear dependency hierarchy.</p> <p>Example:</p> <ul> <li>Project Context: The <code>s3_bucket</code> module depends on a <code>glue_job</code> module, which references the S3 bucket\u2019s ARN.</li> <li>Resolution:</li> </ul> <pre><code># modules/s3_bucket/main.tf\nresource \"aws_s3_bucket\" \"bucket\" {\n  bucket = \"${var.environment}-${var.bucket_name}\"\n}\n\noutput \"bucket_arn\" {\n  value = aws_s3_bucket.bucket.arn\n}\n\n# modules/glue_job/main.tf\ndata \"aws_s3_bucket\" \"bucket\" {\n  bucket = \"${var.environment}-${var.bucket_name}\"\n}\n\nresource \"aws_glue_job\" \"pyspark_job\" {\n  name     = \"${var.environment}-${var.job_name}\"\n  role_arn = var.glue_role_arn\n  command {\n    script_location = \"s3://${data.aws_s3_bucket.bucket.bucket}/${var.script_path}\"\n  }\n}\n</code></pre> <ul> <li>Best Practice: Use data sources to fetch existing resources, avoiding direct dependencies. Validate with the <code>validate</code> job in <code>.gitlab-ci.yml</code>.</li> </ul>"},{"location":"resources/Terraform_Scenarios/#10-what-happens-if-you-rename-a-resource-in-your-terraform-code","title":"10. What happens if you rename a resource in your Terraform code?","text":"<p>Answer: Renaming a resource in Terraform code is interpreted as destroying and recreating the resource. Use <code>terraform state mv</code> to update the state file, preserving the existing infrastructure and avoiding rebuilds or downtime. For S3 buckets, create a new bucket and copy data, as bucket names are immutable.</p> <p>Example:</p> <ul> <li>Project Context: Rename <code>module.data_lake.aws_s3_bucket.bucket</code> to <code>module.data_lake.aws_s3_bucket.data_lake</code> in <code>environments/dev/main.tf</code>.</li> <li>Steps:</li> </ul> <p><pre><code># Original: environments/dev/main.tf\nmodule \"data_lake\" {\n  source      = \"../../modules/s3_bucket\"\n  bucket_name = \"data-lake\"\n  environment = var.environment\n}\n</code></pre> <pre><code># Updated: modules/s3_bucket/main.tf\nresource \"aws_s3_bucket\" \"data_lake\" {  # Renamed from bucket\n  bucket = \"${var.environment}-${var.bucket_name}\"\n}\n</code></pre> <pre><code>cd environments/dev\nterraform init\nterraform state mv module.data_lake.aws_s3_bucket.bucket module.data_lake.aws_s3_bucket.data_lake\nterraform plan -var-file=terraform.tfvars  # Verify no destroy\nterraform apply -var-file=terraform.tfvars\n</code></pre></p> <ul> <li>Best Practice: Run <code>terraform state mv</code> in the <code>plan_dev</code> job to preview changes, and document renaming in PRs for team review.</li> </ul>"},{"location":"resources/Terraform_Scenarios/#additional-notes","title":"Additional Notes","text":"<ul> <li>Pipeline Integration: Use the <code>.gitlab-ci.yml</code> from the main cheatsheet to automate validation, planning, and applying changes, ensuring safe handling of state, secrets, and drift detection.</li> <li>Documentation: Update <code>README.md</code> to include recovery steps for each scenario (e.g., state restoration, drift detection setup).</li> </ul> <pre><code>## Handling Terraform Scenarios\n- State Deletion: Restore from `s3://my-terraform-state/backups/&lt;env&gt;/&lt;timestamp&gt;.tfstate`.\n- S3 Bucket Renaming: See Scenario 2 for detailed steps.\n- Drift Detection: Run the `drift_detection` job in GitLab CI/CD.\n- Zero-Downtime Updates: Enable `create_before_destroy` and `multi_az` for RDS in `modules/rds/main.tf`.\n</code></pre> <ul> <li>Testing: Test all changes in <code>dev</code> or <code>staging</code> environments using the <code>plan_dev</code> and <code>apply_dev</code> jobs before applying to <code>prod</code>.</li> </ul>"},{"location":"resources/python_cheatsheet/","title":"Python Cheatsheet","text":"<p>WIP</p> <p>Python utility functions to keep in your L1 cache for coding interviews.</p>"},{"location":"resources/python_cheatsheet/#strings","title":"Strings","text":""},{"location":"resources/python_cheatsheet/#stralnum","title":"str.alnum","text":"<p>\"mystring\".isalnum()  </p> <p>str.isalnum() returns True if all characters in the string are alphanumeric and the string is not empty. If the string contains spaces, special characters, or is empty, it will return False.</p> Valid Palindrome <pre><code> class Solution:\n     def isPalindrome(self, s: str) -&gt; bool:\n\n         start,end = 0, len(s)-1\n\n         while start &lt; end :\n             if not s[start].isalnum() : \n                 start+=1\n                 continue\n             if not s[end].isalnum() :\n                 end-=1\n                 continue\n\n             if not s[start].lower() == s[end].lower() :\n                 return False\n\n             start +=1\n             end-=1\n\n         return True\n</code></pre>"},{"location":"resources/python_cheatsheet/#ord","title":"ord","text":"<p>ord('c')</p> <p>The ord() function in Python is used to get the Unicode code point (an integer) of a given character. It is the inverse of the chr() function, which converts a Unicode code point back to a character.</p> Score of String <pre><code>class Solution:\n    def scoreOfString(self, s: str) -&gt; int:\n        # Initializing this to cancel out the effect of the first element.\n        prev=ord(s[0]) \n        res = 0 \n        for c in s :\n            curr = ord(c)\n            res+= abs(curr - prev)\n            prev = curr \n        return res \n</code></pre>"},{"location":"resources/python_cheatsheet/#bisect-left-with-custom-key","title":"Bisect left with custom key","text":"<p>https://docs.python.org/3/library/bisect.html</p> <pre><code># from solution for https://leetcode.com/problems/insert-interval/\nfrom bisect import bisect_left\nintervals = [[0,1], [4,5]]\nnew_interval = [3,4]\nintervals.insert(\n     bisect_left(intervals,new_interval[0],key=lambda x : x[0]),\n     new_interval\n)\n</code></pre> <p>In Python, the insert() method is used to add an element to a list at a specific, chosen index. This method modifies the list in-place, meaning it directly changes the original list rather than creating a new one.</p> <p><code>list.insert(index,element)</code></p>"},{"location":"resources/python_cheatsheet/#sort-by-ascending-with-descending-tie-breaker","title":"Sort by ascending with descending tie breaker","text":"<pre><code>\"\"\"\nhttps://leetcode.com/problems/remove-covered-intervals/description/\n\"\"\"\nclass Solution:\n    def removeCoveredIntervals(self, intervals: List[List[int]]) -&gt; int:\n        # Sort by start point.\n        # If two intervals share the same start point\n        # put the longer one to be the first.\n        intervals.sort(key = lambda x: (x[0], -x[1]))\n        count = 0\n\n        prev_end = 0\n        for _, end in intervals:\n            # if current interval is not covered\n            # by the previous one\n            if end &gt; prev_end:\n                count += 1    \n                prev_end = end\n\n        return count\n</code></pre>"},{"location":"blog/archive/2025/","title":"2025","text":""},{"location":"blog/category/c/","title":"C","text":""},{"location":"blog/category/basics/","title":"Basics","text":""},{"location":"blog/category/encodings/","title":"Encodings","text":""},{"location":"blog/category/aws/","title":"AWS","text":""},{"location":"blog/category/iam/","title":"IAM","text":""},{"location":"blog/category/etl/","title":"ETL","text":""},{"location":"blog/category/serverless/","title":"Serverless","text":""}]}